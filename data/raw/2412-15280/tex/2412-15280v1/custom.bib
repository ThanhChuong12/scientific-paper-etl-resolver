@article{chatgpt,
	author = {OpenAI},
	journal = {OpenAI blog},
	title = {large-scale generative pre-training model for conversation},
	url = {https://openai.com/blog/chatgpt},
	year = {2022},
	bdsk-url-1 = {https://openai.com/blog/chatgpt}}


@misc{openai2023gpt4,
	archiveprefix = {arXiv},
	author = {OpenAI},
	eprint = {2303.08774},
	primaryclass = {cs.CL},
	title = {GPT-4 Technical Report},
	year = {2023}}

@article{DBLP:journals/corr/abs-2302-13971,
	author = {Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie{-}Anne Lachaux and Timoth{\'{e}}e Lacroix and Baptiste Rozi{\`{e}}re and Naman Goyal and Eric Hambro and Faisal Azhar and Aur{\'{e}}lien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-2302-13971.bib},
	doi = {10.48550/ARXIV.2302.13971},
	eprint = {2302.13971},
	eprinttype = {arXiv},
	journal = {CoRR},
	timestamp = {Mon, 28 Aug 2023 21:26:20 +0200},
	title = {LLaMA: Open and Efficient Foundation Language Models},
	url = {https://doi.org/10.48550/arXiv.2302.13971},
	volume = {abs/2302.13971},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.2302.13971},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2302.13971}}


@misc{touvron2023llama,
	archiveprefix = {arXiv},
	author = {Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
	eprint = {2307.09288},
	primaryclass = {cs.CL},
	title = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
	year = {2023}}


@article{chen2023combating,
  title={Combating misinformation in the age of llms: Opportunities and challenges},
  author={Chen, Canyu and Shu, Kai},
  journal={arXiv preprint arXiv:2311.05656},
  year={2023}
}

@article{zhu2020modifying,
  title={Modifying memories in transformer models},
  author={Zhu, Chen and Rawat, Ankit Singh and Zaheer, Manzil and Bhojanapalli, Srinadh and Li, Daliang and Yu, Felix and Kumar, Sanjiv},
  journal={arXiv preprint arXiv:2012.00363},
  year={2020}
}

@article{meng2022mass,
  title={Mass-editing memory in a transformer},
  author={Meng, Kevin and Sharma, Arnab Sen and Andonian, Alex and Belinkov, Yonatan and Bau, David},
  journal={arXiv preprint arXiv:2210.07229},
  year={2022}
}

@article{meng2022locating,
  title={Locating and editing factual associations in GPT},
  author={Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17359--17372},
  year={2022}
}

@inproceedings{mitchell2022memory,
  title={Memory-based model editing at scale},
  author={Mitchell, Eric and Lin, Charles and Bosselut, Antoine and Manning, Christopher D and Finn, Chelsea},
  booktitle={International Conference on Machine Learning},
  pages={15817--15831},
  year={2022},
  organization={PMLR}
}

@article{sinitsin2020editable,
  title={Editable neural networks},
  author={Sinitsin, Anton and Plokhotnyuk, Vsevolod and Pyrkin, Dmitriy and Popov, Sergei and Babenko, Artem},
  journal={arXiv preprint arXiv:2004.00345},
  year={2020}
}

@article{yao2023editing,
  title={Editing large language models: Problems, methods, and opportunities},
  author={Yao, Yunzhi and Wang, Peng and Tian, Bozhong and Cheng, Siyuan and Li, Zhoubo and Deng, Shumin and Chen, Huajun and Zhang, Ningyu},
  journal={arXiv preprint arXiv:2305.13172},
  year={2023}
}

@article{huang2023transformer,
  title={Transformer-patcher: One mistake worth one neuron},
  author={Huang, Zeyu and Shen, Yikang and Zhang, Xiaofeng and Zhou, Jie and Rong, Wenge and Xiong, Zhang},
  journal={arXiv preprint arXiv:2301.09785},
  year={2023}
}

@article{de2021editing,
  title={Editing factual knowledge in language models},
  author={De Cao, Nicola and Aziz, Wilker and Titov, Ivan},
  journal={arXiv preprint arXiv:2104.08164},
  year={2021}
}

@article{cohen2024evaluating,
  title={Evaluating the ripple effects of knowledge editing in language models},
  author={Cohen, Roi and Biran, Eden and Yoran, Ori and Globerson, Amir and Geva, Mor},
  journal={Transactions of the Association for Computational Linguistics},
  volume={12},
  pages={283--298},
  year={2024},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@article{zhong2023mquake,
  title={Mquake: Assessing knowledge editing in language models via multi-hop questions},
  author={Zhong, Zexuan and Wu, Zhengxuan and Manning, Christopher D and Potts, Christopher and Chen, Danqi},
  journal={arXiv preprint arXiv:2305.14795},
  year={2023}
}

@article{wang2024deepedit,
  title={DeepEdit: Knowledge Editing as Decoding with Constraints},
  author={Wang, Yiwei and Chen, Muhao and Peng, Nanyun and Chang, Kai-Wei},
  journal={arXiv preprint arXiv:2401.10471},
  year={2024}
}

@article{shi2024retrieval,
  title={Retrieval-Enhanced Knowledge Editing for Multi-Hop Question Answering in Language Models},
  author={Shi, Yucheng and Tan, Qiaoyu and Wu, Xuansheng and Zhong, Shaochen and Zhou, Kaixiong and Liu, Ninghao},
  journal={arXiv preprint arXiv:2403.19631},
  year={2024}
}

@inproceedings{li-etal-2023-contrastive,
	abstract = {Given a language model (LM), maximum probability is a poor decoding objective for open-ended generation, because it produces short and repetitive text. On the other hand, sampling can often produce incoherent text that drifts from the original topics. We propose contrastive decoding (CD), a reliable decoding approach that optimizes a contrastive objective subject to a plausibility constraint. The contrastive objective returns the difference between the likelihood under a large LM (called the expert, e.g. OPT-13B) and a small LM (called the amateur, e.g. OPT-125M), and the constraint ensures that the outputs are plausible. CD is inspired by the fact that the failures of larger LMs (e.g., repetition, inco- herence) are even more prevalent in smaller LMs, and that this difference signals which texts should be preferred. CD requires zero additional training, and produces higher quality text than decoding from the larger LM alone. It also works across model scales (OPT-13B and GPT2-1.5B) and significantly outperforms four strong decoding algorithms (e.g., nucleus, top-k) in automatic and human evaluations across wikipedia, news and story domains.},
	address = {Toronto, Canada},
	author = {Li, Xiang Lisa and Holtzman, Ari and Fried, Daniel and Liang, Percy and Eisner, Jason and Hashimoto, Tatsunori and Zettlemoyer, Luke and Lewis, Mike},
	booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	doi = {10.18653/v1/2023.acl-long.687},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	pages = {12286--12312},
	publisher = {Association for Computational Linguistics},
	title = {Contrastive Decoding: Open-ended Text Generation as Optimization},
	url = {https://aclanthology.org/2023.acl-long.687},
	year = {2023},
	bdsk-url-1 = {https://aclanthology.org/2023.acl-long.687},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2023.acl-long.687}}

@article{chuang2023dola,
	author = {Chuang, Yung-Sung and Xie, Yujia and Luo, Hongyin and Kim, Yoon and Glass, James and He, Pengcheng},
	journal = {arXiv preprint arXiv:2309.03883},
	title = {DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models},
	year = {2023}}

@article{zhang2023alleviating,
  title={Alleviating hallucinations of large language models through induced hallucinations},
  author={Zhang, Yue and Cui, Leyang and Bi, Wei and Shi, Shuming},
  journal={arXiv preprint arXiv:2312.15710},
  year={2023}
}

@article{holtzman2019curious,
  title={The curious case of neural text degeneration},
  author={Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
  journal={arXiv preprint arXiv:1904.09751},
  year={2019}
}

@article{fan2018hierarchical,
  title={Hierarchical neural story generation},
  author={Fan, Angela and Lewis, Mike and Dauphin, Yann},
  journal={arXiv preprint arXiv:1805.04833},
  year={2018}
}

@article{hu2023survey,
  title={A survey of knowledge enhanced pre-trained language models},
  author={Hu, Linmei and Liu, Zeyi and Zhao, Ziwang and Hou, Lei and Nie, Liqiang and Li, Juanzi},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2023},
  publisher={IEEE}
}

@article{pan2024unifying,
  title={Unifying large language models and knowledge graphs: A roadmap},
  author={Pan, Shirui and Luo, Linhao and Wang, Yufei and Chen, Chen and Wang, Jiapu and Wu, Xindong},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2024},
  publisher={IEEE}
}

@article{huang2023look,
  title={Look before you leap: An exploratory study of uncertainty measurement for large language models},
  author={Huang, Yuheng and Song, Jiayang and Wang, Zhijie and Chen, Huaming and Ma, Lei},
  journal={arXiv preprint arXiv:2307.10236},
  year={2023}
}

@inproceedings{feng2024retrieval,
  title={Retrieval-generation synergy augmented large language models},
  author={Feng, Zhangyin and Feng, Xiaocheng and Zhao, Dezhi and Yang, Maojin and Qin, Bing},
  booktitle={ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={11661--11665},
  year={2024},
  organization={IEEE}
}

@article{madaan2022memory,
  title={Memory-assisted prompt editing to improve gpt-3 after deployment},
  author={Madaan, Aman and Tandon, Niket and Clark, Peter and Yang, Yiming},
  journal={arXiv preprint arXiv:2201.06009},
  year={2022}
}

@article{zheng2023can,
  title={Can We Edit Factual Knowledge by In-Context Learning?},
  author={Zheng, Ce and Li, Lei and Dong, Qingxiu and Fan, Yuxuan and Wu, Zhiyong and Xu, Jingjing and Chang, Baobao},
  journal={arXiv preprint arXiv:2305.12740},
  year={2023}
}


@article{mitchell2021fast,
  title={Fast model editing at scale},
  author={Mitchell, Eric and Lin, Charles and Bosselut, Antoine and Finn, Chelsea and Manning, Christopher D},
  journal={arXiv preprint arXiv:2110.11309},
  year={2021}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{wang2023easyedit,
  title={Easyedit: An easy-to-use knowledge editing framework for large language models},
  author={Wang, Peng and Zhang, Ningyu and Xie, Xin and Yao, Yunzhi and Tian, Bozhong and Wang, Mengru and Xi, Zekun and Cheng, Siyuan and Liu, Kangwei and Zheng, Guozhou and others},
  journal={arXiv preprint arXiv:2308.07269},
  year={2023}
}


@article{li2024understanding,
  title={Understanding and Patching Compositional Reasoning in LLMs},
  author={Li, Zhaoyi and Jiang, Gangwei and Xie, Hong and Song, Linqi and Lian, Defu and Wei, Ying},
  journal={arXiv preprint arXiv:2402.14328},
  year={2024}
}

@article{kang2024comparing,
  title={Comparing hallucination detection metrics for multilingual generation},
  author={Kang, Haoqiang and Blevins, Terra and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2402.10496},
  year={2024}
}

@article{yang2024kg,
  title={KG-Rank: Enhancing Large Language Models for Medical QA with Knowledge Graphs and Ranking Techniques},
  author={Yang, Rui and Liu, Haoran and Zeng, Qingcheng and Ke, Yu He and Li, Wanxin and Cheng, Lechao and Chen, Qingyu and Caverlee, James and Matsuo, Yutaka and Li, Irene},
  journal={arXiv preprint arXiv:2403.05881},
  year={2024}
}

@article{song2024fmint,
  title={FMint: Bridging Human Designed and Data Pretrained Models for Differential Equation Foundation Model},
  author={Song, Zezheng and Yuan, Jiaxin and Yang, Haizhao},
  journal={arXiv preprint arXiv:2404.14688},
  year={2024}
}

@misc{mei2024slang,
      title={SLANG: New Concept Comprehension of Large Language Models}, 
      author={Lingrui Mei and Shenghua Liu and Yiwei Wang and Baolong Bi and Xueqi Cheng},
      year={2024},
      eprint={2401.12585},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{xu2024editing,
  title={Editing Factual Knowledge and Explanatory Ability of Medical Large Language Models},
  author={Xu, Derong and Zhang, Ziheng and Zhu, Zhihong and Lin, Zhenxi and Liu, Qidong and Wu, Xian and Xu, Tong and Zhao, Xiangyu and Zheng, Yefeng and Chen, Enhong},
  journal={arXiv preprint arXiv:2402.18099},
  year={2024}
}

@misc{bi2024decoding,
  title={Decoding by Contrasting Knowledge: Enhancing LLMs' Confidence on Edited Facts}, 
  author={Baolong Bi and Shenghua Liu and Lingrui Mei and Yiwei Wang and Pengliang Ji and Xueqi Cheng},
  year={2024},
  eprint={2405.11613},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}

@inproceedings{niwattanakul2013using,
  title={Using of Jaccard coefficient for keywords similarity},
  author={Niwattanakul, Suphakit and Singthongchai, Jatsada and Naenudorn, Ekkachai and Wanapu, Supachanun},
  booktitle={Proceedings of the international multiconference of engineers and computer scientists},
  volume={1},
  number={6},
  pages={380--384},
  year={2013}
}

@article{liu2023summary,
	author = {Liu, Yiheng and Han, Tianle and Ma, Siyuan and Zhang, Jiayue and Yang, Yuanyuan and Tian, Jiaming and He, Hao and Li, Antong and He, Mengshen and Liu, Zhengliang and others},
	journal = {Meta-Radiology},
	pages = {100017},
	publisher = {Elsevier},
	title = {Summary of chatgpt-related research and perspective towards the future of large language models},
	year = {2023}}



@misc{chang2023survey,
	archiveprefix = {arXiv},
	author = {Yupeng Chang and Xu Wang and Jindong Wang and Yuan Wu and Linyi Yang and Kaijie Zhu and Hao Chen and Xiaoyuan Yi and Cunxiang Wang and Yidong Wang and Wei Ye and Yue Zhang and Yi Chang and Philip S. Yu and Qiang Yang and Xing Xie},
	eprint = {2307.03109},
	primaryclass = {cs.CL},
	title = {A Survey on Evaluation of Large Language Models},
	year = {2023}}

@article{zhang2023survey,
  title={A survey of controllable text generation using transformer-based pre-trained language models},
  author={Zhang, Hanqing and Song, Haolin and Li, Shaoyu and Zhou, Ming and Song, Dawei},
  journal={ACM Computing Surveys},
  volume={56},
  number={3},
  pages={1--37},
  year={2023},
  publisher={ACM New York, NY}
}

@misc{tonmoy2024comprehensive,
	archiveprefix = {arXiv},
	author = {S. M Towhidul Islam Tonmoy and S M Mehedi Zaman and Vinija Jain and Anku Rani and Vipula Rawte and Aman Chadha and Amitava Das},
	eprint = {2401.01313},
	primaryclass = {cs.CL},
	title = {A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models},
	year = {2024}}

@misc{huang2023survey,
	archiveprefix = {arXiv},
	author = {Lei Huang and Weijiang Yu and Weitao Ma and Weihong Zhong and Zhangyin Feng and Haotian Wang and Qianglong Chen and Weihua Peng and Xiaocheng Feng and Bing Qin and Ting Liu},
	eprint = {2311.05232},
	primaryclass = {cs.CL},
	title = {A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions},
	year = {2023}}


@misc{zhang2023sirens,
	archiveprefix = {arXiv},
	author = {Yue Zhang and Yafu Li and Leyang Cui and Deng Cai and Lemao Liu and Tingchen Fu and Xinting Huang and Enbo Zhao and Yu Zhang and Yulong Chen and Longyue Wang and Anh Tuan Luu and Wei Bi and Freda Shi and Shuming Shi},
	eprint = {2309.01219},
	primaryclass = {cs.CL},
	title = {Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models},
	year = {2023}}

@article{wang2023survey,
  title={Survey on factuality in large language models: Knowledge, retrieval and domain-specificity},
  author={Wang, Cunxiang and Liu, Xiaoze and Yue, Yuanhao and Tang, Xiangru and Zhang, Tianhang and Jiayang, Cheng and Yao, Yunzhi and Gao, Wenyang and Hu, Xuming and Qi, Zehan and others},
  journal={arXiv preprint arXiv:2310.07521},
  year={2023}
}

@misc{nakano2022webgptbrowserassistedquestionansweringhuman,
      title={WebGPT: Browser-assisted question-answering with human feedback}, 
      author={Reiichiro Nakano and Jacob Hilton and Suchir Balaji and Jeff Wu and Long Ouyang and Christina Kim and Christopher Hesse and Shantanu Jain and Vineet Kosaraju and William Saunders and Xu Jiang and Karl Cobbe and Tyna Eloundou and Gretchen Krueger and Kevin Button and Matthew Knight and Benjamin Chess and John Schulman},
      year={2022},
      eprint={2112.09332},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2112.09332}, 
}

@misc{yao2023reactsynergizingreasoningacting,
      title={ReAct: Synergizing Reasoning and Acting in Language Models}, 
      author={Shunyu Yao and Jeffrey Zhao and Dian Yu and Nan Du and Izhak Shafran and Karthik Narasimhan and Yuan Cao},
      year={2023},
      eprint={2210.03629},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2210.03629}, 
}

@misc{qin2024toollearningfoundationmodels,
      title={Tool Learning with Foundation Models}, 
      author={Yujia Qin and Shengding Hu and Yankai Lin and Weize Chen and Ning Ding and Ganqu Cui and Zheni Zeng and Yufei Huang and Chaojun Xiao and Chi Han and Yi Ren Fung and Yusheng Su and Huadong Wang and Cheng Qian and Runchu Tian and Kunlun Zhu and Shihao Liang and Xingyu Shen and Bokai Xu and Zhen Zhang and Yining Ye and Bowen Li and Ziwei Tang and Jing Yi and Yuzhang Zhu and Zhenning Dai and Lan Yan and Xin Cong and Yaxi Lu and Weilin Zhao and Yuxiang Huang and Junxi Yan and Xu Han and Xian Sun and Dahai Li and Jason Phang and Cheng Yang and Tongshuang Wu and Heng Ji and Zhiyuan Liu and Maosong Sun},
      year={2024},
      eprint={2304.08354},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2304.08354}, 
}

@misc{guu2020realmretrievalaugmentedlanguagemodel,
      title={REALM: Retrieval-Augmented Language Model Pre-Training}, 
      author={Kelvin Guu and Kenton Lee and Zora Tung and Panupong Pasupat and Ming-Wei Chang},
      year={2020},
      eprint={2002.08909},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2002.08909}, 
}

@misc{izacard2021leveragingpassageretrievalgenerative,
      title={Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering}, 
      author={Gautier Izacard and Edouard Grave},
      year={2021},
      eprint={2007.01282},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2007.01282}, 
}

@misc{zhong2022traininglanguagemodelsmemory,
      title={Training Language Models with Memory Augmentation}, 
      author={Zexuan Zhong and Tao Lei and Danqi Chen},
      year={2022},
      eprint={2205.12674},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2205.12674}, 
}


@misc{si2023promptinggpt3reliable,
      title={Prompting GPT-3 To Be Reliable}, 
      author={Chenglei Si and Zhe Gan and Zhengyuan Yang and Shuohang Wang and Jianfeng Wang and Jordan Boyd-Graber and Lijuan Wang},
      year={2023},
      eprint={2210.09150},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2210.09150}, 
}

@article{petroni2020context,
  title={How context affects language models' factual predictions},
  author={Petroni, Fabio and Lewis, Patrick and Piktus, Aleksandra and Rockt{\"a}schel, Tim and Wu, Yuxiang and Miller, Alexander H and Riedel, Sebastian},
  journal={arXiv preprint arXiv:2005.04611},
  year={2020}
}

@misc{xie2024adaptivechameleonstubbornsloth,
      title={Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts}, 
      author={Jian Xie and Kai Zhang and Jiangjie Chen and Renze Lou and Yu Su},
      year={2024},
      eprint={2305.13300},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.13300}, 
}

@article{shi2023trusting,
  title={Trusting your evidence: Hallucinate less with context-aware decoding},
  author={Shi, Weijia and Han, Xiaochuang and Lewis, Mike and Tsvetkov, Yulia and Zettlemoyer, Luke and Yih, Scott Wen-tau},
  journal={arXiv preprint arXiv:2305.14739},
  year={2023}
}

@article{bi2024adaptive,
  title={Adaptive Token Biaser: Knowledge Editing via Biasing Key Entities},
  author={Bi, Baolong and Liu, Shenghua and Wang, Yiwei and Mei, Lingrui and Gao, Hongcheng and Xu, Yilong and Cheng, Xueqi},
  journal={arXiv preprint arXiv:2406.12468},
  year={2024}
}

@article{zhou2023context,
  title={Context-faithful prompting for large language models},
  author={Zhou, Wenxuan and Zhang, Sheng and Poon, Hoifung and Chen, Muhao},
  journal={arXiv preprint arXiv:2303.11315},
  year={2023}
}

@article{ming2024faitheval,
  title={FaithEval: Can Your Language Model Stay Faithful to Context, Even If" The Moon is Made of Marshmallows"},
  author={Ming, Yifei and Purushwalkam, Senthil and Pandit, Shrey and Ke, Zixuan and Nguyen, Xuan-Phi and Xiong, Caiming and Joty, Shafiq},
  journal={arXiv preprint arXiv:2410.03727},
  year={2024}
}

@article{bi2024factuality,
  title={Is Factuality Enhancement a Free Lunch For LLMs? Better Factuality Can Lead to Worse Context-Faithfulness},
  author={Bi, Baolong and Liu, Shenghua and Wang, Yiwei and Mei, Lingrui and Fang, Junfeng and Gao, Hongcheng and Ni, Shiyu and Cheng, Xueqi},
  journal={Authorea Preprints},
  year={2024},
  publisher={Authorea}
}

@article{liu2023trustworthy,
  title={Trustworthy LLMs: A survey and guideline for evaluating large language models' alignment},
  author={Liu, Yang and Yao, Yuanshun and Ton, Jean-Francois and Zhang, Xiaoying and Cheng, Ruocheng Guo Hao and Klochkov, Yegor and Taufiq, Muhammad Faaiz and Li, Hang},
  journal={arXiv preprint arXiv:2308.05374},
  year={2023}
}

@article{shen2023large,
  title={Large language model alignment: A survey},
  author={Shen, Tianhao and Jin, Renren and Huang, Yufei and Liu, Chuang and Dong, Weilong and Guo, Zishan and Wu, Xinwei and Liu, Yan and Xiong, Deyi},
  journal={arXiv preprint arXiv:2309.15025},
  year={2023}
}

@article{tian2023fine,
  title={Fine-tuning language models for factuality},
  author={Tian, Katherine and Mitchell, Eric and Yao, Huaxiu and Manning, Christopher D and Finn, Chelsea},
  journal={arXiv preprint arXiv:2311.08401},
  year={2023}
}

@article{cao2023defending,
  title={Defending against alignment-breaking attacks via robustly aligned llm},
  author={Cao, Bochuan and Cao, Yuanpu and Lin, Lu and Chen, Jinghui},
  journal={arXiv preprint arXiv:2309.14348},
  year={2023}
}

@article{rafailov2024direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{kwiatkowski2019natural,
  title={Natural questions: a benchmark for question answering research},
  author={Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and others},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={453--466},
  year={2019},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{vrandevcic2014wikidata,
  title={Wikidata: a free collaborative knowledgebase},
  author={Vrande{\v{c}}i{\'c}, Denny and Kr{\"o}tzsch, Markus},
  journal={Communications of the ACM},
  volume={57},
  number={10},
  pages={78--85},
  year={2014},
  publisher={ACM New York, NY, USA}
}

@article{longpre2021entity,
  title={Entity-based knowledge conflicts in question answering},
  author={Longpre, Shayne and Perisetla, Kartik and Chen, Anthony and Ramesh, Nikhil and DuBois, Chris and Singh, Sameer},
  journal={arXiv preprint arXiv:2109.05052},
  year={2021}
}

@article{lin2021truthfulqa,
  title={Truthfulqa: Measuring how models mimic human falsehoods},
  author={Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  journal={arXiv preprint arXiv:2109.07958},
  year={2021}
}

@article{kaddour2023challenges,
  title={Challenges and applications of large language models},
  author={Kaddour, Jean and Harris, Joshua and Mozes, Maximilian and Bradley, Herbie and Raileanu, Roberta and McHardy, Robert},
  journal={arXiv preprint arXiv:2307.10169},
  year={2023}
}

@article{mei2024not,
  title={" Not Aligned" is Not" Malicious": Being Careful about Hallucinations of Large Language Models' Jailbreak},
  author={Mei, Lingrui and Liu, Shenghua and Wang, Yiwei and Bi, Baolong and Mao, Jiayi and Cheng, Xueqi},
  journal={arXiv preprint arXiv:2406.11668},
  year={2024}
}

@inproceedings{gunjal2024detecting,
  title={Detecting and preventing hallucinations in large vision language models},
  author={Gunjal, Anisha and Yin, Jihan and Bas, Erhan},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  pages={18135--18143},
  year={2024}
}

@article{zhang2024mm,
  title={Mm-llms: Recent advances in multimodal large language models},
  author={Zhang, Duzhen and Yu, Yahan and Li, Chenxing and Dong, Jiahua and Su, Dan and Chu, Chenhui and Yu, Dong},
  journal={arXiv preprint arXiv:2401.13601},
  year={2024}
}

@article{liu2024exploring,
  title={Exploring and evaluating hallucinations in llm-powered code generation},
  author={Liu, Fang and Liu, Yang and Shi, Lin and Huang, Houkun and Wang, Ruifeng and Yang, Zhen and Zhang, Li},
  journal={arXiv preprint arXiv:2404.00971},
  year={2024}
}

@article{forgan2005building,
  title={Building the museum: Knowledge, conflict, and the power of place},
  author={Forgan, Sophie},
  journal={Isis},
  volume={96},
  number={4},
  pages={572--585},
  year={2005},
  publisher={The University of Chicago Press}
}

@article{xu2024knowledge,
  title={Knowledge conflicts for llms: A survey},
  author={Xu, Rongwu and Qi, Zehan and Guo, Zhijiang and Wang, Cunxiang and Wang, Hongru and Zhang, Yue and Xu, Wei},
  journal={arXiv preprint arXiv:2403.08319},
  year={2024}
}

@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}

@article{gao2023retrieval,
  title={Retrieval-augmented generation for large language models: A survey},
  author={Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Haofen},
  journal={arXiv preprint arXiv:2312.10997},
  year={2023}
}

@article{jiang2023active,
  title={Active retrieval augmented generation},
  author={Jiang, Zhengbao and Xu, Frank F and Gao, Luyu and Sun, Zhiqing and Liu, Qian and Dwivedi-Yu, Jane and Yang, Yiming and Callan, Jamie and Neubig, Graham},
  journal={arXiv preprint arXiv:2305.06983},
  year={2023}
}

@article{chen2022rich,
  title={Rich knowledge sources bring complex knowledge conflicts: Recalibrating models to reflect conflicting evidence},
  author={Chen, Hung-Ting and Zhang, Michael JQ and Choi, Eunsol},
  journal={arXiv preprint arXiv:2210.13701},
  year={2022}
}

@article{karpukhin2020dense,
  title={Dense passage retrieval for open-domain question answering},
  author={Karpukhin, Vladimir and O{\u{g}}uz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau},
  journal={arXiv preprint arXiv:2004.04906},
  year={2020}
}


@article{li2024investigating,
  title={Investigating Context-Faithfulness in Large Language Models: The Roles of Memory Strength and Evidence Style},
  author={Li, Yuepei and Zhou, Kang and Qiao, Qiao and Nguyen, Bach and Wang, Qing and Li, Qi},
  journal={arXiv preprint arXiv:2409.10955},
  year={2024}
}

@article{bi2024struedit,
  title={Struedit: Structured outputs enable the fast and accurate knowledge editing for large language models},
  author={Bi, Baolong and Liu, Shenghua and Wang, Yiwei and Mei, Lingrui and Gao, Hongcheng and Fang, Junfeng and Cheng, Xueqi},
  journal={arXiv preprint arXiv:2409.10132},
  year={2024}
}

@article{huang2024can,
  title={Can Knowledge Editing Really Correct Hallucinations?},
  author={Huang, Baixiang and Chen, Canyu and Xu, Xiongxiao and Payani, Ali and Shu, Kai},
  journal={arXiv preprint arXiv:2410.16251},
  year={2024}
}

@article{li2024cmmath,
  title={Cmmath: A chinese multi-modal math skill evaluation benchmark for foundation models},
  author={Li, Zhong-Zhi and Zhang, Ming-Liang and Yin, Fei and Ji, Zhi-Long and Bai, Jin-Feng and Pan, Zhen-Ru and Zeng, Fan-Hu and Xu, Jian and Zhang, Jia-Xin and Liu, Cheng-Lin},
  journal={arXiv preprint arXiv:2407.12023},
  year={2024}
}

@article{zhang2024geoeval,
  title={GeoEval: benchmark for evaluating LLMs and Multi-Modal Models on geometry problem-solving},
  author={Zhang, Jiaxin and Li, Zhongzhi and Zhang, Mingliang and Yin, Fei and Liu, Chenglin and Moshfeghi, Yashar},
  journal={arXiv preprint arXiv:2402.10104},
  year={2024}
}

@inproceedings{bi2024lpnl,
  title={Lpnl: Scalable link prediction with large language models},
  author={Bi, Baolong and Liu, Shenghua and Wang, Yiwei and Mei, Lingrui and Cheng, Xueqi},
  booktitle={Findings of the Association for Computational Linguistics ACL 2024},
  pages={3615--3625},
  year={2024}
}