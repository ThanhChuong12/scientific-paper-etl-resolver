\section{Conclusion}
In this paper, we introduced a generalized multilingual ASR pipeline~\shortname, that operates effectively without relying on language-specific modules. 
By utilizing Romanized transcription as a unified representation across languages, we structured the multilingual ASR pipeline into two phases.
Initially, Romanization aligns phonetic and orthographic features, allowing the universal transcription to be effectively generalized across diverse languages and trained efficiently with a smaller dataset. 
Subsequently, we used a frozen LLM to convert the universal transcription into language-specific ones.
This inversion process showed remarkable performance across languages, including those not previously encountered.
Our experiments demonstrated that the proposed method not only maintains performance for high-resource languages but also significantly outperforms existing methods for low-resource languages, all while effectively handling unseen languages.
Furthermore, our approach matched the performance of models that employ language-specific modules, despite not using any such components. 
We anticipate that this research will provide a viable alternative for utilizing LLMs to support universal multilingual ASR systems across a variety of applications.