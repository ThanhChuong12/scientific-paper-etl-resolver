
@inproceedings{clip,
	title={Learning transferable visual models from natural language supervision},
	author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
	booktitle={ICML},
	pages={8748--8763},
	year={2021}
}

@inproceedings{align,
	title={Scaling up visual and vision-language representation learning with noisy text supervision},
	author={Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc and Sung, Yun-Hsuan and Li, Zhen and Duerig, Tom},
	booktitle={ICML},
	pages={4904--4916},
	year={2021}
}

@inproceedings{cocoop,
	title={Conditional Prompt Learning for Vision-Language Models},
	author={Zhou, Kaiyang and Yang, Jingkang and Loy, Chen Change and Liu, Ziwei},
	booktitle={CVPR},
	pages = {16795--16804},
	year={2022}
}

@inproceedings{coop,
	title={Learning to Prompt for Vision-Language Models},
	author={Zhou, Kaiyang and Yang, Jingkang and Loy, Chen Change and Liu, Ziwei},
	booktitle={IJCV},
	pages={2337--2348},
	year={2022}
}

@inproceedings{food101,
	title={Food-101--mining discriminative components with random forests},
	author={Bossard, Lukas and Guillaumin, Matthieu and Van Gool, Luc},
	booktitle={ECCV},
	pages={446--461},
	year={2014}
}

@inproceedings{dtd,
	title={Describing textures in the wild},
	author={Cimpoi, Mircea and Maji, Subhransu and Kokkinos, Iasonas and Mohamed, Sammy and Vedaldi, Andrea},
	booktitle={CVPR},
	pages={3606--3613},
	year={2014}
}

@inproceedings{imagenet,
	title={Imagenet: A large-scale hierarchical image database},
	author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
	booktitle={CVPR},
	pages={248--255},
	year={2009}
}

@inproceedings{caltech101,
	title={Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories},
	author={Fei-Fei, Li and Fergus, Rob and Perona, Pietro},
	booktitle={CVIU},
	pages        = {59--70},
  	year         = {2007}
}

@inproceedings{clip-adapter,
	title={Clip-adapter: Better vision-language models with feature adapters},
	author={Gao, Peng and Geng, Shijie and Zhang, Renrui and Ma, Teli and Fang, Rongyao and Zhang, Yongfeng and Li, Hongsheng and Qiao, Yu},
	booktitle={IJCV},
	pages={581-595},
	year={2024}
}

@inproceedings{eurosat,
	title={Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification},
	author={Helber, Patrick and Bischke, Benjamin and Dengel, Andreas and Borth, Damian},
	booktitle={IGARSS},
	pages={204--207},
	year={2018}
}

@inproceedings{imagenet-r,
	title={The many faces of robustness: A critical analysis of out-of-distribution generalization},
	author={Hendrycks, Dan and Basart, Steven and Mu, Norman and Kadavath, Saurav and Wang, Frank and Dorundo, Evan and Desai, Rahul and Zhu, Tyler and Parajuli, Samyak and Guo, Mike and others},
	booktitle={ICCV},
	pages={8320--8329},
	year={2021}
}

@inproceedings{imagenet-a,
	title={Natural adversarial examples},
	author={Hendrycks, Dan and Zhao, Kevin and Basart, Steven and Steinhardt, Jacob and Song, Dawn},
	booktitle={CVPR},
	pages={15262--15271},
	year={2021}
}


@inproceedings{oxfordpets,
	title={Cats and dogs},
	author={Parkhi, Omkar M and Vedaldi, Andrea and Zisserman, Andrew and Jawahar, CV},
	booktitle={CVPR},
	pages = {3498--3505},
	year={2012}
}

@inproceedings{stanfordcars,
	title={3d object representations for fine-grained categorization},
	author={Krause, Jonathan and Stark, Michael and Deng, Jia and Fei-Fei, Li},
	booktitle={ICCVW},
	pages={554--561},
	year={2013}
}

@inproceedings{maple,
	title={MaPLe: Multi-modal Prompt Learning},
	author={khattak, Muhammad Uzair and Rasheed, Hanoona and Maaz, Muhammad and Khan, Salman and Khan, Fahad Shahbaz},
	booktitle={CVPR},
	pages={19113--19122},
	year={2023}
}

@inproceedings{proda,
	title={Prompt distribution learning},
	author={Lu, Yuning and Liu, Jianzhuang and Zhang, Yonggang and Liu, Yajing and Tian, Xinmei},
	booktitle={CVPR},
	pages={5206--5215},
	year={2022}
}

@inproceedings{kgcoop,
	title={Visual-Language Prompt Tuning with Knowledge-guided Context Optimization},
	author={Hantao Yao, Rui Zhang, Changsheng Xu},
	booktitle={CVPR},
	pages={6757--6767},
	year={2023}
}

@inproceedings{plot,
	title={Prompt Learning with Optimal Transport for Vision-Language Models},
	author={Chen, Guangyi and Yao, Weiran and Song, Xiangchen and Li, Xinyue and Rao, Yongming and Zhang, Kun},
	booktitle={ICLR},
	pages={1--13},
	year={2023}
}

@inproceedings{oner,
	title={Unifying Vision-Language Representation Space with Single-Tower Transformer},
	author={Jang, Jiho and Kong, Chaerin and Jeon, Donghyeon and Kim, Seonhoon and Kwak, Nojun},
	booktitle={AAAI},
	pages={980--988},
	year={2023}
}


@inproceedings{tcl,
	title={Vision-Language Pre-Training with Triple Contrastive Learning},
	author={Yang, Jie and Duan, Jingjing and Tran, Siqi and Xu, Yuxin and Chanda, Subhabrata and Chen, Linjie and Zeng, Bing and Chilimbi, Trishul and Huang, Jing},
	booktitle={CVPR},
	pages={15671--15680},
	year={2022}
}

@inproceedings{declip,
	title={Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image  Pre-training Paradigm},
	author={Yangguang Li and Feng Liang and Lichen Zhao and Yufeng Cui and Wanli Ouyang and Jing Shao and Fengwei Yu and Junjie Yan},
	booktitle={ICLR},
	pages={1--13},
	year={2022},
}

@inproceedings{msclip,
	title={Learning visual representation from modality-shared contrastive language-image pre-training},
	author={You, Haoxuan and Zhou, Luowei and Xiao, Bin and Codella, Noel and Cheng, Yu and Xu, Ruochen and Chang, Shih-Fu and Yuan, Lu},
	booktitle={ECCV},
	pages={69--87},
	year={2022},
}

@inproceedings{calip,
	title={Calip: Zero-shot enhancement of clip with parameter-free attention},
	author={Guo, Ziyu and Zhang, Renrui and Qiu, Longtian and Ma, Xianzheng and Miao, Xupeng and He, Xuming and Cui, Bin},
	booktitle={AAAI},
	pages={746--754},
	year={2023}
}

@inproceedings{flowers102,
	title={Automated flower classification over a large number of classes},
	author={Nilsback, Maria-Elena and Zisserman, Andrew},
	booktitle={ICVGIP},
	pages={722--729},
	year={2008}
}

@inproceedings{tipadap,
	title={Tip-adapter: Training-free adaption of clip for few-shot classification},
	author={Zhang, Renrui and Zhang, Wei and Fang, Rongyao and Gao, Peng and Li, Kunchang and Dai, Jifeng and Qiao, Yu and Li, Hongsheng},
	booktitle={ECCV},
	pages={493--510},
	year={2022}
}

@article{prompt-adapter,
	title={Prompt Tuning based Adapter for Vision-Language Model Adaption},
	author={Sun, Jingchen and Qin, Jiayu and Lin, Zihao and Chen, Changyou},
	journal={arXiv preprint arXiv:2303.15234},
	year={2023}
}

@article{fgvc,
	title={Fine-grained visual classification of aircraft},
	author={Maji, Subhransu and Rahtu, Esa and Kannala, Juho and Blaschko, Matthew and Vedaldi, Andrea},
	journal={arXiv preprint arXiv:1306.5151},
	year={2013}
}

@article{ucf101,
	title={UCF101: A dataset of 101 human actions classes from videos in the wild},
	author={Soomro, Khurram and Zamir, Amir Roshan and Shah, Mubarak},
	journal={arXiv preprint arXiv:1212.0402},
	year={2012}
}

@inproceedings{sun397,
	title={SUN database: Large-scale scene recognition from abbey to zoo},
	author={Xiao, Jianxiong and Hays, James and Ehinger, Krista A and Oliva, Aude and Torralba, Antonio},
	booktitle={CVPR},
	pages={3485--3492},
	year={2010}
}

@inproceedings{imagenetv2,
	title={Do imagenet classifiers generalize to imagenet?},
	author={Recht, Benjamin and Roelofs, Rebecca and Schmidt, Ludwig and Shankar, Vaishaal},
	booktitle={ICML},
	pages={5389--5400},
	year={2019}
}


@inproceedings{imagenetsketch,
	title={Learning robust global representations by penalizing local predictive power},
	author={Wang, Haohan and Ge, Songwei and Lipton, Zachary and Xing, Eric P},
	booktitle={NeurIPS},
	pages = {10506--10518},
	year={2019}
}

@inproceedings{nlp1,
	title={Making Pre-trained Language Models Better Few-shot Learners},
	author={Gao, Tianyu and Fisch, Adam and Chen, Danqi},
	booktitle={ACL},
	pages={3816--3830},
	year={2021}
}

@inproceedings{nlp2,
	title={How can we know what language models know?},
	author={Jiang, Zhengbao and Xu, Frank F and Araki, Jun and Neubig, Graham},
	booktitle={TACL},
	pages={423--438},
	year={2020}
}

@inproceedings{nlp3,
	title = "The Power of Scale for Parameter-Efficient Prompt Tuning",
	author = "Lester, Brian  and
	Al-Rfou, Rami  and
	Constant, Noah",
	booktitle = "EMNLP",
	year = "2021",
	pages = "3045--3059",
}

@inproceedings{nlp4,
	title={Prefix-tuning: Optimizing continuous prompts for generation},
	author={Li, Xiang Lisa and Liang, Percy},
	booktitle={ACL},
	pages={4582--4597},
	year={2021}
}


@inproceedings{nlp5,
	title={Autoprompt: Eliciting knowledge from language models with automatically generated prompts},
	author={Shin, Taylor and Razeghi, Yasaman and Logan IV, Robert L and Wallace, Eric and Singh, Sameer},
	booktitle={EMNLP},
	pages={4222--4235},
	year={2020}
}

@inproceedings{nlp6,
	title={Factual probing is [mask]: Learning vs. learning to recall},
	author={Zhong, Zexuan and Friedman, Dan and Chen, Danqi},
	booktitle={NAACL-HLT},
	pages={5017--5033},
	year={2021}
}

@inproceedings{dpt,
	author={Xing, Yinghui and Wu, Qirui and Cheng, De and Zhang, Shizhou and Liang, Guoqiang and Wang, Peng and Zhang., Yanning},
	booktitle={TMM}, 
	title={Dual Modality Prompt Tuning for Vision-Language Pre-Trained Model}, 
	year={2023},
	pages={1-13}
}

@inproceedings{hiclip,
	title={HiCLIP: Contrastive language-image pretraining with hierarchy-aware attention},
	author={Geng, Shijie and Yuan, Jianbo and Tian, Yu and Chen, Yuxiao and Zhang, Yongfeng},
	booktitle={ICLR},
	pages={1--13},
	year={2023}
}

@inproceedings{prograd,
	author = {Zhu, Beier and Niu, Yulei and Han, Yucheng and Wu, Yue and Zhang, Hanwang},
	title = {Prompt-aligned Gradient for Prompt Tuning},
	booktitle={ICCV},
	pages={15613-15623},
	year = {2023}
}

@inproceedings{goodprompt1,
	title={A good prompt is worth millions of parameters: Low-resource prompt-based learning for vision-language models},
	author={Jin, Wenpeng and Cheng, Yuxuan and Shen, Yelong and Chen, Wei and Ren, Xiang},
	booktitle={ACL},
	year={2022},
	pages={2763--2775}
}

@inproceedings{mauve,
	title={MAUVE: Measuring the Gap Between Neural Text and Human Text using Divergence Frontiers},
	author={Pillutla, Krishna and Swayamdipta, Swabha and Zellers, Rowan and Thickstun, John and Welleck, Sean and Choi, Yejin and Harchaoui, Zaid},
	booktitle = {NeurIPS},
	pages = {4816--4828},
	year      = {2021}
}

@inproceedings{harmonic,
	title={Zero-shot learning-the good, the bad and the ugly},
	author={Xian, Yongqin and Schiele, Bernt and Akata, Zeynep},
	booktitle={CVPR},
	year={2017}
}

@inproceedings{energy,
	title={Energy-Inspired Self-Supervised Pretraining for Vision Models},
	author={Wang, Ze and Wang, Jiang and Liu, Zicheng and Qiu, Qiang},
	booktitle={ICLR},
	year={2023}
}

@inproceedings{resnet,
	title={Deep residual learning for image recognition},
	author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	booktitle={CVPR},
	pages={770--778},
	year={2016}
}

@inproceedings{vit,
	title={An image is worth 16x16 words: Transformers for image recognition at scale},
	author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
	booktitle={ICLR},
	pages={1--13},
	year={2021}
}

@inproceedings{CaFo,
	title={Prompt, generate, then cache: Cascade of foundation models makes strong few-shot learners},
	author={Zhang, Renrui and Hu, Xiangfei and Li, Bohao and Huang, Siyuan and Deng, Hanqiu and Qiao, Yu and Gao, Peng and Li, Hongsheng},
	booktitle={CVPR},
	pages={15211--15222},
	year={2023}
}

@inproceedings{TaskRes,
	title={Task residual for tuning vision-language models},
	author={Yu, Tao and Lu, Zhihe and Jin, Xin and Chen, Zhibo and Wang, Xinchao},
	booktitle={CVPR},
	pages={10899--10909},
	year={2023}
}

@inproceedings{GPT3,
	title={Language models are few-shot learners},
	author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
	booktitle={NeurIPS},
	pages={1877--1901},
	year={2020}
}

@inproceedings{DINO,
	title={Emerging properties in self-supervised vision transformers},
	author={Caron, Mathilde and Touvron, Hugo and Misra, Ishan and J{\'e}gou, Herv{\'e} and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
	booktitle={ICCV},
	pages={9650--9660},
	year={2021}
}

@inproceedings{DALL-E,
	title={Zero-shot text-to-image generation},
	author={Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
	booktitle={ICML},
	pages={8821--8831},
	year={2021}
}

@inproceedings{PromptSRC,
	title={Self-regulating prompts: Foundational model adaptation without forgetting},
	author={Khattak, Muhammad Uzair and Wasim, Syed Talal and Naseer, Muzammal and Khan, Salman and Yang, Ming-Hsuan and Khan, Fahad Shahbaz},
	booktitle={ICCV},
	pages={15190--15200},
	year={2023}
}

@inproceedings{attention,
	title={Attention Is All You Need},
	author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser},
	booktitle={NeurIPS},
	pages={5998--6008},
	year={2017}
}
@inproceedings{self-distill,
  title={Be your own teacher: Improve the performance of convolutional neural networks via self distillation},
  author={Zhang, Linfeng and Song, Jiebo and Gao, Anni and Chen, Jingwei and Bao, Chenglong and Ma, Kaisheng},
  booktitle={ICCV},
  pages={3713--3722},
  year={2019}
}

@inproceedings{ATC,
  author       = {Chunjin Yang and
                  Fanman Meng and
                  Shuai Chen and
                  Mingyu Liu and
                  Runtong Zhang},
  title        = {Instance-Wise Adaptive Tuning and Caching for Vision-Language Models},
  booktitle    = {ECAI},
  pages        = {2834--2841},
  year         = {2023}
}
