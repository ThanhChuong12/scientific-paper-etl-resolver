@STRING{ACCV = {Proc. Asian Conf. Comp. Vis.}}
@STRING{CVIU = {Comp. Vis. Image Understanding}}
@STRING{CVPR = {Proc. IEEE Conf. Comp. Vis. Patt. Recogn.}}
@STRING{NN = {Proc. IEEE Conf. on Neural Networks}}
@STRING{CVPRW = {Proc. IEEE Conf. Comp. Vis. Patt. Recogn. Workshops}}
@STRING{ICLR = {Proc. Int. Conf. Learn. Repren.}}
@STRING{ICLRW = {Proc. Int. Conf. Learn. Repren. Workshops}}
@STRING{ECCV = {Proc. Eur. Conf. Comp. Vis.}}
@STRING{BMVC = {Proc. Brit. Mach. Vis. Conf.}}
@STRING{ICCV = {Proc. IEEE Int. Conf. Comp. Vis.}}
@STRING{ICPR = {Proc. IEEE Int. Conf. Patt. Recogn.}}
@STRING{ICIP = {Proc. IEEE Int. Conf. Image Process.}}
@STRING{ICDAR = {Proc. IEEE Int. Conf. Doc. Anal. and Recogn.}}
@STRING{ICML = {Proc. Int. Conf. Mach. Learn.}}
@STRING{IEEE_J_CASVT = {{IEEE} Trans. Circuits Syst. Video Technol.}}
@STRING{TPAMI = {{IEEE} Trans. Pattern Anal. Mach. Intell.}}
@STRING{IEEE_J_TIP = {{IEEE} Trans. Image Proc.}}
@STRING{IJCV = {Int. J. Comp. Vis.}}
@STRING{IJDAR = {Int. J. Doc. Anal. Recogn.}}
@STRING{HPCA = {Proc. IEEE Int. Sym. on High-Perf. Comp. Arch.}}
@STRING{DAS = {Proc. IEEE Int. Workshop. Doc. Anal. Syst.}}
@STRING{JMLR = {J. Mach. Learn. Res.}}
@STRING{ML = {Mach. Learn.}}
@STRING{MS = {Management Sci.}}
@STRING{NIPS = {Proc. Adv. Neural Inf. Process. Syst.}}
@STRING{NIPSW = {Proc. Adv. Neural Inf. Process. Syst. Workshops}}
@STRING{PR = {Pattern Recogn.}}
@STRING{SIGIR = {Ann. ACM SIGIR Conf.}}
@STRING{IJCAI = {Int. Joi. Conf. on Artificial Intelligence}}
@STRING{VLDB = {Proc. Int. Conf. Very Large Datadases}}
@STRING{AISTATS = {Proc. Int. Conf. Artif. Intell. Stat.}}
@STRING{AAAI={Proc. AAAI Conf. on Arti. Intel.}}
@STRING{CIVR={Proc.  ACM Int. Conf. on Image and Video Retrieval.}}
@STRING{ACM_Multi={Proc.  ACM Int. Conf. on Multimedia.}}
@STRING{ACM_SIGIR={Proc. Annual ACM SIGIR Conf.}}
@STRING{ACL={Proc. Annual Associa. Comp. Linguis.}}
@STRING{ICRA = {Proc. Int. Conf.  Robotics and Automation}}
@STRING{MLSys = {Proc. Int. Conf. Mach. Learn. and Syst.}}


@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{roziere2023code,
  title={Code llama: Open foundation models for code},
  author={Roziere, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Remez, Tal and Rapin, J{\'e}r{\'e}my and others},
  journal={arXiv preprint arXiv:2308.12950},
  year={2023}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{yu2023metamath,
  title={Metamath: Bootstrap your own mathematical questions for large language models},
  author={Yu, Longhui and Jiang, Weisen and Shi, Han and Yu, Jincheng and Liu, Zhengying and Zhang, Yu and Kwok, James T and Li, Zhenguo and Weller, Adrian and Liu, Weiyang},
  journal={arXiv preprint arXiv:2309.12284},
  year={2023}
}

@article{wu2023pmc,
  title={Pmc-llama: Further finetuning llama on medical papers},
  author={Wu, Chaoyi and Zhang, Xiaoman and Zhang, Ya and Wang, Yanfeng and Xie, Weidi},
  journal={arXiv preprint arXiv:2304.14454},
  year={2023}
}

@misc{Chinese-Mistral,
    author = {Zhou, Chen and Yuqi, Bai},
    title = {Chinese-Mistral: An Efficient and Effective Chinese Large Language Model},
    year = {2024},
    publisher = {GitHub},
    journal = {GitHub repository},
    howpublished = {\url{https://github.com/THU-ESIS/Chinese-Mistral}}
}

@article{jiang2023llm,
  title={Llm-blender: Ensembling large language models with pairwise ranking and generative fusion},
  author={Jiang, Dongfu and Ren, Xiang and Lin, Bill Yuchen},
  journal={arXiv preprint arXiv:2306.02561},
  year={2023}
}

@article{tang2024merging,
  title={Merging Multi-Task Models via Weight-Ensembling Mixture of Experts},
  author={Tang, Anke and Shen, Li and Luo, Yong and Yin, Nan and Zhang, Lefei and Tao, Dacheng},
  journal={arXiv preprint arXiv:2402.00433},
  year={2024}
}

@article{lu2023routing,
  title={Routing to the expert: Efficient reward-guided ensemble of large language models},
  author={Lu, Keming and Yuan, Hongyi and Lin, Runji and Lin, Junyang and Yuan, Zheng and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2311.08692},
  year={2023}
}

@article{yadav2024ties,
  title={Ties-merging: Resolving interference when merging models},
  author={Yadav, Prateek and Tam, Derek and Choshen, Leshem and Raffel, Colin A and Bansal, Mohit},
  journal=NIPS,
  volume={36},
  year={2024}
}

@article{yang2023adamerging,
  title={Adamerging: Adaptive model merging for multi-task learning},
  author={Yang, Enneng and Wang, Zhenyi and Shen, Li and Liu, Shiwei and Guo, Guibing and Wang, Xingwei and Tao, Dacheng},
  journal=ICLR,
  year={2024}
}

@article{yu2023language,
  title={Language models are super mario: Absorbing abilities from homologous models as a free lunch},
  author={Yu, Le and Yu, Bowen and Yu, Haiyang and Huang, Fei and Li, Yongbin},
  journal=ICML,
  year={2024}
}

@article{stoica2023zipit,
  title={Zipit! merging models from different tasks without training},
  author={Stoica, George and Bolya, Daniel and Bjorner, Jakob and Ramesh, Pratik and Hearn, Taylor and Hoffman, Judy},
  journal=ICLR,
  year={2024}
}

@article{kim2023solar,
  title={Solar 10.7 b: Scaling large language models with simple yet effective depth up-scaling},
  author={Kim, Dahyun and Park, Chanjun and Kim, Sanghoon and Lee, Wonsung and Song, Wonho and Kim, Yunsu and Kim, Hyeonwoo and Kim, Yungi and Lee, Hyeonju and Kim, Jihoo and others},
  journal={arXiv preprint arXiv:2312.15166},
  year={2023}
}

@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{hendrycks2021measuring,
  title={Measuring mathematical problem solving with the math dataset},
  author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
  journal=NIPS,
  year={2021}
}
@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{austin2021program,
  title={Program synthesis with large language models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}

@article{ainsworth2022git,
  title={Git re-basin: Merging models modulo permutation symmetries},
  author={Ainsworth, Samuel K and Hayase, Jonathan and Srinivasa, Siddhartha},
  journal=ICLR,
  year={2023}
}

@article{entezari2021role,
  title={The role of permutation invariance in linear mode connectivity of neural networks},
  author={Entezari, Rahim and Sedghi, Hanie and Saukh, Olga and Neyshabur, Behnam},
  journal=ICLR,
  year={2022}
}


@article{matena2022merging,
  title={Merging models with fisher-weighted averaging},
  author={Matena, Michael S and Raffel, Colin A},
  journal=NIPS,
  year={2022}
}

@article{wortsman2022model,
  title={Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time},
  author={Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Ya and Roelofs, Rebecca and Gontijo-Lopes, Raphael and Morcos, Ari S and Namkoong, Hongseok and Farhadi, Ali and Carmon, Yair and Kornblith, Simon and others},
  journal=ICML,
  year={2022}
}

@article{ilharco2022editing,
  title={Editing models with task arithmetic},
  author={Ilharco, Gabriel and Ribeiro, Marco Tulio and Wortsman, Mitchell and Gururangan, Suchin and Schmidt, Ludwig and Hajishirzi, Hannaneh and Farhadi, Ali},
  journal=ICLR,
  year={2023}
}

@article{dodge2020fine,
  title={Fine-Tuning Pretrained Language Models: Weight Initializations},
  author={Dodge, Jesse and Ilharco, Gabriel and Schwartz, Roy and Farhadi, Ali and Hajishirzi, Hannaneh and Smith, Noah},
  journal={Data Orders, and Early Stopping. arXiv},
  year={2020}
}

@article{jiang2023effective,
  title={Effective and Parameter-Efficient Reusing Fine-Tuned Models},
  author={Jiang, Weisen and Lin, Baijiong and Shi, Han and Zhang, Yu and Kwok, James T and others},
  journal={arXiv preprint arXiv:2310.01886},
  year={2023}
}

@article{chen2023frugalgpt,
  title={Frugalgpt: How to use large language models while reducing cost and improving performance},
  author={Chen, Lingjiao and Zaharia, Matei and Zou, James},
  journal={arXiv preprint arXiv:2305.05176},
  year={2023}
}

@article{shnitzer2023large,
  title={Large language model routing with benchmark datasets},
  author={Shnitzer, Tal and Ou, Anthony and Silva, M{\'\i}rian and Soule, Kate and Sun, Yuekai and Solomon, Justin and Thompson, Neil and Yurochkin, Mikhail},
  journal={arXiv preprint arXiv:2309.15789},
  year={2023}
}

@article{speechless-code-mistral-7b-v1.0,
author = { },
year={2023},
title = {Speechless-Code-Mistral-7b-V1.0},
journal = {\url{https://huggingface.co/uukuguy/speechless-code-mistral-7b-v1.0}},
}

@article{dolphin-2.2.1-mistral-7b,
title = {{Dolphin-2.2.1-Mistral-7B}},
year={2023},
journal = {\url{https://huggingface.co/cognitivecomputations/dolphin-2.2.1-mistral-7b}},
}

@article{Chinese-Mistral-7B-Instruct-v0.1,
author = {},
year={2023},
title = {{Chinese-Mistral-7B-Instruct-v0.1}},
journal = {\url{https://huggingface.co/itpossible/Chinese-Mistral-7B-Instruct-v0.1}},
}

@article{bert-base-multilingual-cased,
author = {},
year={2023},
title = {{bert-base-multilingual-cased}},
journal = {\url{https://huggingface.co/google-bert/bert-base-multilingual-cased}},
}

@article{MetaMath-Mistral-7B,
author = {},
year={2023},
title = {{MetaMath-Mistral-7B}},
journal = {\url{https://huggingface.co/meta-math/MetaMath-Mistral-7B}},
}

@article{Hercules-2.5-Mistral-7B,
author = {},
year={2023},
title = {{Hercules-2.5-Mistral-7B}},
journal = {\url{https://huggingface.co/Locutusque/Hercules-2.5-Mistral-7B}},
}

@article{CollectiveCognition-v1.1-Mistral-7B,
title = {{CollectiveCognition-v1.1-Mistral-7B}},
year={2023},
journal = {\url{https://huggingface.co/teknium/CollectiveCognition-v1.1-Mistral-7B}},
}

@article{Dolphin,
title = {{Dolphin}},
year={2023},
journal = {\url{https://huggingface.co/datasets/cognitivecomputations/dolphin}},
}

@article{WizardLM-evol-instruct-V2-196k,
year={2023},
title = {{WizardLM-evol-instruct-V2-196k}},
journal = {\url{https://huggingface.co/datasets/MaziyarPanahi/WizardLM_evol_instruct_V2_196k}},
}

@article{Wizard-LM-Chinese-instruct-evol,
year={2023},
title = {{Wizard-LM-Chinese-instruct-evol}},
howpublished = {\url{https://huggingface.co/datasets/silk-road/Wizard-LM-Chinese-instruct-evol}},
}

@article{hendrycks2020measuring,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal=ICLR,
  year={2021}
}

@article{huang2024c,
  title={C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models},
  author={Huang, Yuzhen and Bai, Yuzhuo and Zhu, Zhihao and Zhang, Junlei and Zhang, Jinghan and Su, Tangjun and Liu, Junteng and Lv, Chuancheng and Zhang, Yikai and Fu, Yao and others},
  journal=NIPS,
  year={2024}
}

@article{li2023cmmlu,
  title={Cmmlu: Measuring massive multitask language understanding in chinese},
  author={Li, Haonan and Zhang, Yixuan and Koto, Fajri and Yang, Yifei and Zhao, Hai and Gong, Yeyun and Duan, Nan and Baldwin, Timothy},
  journal={arXiv preprint arXiv:2306.09212},
  year={2023}
}

@article{contributors2023opencompass,
  title={Opencompass: A universal evaluation platform for foundation models},
  author={Contributors, OpenCompass},
  journal={GitHub repository},
  year={2023}
}

@inproceedings{zhong2023agieval,
  title={AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models},
  author={Zhong, Wanjun and Cui, Ruixiang and Guo, Yiduo and Liang, Yaobo and Lu, Shuai and Wang, Yanlin and Saied, Amin and Chen, Weizhu and Duan, Nan},
  booktitle={Findings of the Association for Computational Linguistics: NAACL 2024},
  year={2024}
}

@inproceedings{suzgun2022challenging,
  title={Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them},
  author={Suzgun, Mirac and Scales, Nathan and Sch{\"a}rli, Nathanael and Gehrmann, Sebastian and Tay, Yi and Chung, Hyung Won and Chowdhery, Aakanksha and Le, Quoc and Chi, Ed and Zhou, Denny and others},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2023},
  pages={13003--13051},
  year={2023}
}

@article{goddard2024arcee,
  title={Arcee's MergeKit: A Toolkit for Merging Large Language Models},
  author={Goddard, Charles and Siriwardhana, Shamane and Ehghaghi, Malikeh and Meyers, Luke and Karpukhin, Vlad and Benedict, Brian and McQuade, Mark and Solawetz, Jacob},
  journal={arXiv preprint arXiv:2403.13257},
  year={2024}
}

@inproceedings{talmor2018commonsenseqa,
  title={CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge},
  author={Talmor, Alon and Herzig, Jonathan and Lourie, Nicholas and Berant, Jonathan},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={4149--4158},
  year={2019}
}

@inproceedings{joshi2017triviaqa,
  title={TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension},
  author={Joshi, Mandar and Choi, Eunsol and Weld, Daniel S and Zettlemoyer, Luke},
  booktitle={Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1601--1611},
  year={2017}
}


@article{daheim2023model,
      title={Model Merging by Uncertainty-Based Gradient Matching}, 
      author={Nico Daheim and Thomas MÃ¶llenhoff and Edoardo Maria Ponti and Iryna Gurevych and Mohammad Emtiyaz Khan},
      journal=ICLR,
      year={2024},
}

@article{NEURIPS2022_70c26937,
	author = {Matena, Michael S and Raffel, Colin A},
	journal = NIPS,
	title = {Merging Models with Fisher-Weighted Averaging},
	year = {2022},
}

@article{toshniwal2024openmath,
  title   = {OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset},
  author  = {Shubham Toshniwal and Ivan Moshkov and Sean Narenthiran and Daria Gitman and Fei Jia and Igor Gitman},
  year    = {2024},
  journal = {arXiv preprint arXiv: Arxiv-2402.10176}
}

@article{luo2023wizardcoder,
  title={WizardCoder: Empowering Code Large Language Models with Evol-Instruct},
  author={Luo, Ziyang and Xu, Can and Zhao, Pu and Sun, Qingfeng and Geng, Xiubo and Hu, Wenxiang and Tao, Chongyang and Ma, Jing and Lin, Qingwei and Jiang, Daxin},
  journal=ICLR,
  year={2024}
}

@misc{liu2024meswitch,
      title={ME-Switch: A Memory-Efficient Expert Switching Framework for Large Language Models}, 
      author={Jing Liu and Ruihao Gong and Mingyang Zhang and Yefei He and Jianfei Cai and Bohan Zhuang},
      year={2024},
      eprint={2406.09041},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}

@article{mason2024makes,
  title={What Makes a Good Prune? Maximal Unstructured Pruning for Maximal Cosine Similarity},
  author={Mason-Williams, Gabryel and Dahlqvist, Fredrik},
  journal=ICLR,
  year={2024}
}

@inproceedings{klabunde2023towards,
  title={Towards Measuring Representational Similarity of Large Language Models},
  author={Klabunde, Max and Amor, Mehdi Ben and Granitzer, Michael and Lemmerich, Florian},
  booktitle={UniReps: the First Workshop on Unifying Representations in Neural Models},
  year={2023}
}