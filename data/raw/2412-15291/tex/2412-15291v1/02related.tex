\section{Related Work}
\vspace{-0.1in}

Applying LLMs in political science represents a new and rapidly evolving field. 
Although LLMs have revolutionized natural language processing and a variety of other domains, their potential in political science remains largely untapped. Initial studies have demonstrated promising results in election forecasting, policy analysis, and public opinion simulation \cite{smith2023llms, johnson2024political}. However, political science involves complex social dynamics and multilayered causal relationships, which pose significant challenges to effectively utilize LLMs \cite{brown2023challenges}. 
Recent work, e.g., \citet{chen2024decoding}, emphasizes the need for domain-specific fine-tuning to accurately capture the nuances of political discourse, highlighting the limitations of general-purpose LLMs.


LLMs' applications in election forecasting is still in its early stages. 
For instance, the ``Political Campus'' project by \citet{roberts2023political} created a benchmark dataset for evaluating LLM performance on election-related tasks, highlighting both potential and limitations in political forecasting. Similarly, the work by \citet{kim2024sentiment} using LLMs for policy sentiment analysis underscores the need for careful interpretation of results due to the challenges associated with the inherent complexity of political language. 
Our research aims to address these challenges by integrating LLMs with ABMs to predict election outcomes and comprehensively analyze voter behavior. This integrated framework combines macro-level political discourse analysis with micro-level voter behavior modeling, bridging the gap identified by previous studies such as \citet{lopez2023bridging} regarding the disconnect between large-scale text analysis and individual-level decision-making. For a more comprehensive related work, please see Appx. \ref{appx:related}.
\vspace{-0.1in}

