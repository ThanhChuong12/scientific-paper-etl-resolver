@article{smith2023llms,
  title={LLMs in Political Forecasting: A New Frontier},
  author={Smith, John and Doe, Jane},
  journal={Journal of Political Technology},
  volume={15},
  number={2},
  pages={123--145},
  year={2023},
  publisher={Political Science Press}
}

@article{li2024political,
  title={Political-LLM: Large Language Models in Political Science},
  author={Li, Lincan and Li, Jiaqi and Chen, Catherine and Gui, Fred and Yang, Hongjia and Yu, Chenxiao and Wang, Zhengguang and Cai, Jianing and Zhou, Junlong Aaron and Shen, Bolin and others},
  journal={arXiv preprint arXiv:2412.06864},
  year={2024}
}

@article{liu2024drugagent,
  title={DrugAgent: Automating AI-aided Drug Discovery Programming through LLM Multi-Agent Collaboration},
  author={Liu, Sizhe and Lu, Yizhou and Chen, Siyu and Hu, Xiyang and Zhao, Jieyu and Fu, Tianfan and Zhao, Yue},
  journal={arXiv preprint arXiv:2411.15692},
  year={2024}
}

@inproceedings{johnson2024political,
  title={Political Sentiment Analysis using Large Language Models},
  author={Johnson, Emily and Lee, David},
  booktitle={Proceedings of the International Conference on Computational Social Science},
  pages={78--92},
  year={2024},
  organization={ICCSS}
}

@article{brown2023challenges,
  title={Challenges in Applying LLMs to Complex Political Systems},
  author={Brown, Robert},
  journal={Computational Politics Review},
  volume={8},
  number={3},
  pages={301--320},
  year={2023},
  publisher={Digital Politics Association}
}

@article{chen2024decoding,
  title={Decoding Political Rhetoric: An LLM-based Approach},
  author={Chen, Wei and Rodriguez, Maria},
  journal={AI in Governance},
  volume={5},
  number={1},
  pages={45--67},
  year={2024},
  publisher={AI Governance Institute}
}

@inproceedings{wilson2024robust,
  title={Robust Political Language Models: Mitigating Bias and Misinterpretation},
  author={Wilson, Sarah and Taylor, James and Patel, Raj},
  booktitle={Proceedings of the Conference on AI Ethics in Politics},
  pages={112--128},
  year={2024},
  organization={AIEP}
}

@article{thompson2023ethical,
  title={Ethical Considerations in AI-driven Political Analysis},
  author={Thompson, Alice},
  journal={Journal of AI and Society},
  volume={12},
  number={4},
  pages={567--589},
  year={2023},
  publisher={Societal Impact of AI Association}
}

@book{downs1957economic,
  title={An Economic Theory of Democracy},
  author={Downs, Anthony},
  year={1957},
  publisher={Harper \& Row}
}

@article{black1948rationale,
  title={On the Rationale of Group Decision-making},
  author={Black, Duncan},
  journal={Journal of Political Economy},
  volume={56},
  number={1},
  pages={23--34},
  year={1948},
  publisher={University of Chicago Press}
}

@book{lewis1992forecasting,
  title={Forecasting Elections},
  author={Lewis-Beck, Michael S and Rice, Tom W},
  year={1992},
  publisher={Georgetown University Press}
}

@article{erikson2016forecasting,
  title={Forecasting US presidential elections using economic and noneconomic fundamentals},
  author={Erikson, Robert S and Wlezien, Christopher},
  journal={PS: Political Science \& Politics},
  volume={49},
  number={4},
  pages={669--672},
  year={2016},
  publisher={Cambridge University Press}
}

@article{gao2022agent,
  title={Agent-based Modeling for Election Prediction: A Network Approach},
  author={Gao, Lin and Zhang, Wei and Liu, Jian},
  journal={Computational Social Networks},
  volume={9},
  number={1},
  pages={1--22},
  year={2022},
  publisher={Springer}
}

@inproceedings{lemos2019agent,
  title={An Agent-based Model of Voter Turnout and Preference Formation},
  author={Lemos, Carlos and Coelho, Helder and Lopes, Rui J},
  booktitle={Proceedings of the Social Simulation Conference},
  pages={245--256},
  year={2019},
  organization={SSC}
}

@article{collins2023media,
  title={Media Influence and Peer Effects in Voter Decision-Making: An Agent-Based Approach},
  author={Collins, Megan and Martinez, Juan},
  journal={Political Communication Quarterly},
  volume={31},
  number={2},
  pages={178--201},
  year={2023},
  publisher={Political Communication Association}
}

@inproceedings{zhang2024hybrid,
  title={A Hybrid Framework for Election Forecasting: Integrating LLMs and ABMs},
  author={Zhang, Yun and Wang, Li and Kumar, Anil},
  booktitle={Proceedings of the International Conference on Computational Social Science},
  pages={301--315},
  year={2024},
  organization={ICCSS}
}

@article{anderson2023nuances,
  title={Navigating the Nuances: Challenges of Political Language for LLMs},
  author={Anderson, Emma and Taylor, Michael},
  journal={Computational Linguistics in Politics},
  volume={7},
  number={3},
  pages={412--435},
  year={2023},
  publisher={Association for Political NLP}
}

@inproceedings{williams2024challenges,
  title={Challenges and Opportunities in Applying LLMs to Political Discourse Analysis},
  author={Williams, Oliver and Johnson, Sophia and Lee, Kevin},
  booktitle={Proceedings of the Workshop on NLP for Political Science},
  pages={56--70},
  year={2024},
  organization={ACL}
}

@article{roberts2023political,
  title={Political Campus: A Benchmark Dataset for LLM-based Election Prediction},
  author={Roberts, Jennifer and Brown, Thomas and Garcia, Elena},
  journal={Data in Politics},
  volume={4},
  number={2},
  pages={89--112},
  year={2023},
  publisher={Political Data Science Association}
}

@inproceedings{kim2024sentiment,
  title={Large Language Models for Policy Sentiment Analysis: A Case Study},
  author={Kim, Soo-Jin and Patel, Rahul},
  booktitle={Proceedings of the Conference on AI in Public Policy},
  pages={201--215},
  year={2024},
  organization={AIPP}
}

@article{lopez2023bridging,
  title={Bridging the Gap: Integrating Macro-level Language Models with Micro-level Voter Behavior},
  author={Lopez, Carmen and Singh, Rajesh},
  journal={Journal of Computational Political Science},
  volume={6},
  number={4},
  pages={523--547},
  year={2023},
  publisher={Computational Politics Society}
}

@inproceedings{thompson2023multimodal,
  title={Multi-modal Political Prediction Models: Integrating Text, Data, and Behavior},
  author={Thompson, Mark and Lee, Sarah and Chen, Wei},
  booktitle={Proceedings of the International Conference on Machine Learning for Politics},
  pages={78--93},
  year={2023},
  organization={ICLMP}
}

@techreport{consortium2024ethical,
  title={Ethical Considerations in AI-driven Political Analysis and Forecasting},
  author={{Ethical AI in Politics Consortium}},
  year={2024},
  institution={Global Institute for AI Ethics},
  type={White Paper},
  number={2024-03}
}

@misc{chang2024llmsgeneratestructurallyrealistic,
      title={LLMs generate structurally realistic social networks but overestimate political homophily}, 
      author={Serina Chang and Alicja Chaszczewicz and Emma Wang and Maya Josifovska and Emma Pierson and Jure Leskovec},
      year={2024},
      eprint={2408.16629},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url={https://arxiv.org/abs/2408.16629}, 
}

@misc{ross2024llmeconomicusmappingbehavioral,
      title={LLM economicus? Mapping the Behavioral Biases of LLMs via Utility Theory}, 
      author={Jillian Ross and Yoon Kim and Andrew W. Lo},
      year={2024},
      eprint={2408.02784},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.02784}, 
}

@inproceedings{zhang-etal-2024-exploring,
    title = "Exploring Collaboration Mechanisms for {LLM} Agents: A Social Psychology View",
    author = "Zhang, Jintian  and
      Xu, Xin  and
      Zhang, Ningyu  and
      Liu, Ruibo  and
      Hooi, Bryan  and
      Deng, Shumin",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.782/",
    doi = "10.18653/v1/2024.acl-long.782",
    pages = "14544--14607",
    abstract = "As Natural Language Processing (NLP) systems are increasingly employed in intricate social environments, a pressing query emerges: *Can these NLP systems mirror human-esque collaborative intelligence, in a multi-agent society consisting of multiple large language models (LLMs)?* This paper probes the collaboration mechanisms among contemporary NLP systems by melding practical experiments with theoretical insights. We fabricate four unique {\textquoteleft}societies' comprised of LLM agents, where each agent is characterized by a specific {\textquoteleft}trait' (easy-going or overconfident) and engages in collaboration with a distinct {\textquoteleft}thinking pattern' (debate or reflection). Through evaluating these multi-agent societies on three benchmark datasets, we discern that certain collaborative strategies not only outshine previous top-tier approaches but also optimize efficiency (using fewer API tokens). Moreover, our results further illustrate that LLM agents manifest human-like social behaviors, such as conformity and consensus reaching, mirroring foundational social psychology theories. In conclusion, we integrate insights from social psychology to contextualize the collaboration of LLM agents, inspiring further investigations into the collaboration mechanism for LLMs. We commit to sharing our code and datasets, hoping to catalyze further research in this promising avenue."
}

@inproceedings{zhang-lu-2024-adarefiner,
    title = "{A}da{R}efiner: Refining Decisions of Language Models with Adaptive Feedback",
    author = "Zhang, Wanpeng  and
      Lu, Zongqing",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2024",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-naacl.50/",
    doi = "10.18653/v1/2024.findings-naacl.50",
    pages = "782--799",
    abstract = "Large Language Models (LLMs) have demonstrated significant success across various domains. However, their application in complex decision-making tasks frequently necessitates intricate prompt engineering or fine-tuning, leading to challenges in unseen downstream tasks and heavy demands on computational resources. Meanwhile, Reinforcement Learning (RL) has been recognized as effective in decision-making problems but struggles in environments with sparse rewards, such as open-world games. To overcome these challenges, we introduce AdaRefiner, a novel framework designed to enhance the synergy between LLMs and RL feedback. The key component of AdaRefiner is a lightweight Adapter Language Model (LM), which automatically refines task comprehension based on feedback from RL agents. This method mitigates the need for intricate prompt engineering and intensive LLM fine-tuning while maintaining the LLMs' generalization abilities and enhancing their decision-making capabilities in downstream tasks. Empirical evaluations of AdaRefiner on 22 diverse tasks within the open-world game \textit{Crafter} have demonstrated its superior effectiveness, especially in guiding agents towards higher-level and common-sense skills. Our work makes contributions to the automatic self-refinement of LLMs with RL feedback, offering a more adaptable and efficient solution for complex decision-making problems. The code is available at https://github.com/PKU-RL/AdaRefiner."
}

@inproceedings{feng2023pretraining,
  title={From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models},
  author={Feng, Shangbin and Park, Chan Young and Liu, Yuhan and Tsvetkov, Yulia},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={11737--11762},
  year={2023}
}

@article{treier2009nature,
  title={The nature of political ideology in the contemporary electorate},
  author={Treier, Shawn and Hillygus, D Sunshine},
  journal={Public Opinion Quarterly},
  volume={73},
  number={4},
  pages={679--703},
  year={2009},
  publisher={Oxford University Press}
}

@misc{liu2024evaluatinglargelanguagemodel,
      title={Evaluating Large Language Model Biases in Persona-Steered Generation}, 
      author={Andy Liu and Mona Diab and Daniel Fried},
      year={2024},
      eprint={2405.20253},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.20253}, 
}

@inproceedings{li2020sync,
  title={Sync: A copula based framework for generating synthetic data from aggregated sources},
  author={Li, Zheng and Zhao, Yue and Fu, Jialin},
  booktitle={2020 International Conference on Data Mining Workshops (ICDMW)},
  pages={571--578},
  year={2020},
  organization={IEEE}
}

@misc{ANES2020,
  title = {ANES 2020 Time Series Study Full Release},
  author = {American National Election Studies},
  year = {2022},
  note = {[Dataset and documentation]. February 10, 2022 version},
  url = {https://www.electionstudies.org},
  howpublished = {\url{https://www.electionstudies.org}}
}

@misc{ANES2016,
  title = {ANES 2016 Time Series Study Full Release},
  author = {American National Election Studies},
  year = {2019},
  note = {[Dataset and documentation]. September 4, 2019 version},
  url = {https://www.electionstudies.org},
  howpublished = {\url{https://www.electionstudies.org}}
}

@article{chia2023artificial,
  title={Artificial intelligence generated synthetic datasets as the remedy for data scarcity in water quality index estimation},
  author={Chia, Min Yan and Koo, Chai Hoon and Huang, Yuk Feng and Di Chan, Wei and Pang, Jia Yin},
  journal={Water Resources Management},
  volume={37},
  number={15},
  pages={6183--6198},
  year={2023},
  publisher={Springer}
}

@article{potluru2023synthetic,
  title={Synthetic data applications in finance},
  author={Potluru, Vamsi K and Borrajo, Daniel and Coletta, Andrea and Dalmasso, Niccol{\`o} and El-Laham, Yousef and Fons, Elizabeth and Ghassemi, Mohsen and Gopalakrishnan, Sriram and Gosai, Vikesh and Krea{\v{c}}i{\'c}, Eleonora and others},
  journal={arXiv preprint arXiv:2401.00081},
  year={2023}
}

@inproceedings{merinov2023behaviour,
  title={Behaviour-aware Tourist Profiles Data Generation.},
  author={Merinov, Pavel and Massimo, David and Ricci, Francesco},
  booktitle={IIR},
  pages={3--8},
  year={2023}
}

@article{stevenson2023creating,
  title={Creating predictive social impact models of engineered products using synthetic populations},
  author={Stevenson, Phillip D and Mattson, Christopher A and Dahlin, Eric C and Salmon, John L},
  journal={Research in Engineering Design},
  volume={34},
  number={4},
  pages={461--476},
  year={2023},
  publisher={Springer}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{bommasani2021opportunities,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}

@article{zhang2022large,
  title={Large language models encode clinical knowledge},
  author={Zhang, Karan and others},
  journal={Nature},
  volume={618},
  number={7965},
  pages={732--739},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{chalkidis2022lexglue,
  title={LexGLUE: A Benchmark Dataset for Legal Language Understanding in English},
  author={Chalkidis, Ilias and others},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics},
  pages={4310--4330},
  year={2022}
}

@article{yang2022survey,
  title={A survey on deep learning for text-to-music generation},
  author={Yang, Yi-Hsuan and others},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={30},
  pages={1808--1837},
  year={2022},
  publisher={IEEE}
}

@article{xu2022deep,
  title={Deep learning in political science: Introducing rdlmatio to estimate the ideological positions of political actors using roll-call data},
  author={Xu, Yilang},
  journal={Political Analysis},
  volume={30},
  number={4},
  pages={458--467},
  year={2022},
  publisher={Cambridge University Press}
}

@article{haq2023large,
  title={Large language models for political science research},
  author={Haq, Emaad and others},
  journal={Political Analysis},
  pages={1--21},
  year={2023},
  publisher={Cambridge University Press}
}

@article{lerer2022political,
  title={Political sentiment analysis: A comparison of lexicon-based and machine learning approaches},
  author={Lerer, Thomas and others},
  journal={Social Science Computer Review},
  volume={40},
  number={5},
  pages={1135--1154},
  year={2022},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@article{graefe2014accuracy,
  title={Accuracy of vote expectation surveys in forecasting elections},
  author={Graefe, Andreas},
  journal={Public Opinion Quarterly},
  volume={78},
  number={1},
  pages={204--232},
  year={2014},
  publisher={Oxford University Press}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@book{holbrook2016forecasting,
  title={Forecasting US presidential elections},
  author={Holbrook, Thomas M},
  year={2016},
  publisher={Rowman \& Littlefield}
}

@article{evans2023optimizing,
  title={Optimizing Transformer-Based Models for Improved Election Result Predictions},
  author={Evans, Alexander and Johnson, Sarah},
  journal={Journal of Political Analysis},
  volume={41},
  number={2},
  pages={215--232},
  year={2023},
  publisher={Oxford University Press}
}

@article{dodge2019show,
  title={Show your work: Improved reporting of experimental results},
  author={Dodge, Jesse and others},
  journal={arXiv preprint arXiv:1909.03004},
  year={2019}
}

@article{bender2021dangers,
  title={On the dangers of stochastic parrots: Can language models be too big?},
  author={Bender, Emily M and others},
  journal={Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
  pages={610--623},
  year={2021}
}

@article{van2022political,
  title={Political Science in the Era of Artificial Intelligence},
  author={Van der Meer, Tom WG and others},
  journal={Political Analysis},
  volume={30},
  number={3},
  pages={293--305},
  year={2022},
  publisher={Cambridge University Press}
}

@book{levendusky2009partisan,
  title={The Partisan Sort: How Liberals Became Democrats and Conservatives Became Republicans},
  author={Levendusky, Matthew S.},
  year={2009},
  publisher={University of Chicago Press}
}

@article{abramowitz2008polarization,
  title={Is polarization a myth?},
  author={Abramowitz, Alan I. and Saunders, Kyle L.},
  journal={The Journal of Politics},
  volume={70},
  number={2},
  pages={542--555},
  year={2008},
  publisher={Cambridge University Press}
}

@misc{wang2024generalizationvsmemorizationtracing,
      title={Generalization v.s. Memorization: Tracing Language Models' Capabilities Back to Pretraining Data}, 
      author={Xinyi Wang and Antonis Antoniades and Yanai Elazar and Alfonso Amayuelas and Alon Albalak and Kexun Zhang and William Yang Wang},
      year={2024},
      eprint={2407.14985},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.14985}, 
}

@misc{xie2024largelanguagemodelagents,
      title={Can Large Language Model Agents Simulate Human Trust Behaviors?}, 
      author={Chengxing Xie and Canyu Chen and Feiran Jia and Ziyu Ye and Kai Shu and Adel Bibi and Ziniu Hu and Philip Torr and Bernard Ghanem and Guohao Li},
      year={2024},
      eprint={2402.04559},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2402.04559}, 
}

@article{10.1371/journal.pone.0270194,
    doi = {10.1371/journal.pone.0270194},
    author = {Gao, Ming AND Wang, Zhongyuan AND Wang, Kai AND Liu, Chenhui AND Tang, Shiping},
    journal = {PLOS One},
    publisher = {Public Library of Science},
    title = {Forecasting elections with agent-based modeling: Two live experiments},
    year = {2022},
    month = {06},
    volume = {17},
    url = {https://doi.org/10.1371/journal.pone.0270194},
    pages = {1-11},
    abstract = {Election forecasting has been traditionally dominated by subjective surveys and polls or methods centered upon them. We have developed a novel platform for forecasting elections based on agent-based modeling (ABM), which is entirely independent from surveys and polls. The platform uses statistical results from objective data along with simulation models to capture how voters have voted in past elections and how they are likely to vote in an upcoming election. We screen for models that can reproduce results that are very close to the actual results of historical elections and then deploy these selected models to forecast an upcoming election with simulations by combining extrapolated data from historical demographic record and more updated data on economic growth, employment, shock events, and other factors. Here, we report the results of two recent experiments of real-time election forecasting: the 2020 general election in Taiwan and six states in the 2020 general election in the United States. Our mostly objective method using ABM may transform how elections are forecasted and studied.},
    number = {6},

}

@misc{pew2014polarization,
  title={Political Polarization in the American Public},
  author={{Pew Research Center}},
  year={2014},
  month={June},
  institution={Pew Research Center},
  note={\url{https://www.pewresearch.org}}
}

@article{bafumi2009new,
  title={A new partisan voter},
  author={Bafumi, Joseph and Shapiro, Robert Y.},
  journal={The Journal of Politics},
  volume={71},
  number={1},
  pages={1--24},
  year={2009},
  publisher={Cambridge University Press}
}

@misc{wikipedia_swing_state,
  title = {Swing state},
  author = {Wikipedia contributors},
  year = {2024},
  url = {https://en.wikipedia.org/wiki/Swing_state},
  note = {Accessed: 2024-10-12}
}

@article{roberts2020much,
  title={How much knowledge can you pack into the parameters of a language model?},
  author={Roberts, Adam and Raffel, Colin and Shazeer, Noam},
  journal={arXiv preprint arXiv:2002.08910},
  year={2020}
}

@inproceedings{hu-collier-2024-quantifying,
    title = "Quantifying the Persona Effect in {LLM} Simulations",
    author = "Hu, Tiancheng  and
      Collier, Nigel",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.554",
    doi = "10.18653/v1/2024.acl-long.554",
    pages = "10289--10307",
    abstract = "Large language models (LLMs) have shown remarkable promise in simulating human language and behavior. This study investigates how integrating persona variables{---}demographic, social, and behavioral factors{---}impacts LLMs{'} ability to simulate diverse perspectives. We find that persona variables account for {\textless}10{\%} variance in annotations in existing subjective NLP datasets. Nonetheless, incorporating persona variables via prompting in LLMs provides modest but statistically significant improvements. Persona prompting is most effective in samples where many annotators disagree, but their disagreements are relatively minor. Notably, we find a linear relationship in our setting: the stronger the correlation between persona variables and human annotations, the more accurate the LLM predictions are using persona prompting. In a zero-shot setting, a powerful 70b model with persona prompting captures 81{\%} of the annotation variance achievable by linear regression trained on ground truth annotations. However, for most subjective NLP datasets, where persona variables have limited explanatory power, the benefits of persona prompting are limited.",
}

@inproceedings{zhou2020evaluating,
  title={Evaluating commonsense in pre-trained language models},
  author={Zhou, Xuhui and Zhang, Yue and Cui, Leyang and Huang, Dandan},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  pages={9733--9740},
  year={2020}
}

@article{alkhamissi2022review,
  title={A review on language models as knowledge bases},
  author={AlKhamissi, Badr and Li, Millicent and Celikyilmaz, Asli and Diab, Mona and Ghazvininejad, Marjan},
  journal={arXiv preprint arXiv:2204.06031},
  year={2022}
}

@article{ziems2024can,
  title={Can large language models transform computational social science?},
  author={Ziems, Caleb and Held, William and Shaikh, Omar and Chen, Jiaao and Zhang, Zhehao and Yang, Diyi},
  journal={Computational Linguistics},
  volume={50},
  number={1},
  pages={237--291},
  year={2024},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@inproceedings{
zhou2023large,
title={Large Language Models are Human-Level Prompt Engineers},
author={Yongchao Zhou and Andrei Ioan Muresanu and Ziwen Han and Keiran Paster and Silviu Pitis and Harris Chan and Jimmy Ba},
booktitle={The Eleventh International Conference on Learning Representations},
year={2023},
url={https://openreview.net/forum?id=92gvk82DE-}
}

@article{Bisbee_Clinton_Dorff_Kenkel_Larson_2024, title={Synthetic Replacements for Human Survey Data? The Perils of Large Language Models}, DOI={10.1017/pan.2024.5}, journal={Political Analysis}, author={Bisbee, James and Clinton, Joshua D. and Dorff, Cassy and Kenkel, Brenton and Larson, Jennifer M.}, year={2024}, pages={1–16}}

@article{Argyle_Busby_Fulda_Gubler_Rytting_Wingate_2023, 
title={Out of One, Many: Using Language Models to Simulate Human Samples}, volume={31}, DOI={10.1017/pan.2023.2}, number={3}, journal={Political Analysis}, author={Argyle, Lisa P. and Busby, Ethan C. and Fulda, Nancy and Gubler, Joshua R. and Rytting, Christopher and Wingate, David}, year={2023}, pages={337–351}} <div></div>

@data{DVN/JPV20K_2022,
author = {Argyle, Lisa P. and Busby, Ethan C. and Fulda, Nancy and Gubler, Joshua R. and Rytting, Christopher and Wingate, David},
publisher = {Harvard Dataverse},
title = {{Replication Data for: ``Out of One, Many: Using Language Models to Simulate Human Samples''}},
UNF = {UNF:6:Fn2jZOKY/cgo+/5z40iR4Q==},
year = {2022},
version = {V1},
doi = {10.7910/DVN/JPV20K},
url = {https://doi.org/10.7910/DVN/JPV20K}
}

@inproceedings{Li2020COPOD,
title={COPOD: copula-based outlier detection},
author={Li, Zheng and Zhao, Yue and Botta, Nicola and Ionescu, Cezar and Hu, Xiyang},
booktitle={2020 IEEE International Conference on Data Mining (ICDM)},
pages={1118--1123},
year={2020},
organization={IEEE}
}

@article{Potluru2023SyntheticFinance,
title={Synthetic data applications in finance},
author={Potluru, Vijay Kumar and Borrajo, Diego and Coletta, Andrea and Dalmasso, Niccolò and Das, Sumanta and Gupta, Shashi and Harmon, Scott and Kakkar, Nimish and Meng, Yue and Natarajan, Prabhakar and others},
journal={arXiv preprint arXiv:2301.07827},
year={2023}
}

@article{Borisov2022DeepTabularSurvey,
title={Deep neural networks and tabular data: A survey},
author={Borisov, Vadim and Leemann, Thomas and Seßler, Katharina and Haug, Jonas and Pawelczyk, Martin and Kasneci, Gjergji},
journal={IEEE Transactions on Neural Networks and Learning Systems},
volume={34},
number={4},
pages={1686--1711},
year={2022},
publisher={IEEE}
}

@article{Sichani2024SyntheticHealth,
title={Creating High-Quality Synthetic Health Data: Framework for Model Development and Validation},
author={Sichani, Elham Karimi and Smith, Alexandra and El Emam, Khaled and Goldenberg, Anna},
journal={JMIR Formative Research},
volume={8},
pages={e50704},
year={2024},
publisher={JMIR Publications Inc., Toronto, Canada}
}

@misc{pew2021biden,
  author       = {Pew Research Center},
  title        = {Behind Biden’s 2020 Victory},
  year         = {2021},
  month        = {June},
  url          = {https://www.pewresearch.org/politics/2021/06/30/behind-bidens-2020-victory/?utm_source=chatgpt.com},
  note         = {Accessed: 2023-12-08}
}

@misc{zhang2024electionsimmassivepopulationelection,
      title={ElectionSim: Massive Population Election Simulation Powered by Large Language Model Driven Agents}, 
      author={Xinnong Zhang and Jiayu Lin and Libo Sun and Weihong Qi and Yihang Yang and Yue Chen and Hanjia Lyu and Xinyi Mou and Siming Chen and Jiebo Luo and Xuanjing Huang and Shiping Tang and Zhongyu Wei},
      year={2024},
      eprint={2410.20746},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.20746}, 
}

@misc{federal2020elections,
  author       = {{Federal Election Commission}},
  title        = {Federal Elections 2020: Election Results for the U.S. President, the U.S. Senate and the U.S. House of Representatives},
  year         = {2021},
  publisher    = {Federal Election Commission},
  url          = {https://www.fec.gov/resources/cms-content/documents/federalelections2020.pdf},
  note         = {Accessed: 2024-10-01}
}

@misc{nbc2024election,
  author       = {{NBC News}},
  title        = {2024 Presidential Election Results},
  year         = {2024},
  publisher    = {NBC News},
  url          = {https://www.nbcnews.com/politics/2024-presidential-election},
  note         = {Accessed: 2024-12-11}
}

@misc{jenny2024exploringjunglebiaspolitical,
      title={Exploring the Jungle of Bias: Political Bias Attribution in Language Models via Dependency Analysis}, 
      author={David F. Jenny and Yann Billeter and Mrinmaya Sachan and Bernhard Schölkopf and Zhijing Jin},
      year={2024},
      eprint={2311.08605},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.08605}, 
}

@misc{li2024politicalllmlargelanguagemodels,
      title={Political-LLM: Large Language Models in Political Science}, 
      author={Lincan Li and Jiaqi Li and Catherine Chen and Fred Gui and Hongjia Yang and Chenxiao Yu and Zhengguang Wang and Jianing Cai and Junlong Aaron Zhou and Bolin Shen and Alex Qian and Weixin Chen and Zhongkai Xue and Lichao Sun and Lifang He and Hanjie Chen and Kaize Ding and Zijian Du and Fangzhou Mu and Jiaxin Pei and Jieyu Zhao and Swabha Swayamdipta and Willie Neiswanger and Hua Wei and Xiyang Hu and Shixiang Zhu and Tianlong Chen and Yingzhou Lu and Yang Shi and Lianhui Qin and Tianfan Fu and Zhengzhong Tu and Yuzhe Yang and Jaemin Yoo and Jiaheng Zhang and Ryan Rossi and Liang Zhan and Liang Zhao and Emilio Ferrara and Yan Liu and Furong Huang and Xiangliang Zhang and Lawrence Rothenberg and Shuiwang Ji and Philip S. Yu and Yue Zhao and Yushun Dong},
      year={2024},
      eprint={2412.06864},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.06864}, 
}

@misc{park2024generativeagentsimulations1000,
      title={Generative Agent Simulations of 1,000 People}, 
      author={Joon Sung Park and Carolyn Q. Zou and Aaron Shaw and Benjamin Mako Hill and Carrie Cai and Meredith Ringel Morris and Robb Willer and Percy Liang and Michael S. Bernstein},
      year={2024},
      eprint={2411.10109},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2411.10109}, 
}

@article{ross2024llm,
  title={Llm economicus? Mapping the behavioral biases of llms via utility theory},
  author={Ross, Jillian and Kim, Yoon and Lo, Andrew W},
  journal={arXiv preprint arXiv:2408.02784},
  year={2024}
}

@article{wu2023large,
  title={Large language models can be used to estimate the latent positions of politicians},
  author={Wu, Patrick Y and Nagler, Jonathan and Tucker, Joshua A and Messing, Solomon},
  journal={arXiv preprint arXiv:2303.12057},
  year={2023}
}

@article{zhou2024real,
  title={Is this the real life? Is this just fantasy? The misleading success of simulating social interactions with LLMs},
  author={Zhou, Xuhui and Su, Zhe and Eisape, Tiwalayo and Kim, Hyunwoo and Sap, Maarten},
  journal={arXiv preprint arXiv:2403.05020},
  year={2024}
}

@inproceedings{aher2023using,
  title={Using large language models to simulate multiple humans and replicate human subject studies},
  author={Aher, Gati V and Arriaga, Rosa I and Kalai, Adam Tauman},
  booktitle={International Conference on Machine Learning},
  pages={337--371},
  year={2023},
  organization={PMLR}
}

@inproceedings{chuang2024simulating,
  title={Simulating Opinion Dynamics with Networks of LLM-based Agents},
  author={Chuang, Yun-Shiuan and Goyal, Agam and Harlalka, Nikunj and Suresh, Siddharth and Hawkins, Robert and Yang, Sijia and Shah, Dhavan and Hu, Junjie and Rogers, Timothy},
  booktitle={Findings of the Association for Computational Linguistics: NAACL 2024},
  pages={3326--3346},
  year={2024}
}

@article{bornschier2021us,
  title={How ``us''' and ``them''' relates to voting behavior---social structure, social identities, and electoral choice},
  author={Bornschier, Simon and H{\"a}usermann, Silja and Zollinger, Delia and Colombo, C{\'e}line},
  journal={Comparative Political Studies},
  volume={54},
  number={12},
  pages={2087--2122},
  year={2021},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}

@article{taubenfeld2024systematic,
  title={Systematic biases in LLM simulations of debates},
  author={Taubenfeld, Amir and Dover, Yaniv and Reichart, Roi and Goldstein, Ariel},
  journal={arXiv preprint arXiv:2402.04049},
  year={2024}
}

@article{jiang2024donald,
  title={Donald Trumps in the Virtual Polls: Simulating and Predicting Public Opinions in Surveys Using Large Language Models},
  author={Jiang, Shapeng and Wei, Lijia and Zhang, Chen},
  journal={arXiv preprint arXiv:2411.01582},
  year={2024}
}

@article{baker2024simulating,
  title={Simulating the us senate: An llm-driven agent approach to modeling legislative behavior and bipartisanship},
  author={Baker, Zachary R and Azher, Zarif L},
  journal={arXiv preprint arXiv:2406.18702},
  year={2024}
}

@article{fisher2025political,
  title={Political Neutrality in AI is Impossible-But Here is How to Approximate it},
  author={Fisher, Jillian and Appel, Ruth E and Park, Chan Young and Potter, Yujin and Jiang, Liwei and Sorensen, Taylor and Feng, Shangbin and Tsvetkov, Yulia and Roberts, Margaret E and Pan, Jennifer and others},
  journal={arXiv preprint arXiv:2503.05728},
  year={2025}
}