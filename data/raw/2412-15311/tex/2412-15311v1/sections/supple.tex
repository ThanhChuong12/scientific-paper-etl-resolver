 % !TEX root = ../main.tex

\newpage
\appendix
\onecolumn

\renewcommand{\thesection}{\Alph{section}} 
\renewcommand{\thetable}{A\arabic{table}}
\renewcommand{\thefigure}{A\arabic{figure}}

\clearpage
 
\section{Comparisons} 
\label{sec:comparison}
Below is a brief introduction of the comparisons used in our experiments.

 \paragraph{ERM}
 Given a loss function $\ell(\cdot)$, the objective of empirical risk minimization is optimizing the following loss over training data: 
 \begin{align}
    %  \min_\theta \Big\{ \mathcal{L}_\text{ERM}(f_\theta) := \mathbb{E}_{(x,y,a)\sim P}[\ell(f_\theta(x), y)] \Big\}
    \min_\theta \Big\{\frac{1}{n}\sum_{i=1}^n \ell(f_\theta(x_i), y_i) \Big\}.
 \end{align}
%  where $P$ is the empirical distribution over training data.
 
 \paragraph{Class reweighting (CR)}
 To mitigate the class imbalance issue, we can simply reweight the samples based on the inverse of class frequency in the training split, 
  \begin{align}
    \min_\theta \Big\{\frac{1}{n}\sum_{i=1}^n \omega_i \ell(f_\theta(x_i), y_i) \Big\}~~\text{where}~~\omega_i = \frac{n}{\sum_j \mathds{1}(y_j=y_i)}.
 \end{align}
  
 \paragraph{LfF}
Motivated by the observation that bias-aligned samples are more easily learned, LfF~\cite{LfF} simultaneously trains a pair of neural network $(f_B, f_D)$.
The biased model $f_B$ is trained with generalized cross-entropy loss which intends to amplify bias, while the debiased model $f_D$ is trained with a standard cross-entropy loss, where each sample $(x_i, y_i)$ is reweighted by the following relative difficulty score:
\begin{align}
%    \omega_i = \frac{\log f_B(y_i|x_i)}{\log f_B(y_i|x_i) + \log f_D(y_i|x_i)}.
    \omega_i = \frac{\ell(f^B_\theta(x_i), y_i)}{\ell(f^B_\theta(x_i), y_i) + \ell(f^D_\theta(x_i), y_i)}.
\end{align}

 

 \paragraph{JTT}
 JTT~\cite{JTT} consists of two-stage procedures. In the first stage, JTT trains a standard ERM model $\hat{f}(\cdot)$ for several epochs and identifies an error set $E$ of training examples that are misclassified:
 \begin{align}
E := \{(x_i, y_i)~~\text{s.t.}~\hat{f}(x_i) \neq y_i \}.
 \end{align}
 Next, they train a final model $f_\theta(\cdot)$ by upweighting the examples in the error set $E$ as
 \begin{align}
      \min_\theta \Big\{  \lambda_\text{up}\sum_{(x,y)\in E}\ell(f_\theta(x), y) + \sum_{(x,y)\notin E}\ell(f_\theta(x), y) \Big\}.
 \end{align}
 
 
 
 \paragraph{Group DRO}
%  While ERM is the standard way and generally works well, it often learns spurious correlation and consequently, it leads to poor robust accuracy.
%  To tackle this problem, Group DRO aims to minimize the worst-group loss formulated as:
Group DRO~\cite{GroupDRO} aims to minimize the empirical worst-group loss formulated as:
 \begin{align}
  \min_\theta \Big\{
     \max_{g\in \mathcal{G}} \frac{1}{n_g} \sum_{i|g_i=g}^{n_g} \ell(f_\theta(x_i), y_i) \Big\}
 \end{align}
 where $n_g$ is the number of samples assigned to $g^\text{th}$ group.
 Unlike previous approaches, group DRO requires group annotations $g=(y,a)$ on the training split.
%  Unlike Group DRO, we basically assume that we do not have group annotations in training data, and we are only given a small validation set with group annotations.

\paragraph{Group reweighting (GR)}
 Using group annotations, we can extend class reweighting method to group reweighting one based on the inverse of group frequency in the training split, \ie,
%  $\omega_i = n/{\sum_j \mathds{1}(y_j=y_i, a_j=a_i)}$.
   \begin{align}
   & \min_\theta \Big\{\frac{1}{n}\sum_{i=1}^n \omega_i \ell(f_\theta(x_i), y_i) \Big\} \nonumber \\
    &~~~~~~~~~~ \text{where}~~ \omega_i = \frac{n}{\sum_j \mathds{1}(y_j=y_i, a_j=a_i)}
  \end{align}
  
  \paragraph{SUBY/SUBG} To mitigate the data imbalance issue, SUBY subsample majority classes, so all classes have the same size with the smallest class on the training dataset, as in~\cite{idrissi2022simple}.
  Similarly, SUBG subsample majority groups.
 

  


% \subsection{Average accuracy on the Waterbirds dataset}
% For the average accuracy, previous works~\cite{JTT,GroupDRO} reported the weighted average accuracy on the test split of the Waterbirds dataset, where the weights correspond to the proportion of each group in training and validation split, while they reported the original average accuracy (no weight) on the CelebA dataset.
%In our paper, to avoid confusion, we report the original average accuracy (no weight) for all comparisons in all datasets.
% 
 




\section {Attribute-specific Robust Scaling with Group Supervision}
\label{sec:ars}
If the supervision of group (spurious-attribute) information can be utilized during our robust scaling, it will provide flexibility to further improve the performance.
To this end, we first partition the examples based on the values of spurious attributes and find the optimal scaling factors for each partition separately.
Like as the original robust scaling procedure, we obtain the optimal scaling factors for each partition in the validation split and apply them to the test split.
However, this partition-wise scaling is basically unavailable because we do not know the spurious attribute values of the examples in the test split and thus cannot partition them, 
In other words, we need to estimate the spurious-attribute values in the test split for partitioning.
To conduct attribute-specific robust scaling (ARS), we follow a simple algorithm described below:

\begin{enumerate}
\item Partition the examples in the validation split by the values of the spurious attribute.
\item Find the optimal scaling factors for each partition in the validation split. 
\item Train an independent estimator model to classify spurious attribute. 
\item Estimate the spurious attribute values of the examples in the test split using the estimator, and partition the test samples according to their estimated spurious attribute values. 
\item For each sample in the test split, apply the optimal scaling factors obtained in step 2 based on its partition.
\end{enumerate}

To find a set of scale factors corresponding to each partition, we adopt a na\"ive greedy algorithm that performed in one partition at a time.
This attribute-specific robust scaling further increases the robust accuracy compared to the original robust scaling, and also improves the robust coverage, as shown in Table~\ref{tab:grs_all_supple}.
Note that our attribute-specific scaling strategy allows ERM to match the supervised state-of-the-art approach, Group DRO~\cite{GroupDRO}.

One limitation is that it requires the supervision of spurious attribute information to train the estimator model in step 3. 
However, we notice that only a very few examples with the supervision is enough to train the spurious-attribute estimator, because it is much easier to learn as the word ``spurious correlation" suggests.
To determine how much the group-labeled data is needed, we train several spurious-attribute estimators by varying the number of group-labeled examples, and conduct ARS using the estimators.
%In Table~\ref{tab:abl_groupsize}, group-labeled size denotes a ratio of group-labeled samples among all training examples for training estimators, and spurious accuracy indicates the average accuracy of spurious-attribute classification using the estimators on the test split.
Table~\ref{tab:abl_groupsize} validates that, compared to the overall training dataset size, a very small amount of group-labeled examples is enough to achieve high robust accuracy.


 \begin{table*}[t]
\begin{center}
\caption{Results of the attribute-specific robust scaling (ARS) on the CelebA and Waterbirds datasets with the average of three runs (standard deviations in parenthesis), where ARS is applied to maximize each target metric independently.
%Although ERM + ARS exploits the group supervision only during post-processing, it achieves competitive performance to Group DRO which utilizes the group supervision during training.
Note that our post-processing strategy, ARS, allows ERM to achieve competitive performance to Group DRO that utilizes the group supervision during training.
%\textcolor{blue}{Blue} color denotes the target metric for the robust scaling and $\ast$ indicates that the method requires spurious-attribute supervision in the training split.
%\textcolor{blue}{Blue} color denotes the target metric that the robust scaling aims to maximize.
%Compared to RS, GRS improves the overall trade-off.
}
%\vspace{0.2cm}
\label{tab:grs_all_supple}
 \scalebox{0.85}{
 \hspace{-0.3cm}
\setlength\tabcolsep{8pt} 
%\begin{tabular}{cl|cc|cc:c}
\begin{tabular}{cl|cc|ccc}
\toprule
%Method & Worst-. Acc. & Unbiased Acc. &  Average Acc.  & Worst-. Cover. & Unbiased Cover. \\
 & & \multicolumn{2}{c|}{Robust Coverage} & \multicolumn{3}{c}{Accuracy (\%)} \\
Dataset & Method & Worst. & Unbiased & Worst. & {Unbiased} &  Average \\
\hline
 \multirow{3}{*}{CelebA} & ERM  & - & - & 34.5 (6.1) &77.7 (1.8) &95.5 (0.4) \\
%& ERM + RS  &83.0(0.7) &88.1(0.5)&82.1(3.7) &91.1(0.6) &92.2(1.3)   &\textbf{45.0(7.4)} &\textbf{81.7(1.8)} &\textbf{95.8(0.2)} \\
& ERM + ARS  &\textbf{87.6 (1.0)} &\textbf{89.0 (0.2)}&\textbf{88.5 (1.8)} &{91.9 (0.3)}  &\textbf{95.8 (0.1)} \\
\cdashline{2-7}
& Group DRO & 87.3 (0.2) & 88.3 (0.2) & 88.4 (2.3) & \textbf{92.0 (0.4}) & 93.2 (0.8) \\
%ERM + CRS     &83.4(0.1) &88.4(0.4) &87.2(2.0) &91.7(0.2) &91.5(0.8)  &44.1(4.2) &81.3(0.8) &\textbf{95.8(0.1)} \\
%&ERM + IRS     &\textbf{83.4(0.1)} & \textbf{88.4(0.4)} & \textbf{87.2(2.0)} & \textbf{91.7(0.2)} &91.5(0.8)  &44.1(4.2) &81.3(0.8) &\textbf{95.8(0.1)} \\
%\cdashline{2-10}
%&CR & - & - &70.6(6.0) &88.7(1.2) &\textbf{94.2(0.7)}   &\textbf{70.6(6.0)} &\textbf{88.7(1.2)} &94.2(0.7) \\
%&CR + RS  &82.9(0.5) &88.2(0.3) &82.7(5.2) &91.0(1.0) &91.7(1.3)   &48.5(8.9) &82.5(2.2) & \textbf{95.8(0.1)}\\
%%CR + ARS$\ast$  &\textbf{86.9(0.9)} & \textbf{89.0(0.3)} &\textbf{89.5(1.2)} &\textbf{92.3(0.3)} &92.9(0.5)   &48.3(9.5) &82.7(2.2) &\textbf{95.8(0.1)} \\
%%CR + CRS &83.6(1.1) &88.6(0.5) &84.8(1.5) &91.3(0.4) &90.7(1.3)   &48.8(9.1) &82.7(2.4) &\textbf{95.8(0.1)} \\
%&CR + IRS & \textbf{83.6(1.1)} &\textbf{88.6(0.5)} &\textbf{84.8(1.5)} & \textbf{91.3(0.4)} & 90.7(1.3)   &48.8(9.1) &82.7(2.4) &\textbf{95.8(0.1)} \\
\hline
%Group DRO$\ast$ & {88.4(2.3)} & {92.0(0.4)} & {93.2(0.8)} & {87.3(0.2)} & 88.4(0.2) \\
% \multirow{3}{*}{Waterbirds}&ERM     & - & - &70.6(4.2) &86.9(0.6) &89.2(1.0)  \\
 \multirow{3}{*}{Waterbirds}&ERM  & - & - &76.3 (0.8) &89.4 (0.6) &{97.2 (0.2)} \\
%&ERM + RS      & 67.6(0.9) & 77.7(1.0)  &77.0(0.7) &87.5(0.4) &87.8(0.4)  &59.7(6.5) &85.0(1.3) &90.2(0.7) \\
& ERM + ARS      &\textbf{84.4 (1.9)} &\textbf{87.8 (1.7)} &\textbf{89.3 (0.4)} &\textbf{92.5 (0.4)} &\textbf{97.5 (1.0)} \\
%ERM + CRS     &78.0(3.5) &82.4(2.0) &81.5(7.8) &89.7(1.8) &\textbf{90.8(1.4)}  &75.9(6.6) &88.5(1.4) &91.1(1.2) \\
%&ERM + IRS     &\textbf{78.0(3.5)} &\textbf{82.4(2.0)} &\textbf{81.5(7.8)} &\textbf{89.7(1.8)} &\textbf{90.8(1.4)}  &\textbf{75.9(6.6)} &\textbf{88.5(1.4)} &\textbf{91.1(1.2)} \\
\cdashline{2-7}
%& Group DRO & 78.5(1.0) & 83.4(0.5) & 82.2(0.4) & 90.5(0.4) & 91.3(2.0) \\
%& Group DRO & {83.4(1.1)}&{87.4(2.3)} &{89.1(1.7)} &\textbf{92.7(0.8)} & {96.4(1.5)} \\
& Group DRO & {83.4 (1.1)}&{87.4 (2.3)} &88.0 (1.0) &\textbf{92.5 (0.9)} &{95.8 (1.8)} \\
%& CR   & - & -  &65.6(7.3) &85.5(1.5) &\textbf{89.1(0.8)}  & \textbf{65.6(7.3)} &85.5(1.5) &89.1(0.8) \\
%& CR + RS   &67.0(0.8) &77.4(0.5)  &74.1(1.9) &86.8(0.3) &87.8(1.1)  &56.0(6.8) &84.0(1.7) &90.0(0.3) \\
%CR + ARS$\ast$   &\textbf{80.3(1.6)} &\textbf{83.0(1.0)} &\textbf{85.3(0.9)} &\textbf{90.4(0.7)} &\textbf{89.9(1.1)}  &\textbf{79.2(3.5)} &\textbf{89.5(1.2)} &\textbf{92.3(0.3)} \\
%CR + CRS     &73.5(2.4) &80.9(1.2) &77.2(10.2) &88.8(2.6) &89.1(1.9)  &63.1(12.2) &85.9(2.5) &90.3(1.0) \\
%& CR + IRS     & \textbf{73.5(2.4)} & \textbf{80.9(1.2)} & \textbf{77.2(10.2)} & \textbf{88.8(2.6)} & \textbf{89.1(1.9)}  &63.1(12.2) &\textbf{85.9(2.5)} &\textbf{90.3(1.0)} \\
\bottomrule
\end{tabular}
 }
\end{center}

\end{table*}


%  \paragraph{Varying the group-labeled set size to train spurious-attribute estimator (ARS)}
%%As mentioned in Section~\ref{sec:crs}, 
%As mentioned before, we need the group-labeled (spurious-attribute-labeled) examples to train spurious-attribute estimator.
%%  , but we claimed that only a small number of samples are enough.
%To determine how much the group-labeled data is needed, we train several spurious-attribute estimators by varying the number of group-labeled examples, and conduct ARS using the estimators.
%In Table~\ref{tab:abl_groupsize}, group-labeled size denotes a ratio of group-labeled samples among all training examples for training estimators, and spurious accuracy indicates the average accuracy of spurious-attribute classification using the estimators on the test split.
%Table~\ref{tab:abl_groupsize} validates that, compared to the overall training dataset size, a very small amount of group-labeled examples is enough to achieve high robust accuracy, because the spurious attribute is much easier to learn.

\begin{table}[t]
\begin{center}
\caption{Effects of the size of group-labeled examples on the attribute-specific robust scaling on the CelebA dataset.
Group-labeled size denotes a ratio of group-labeled samples among all training examples for training estimators.
Spurious accuracy indicates the average accuracy of spurious-attribute classification using the estimators on the test split.
}
\vspace{0.3cm}
\label{tab:abl_groupsize}
 \scalebox{0.8}{
% \vspace{-0.3cm}
\setlength\tabcolsep{7pt} 
\begin{tabular}{c||c|ccc|cc}
\toprule
&Accuracy (\%)& \multicolumn{3}{c|}{Accuracy (\%)} & \multicolumn{2}{c}{Robust Coverage} \\
Group-labeled size & Spurious &Worst-group & Unbiased &  Average & Worst-group & Unbiased \\
%Group-labeled size & Worst-. Acc. & Unbiased Acc. &  Average Acc.  & Worst-. Cover. & Unbiased Cover. \\
\hline
100\% &98.4 &{89.1 (3.0)} &{92.4 (1.1)} &{93.1 (1.2)} & {87.6 (1.0)} &{89.0 (0.5)} \\
10\%  & 97.7 &88.5 (1.8) &91.9 (0.3) &92.8 (0.6) &86.8 (0.4) &89.0 (0.2) \\
1\%   & 95.8 &88.5 (1.8) &91.9 (0.3) &92.9 (0.6) &87.1 (0.3) &89.0 (0.2) \\
0.1\%&92.6  &88.4 (2.1) &91.8 (0.5) &92.4 (0.8) &87.1 (0.3) &89.0 (0.2) \\
\bottomrule
\end{tabular}
 }
\end{center}
%  \vspace{-0.1cm}
\end{table}

\section{Experimental Details}
\label{sec:exp_detail}

\subsection{Datasets}
\label{sec:datasets_detail}
CelebA~\cite{CelebA} is a large-scale dataset for face image recognition, consisting of 202,599 celebrity images, with 40 attributes labeled on each image.
Among the attributes, we primarily examine \textit{hair color} and \textit{gender} attributes as a target and spurious attributes, respectively.
We follow the original train-validation-test split~\cite{CelebA} for all experiments in the paper.
Waterbirds~\cite{GroupDRO} is a synthesized dataset, which are created by combining bird images in the CUB dataset~\cite{wah2011caltech} and background images from the Places dataset~\cite{zhou2017places}, consisting of 4,795 training examples.
The two attributes---one is the type of bird, \{waterbird, landbird\} and the other is background places, \{water, land\}, are used for the experiments with this dataset.
CivilComments-WILDS~\cite{koh2021wilds} is a large-scale text dataset, which has 269,038 training comments, 45,180 validation comments, and 133,782 test comments. 
This task is to classify whether an online comment is toxic or not, which is spuriously correlated to demographic identities (\textit{male, female, White, Black, LGBTQ, Muslim, Christian, and other religion}).
FMoW-WILDS~\cite{koh2021wilds} is based on the Functional Map of the World dataset~\cite{christie2018functional}, comprising high-resolution satellite images from over 200 countries and over the years 2002-2018. 
The label is one of 62 building or land use categories, and the attribute represents both the year and geographical regions (\textit{Africa, the Americas, Oceania, Asia, or Europe}).
It consists of 76,863 training images from the years 2002-2013, 19,915 validation images from the years 2013-2016, and 22,108 test images from the years 2016-2018.


\subsection{Class-specific Scaling} 
To identify the optimal points, we obtain a set of the average and robust accuracy pairs using a wide range of the class-specific scaling factors, \ie, $\mathbf{s}_i= (1.05)^n~\text{for}-200 \leq n \leq 200$ for $i^\text{th}$ class. Note that we search for the scaling factor of each class in a greedy manner, as stated in Section~\ref{sec:robust_scaling}.

 
\subsection{Hyperparameter Tuning}

We tune the learning rate in $\{10^{-3}, 10^{-4}, 10^{-5}, 10^{-6}\}$ and the weight decay in $\{1.0, 0.1, 10^{-2}, 10^{-4}\}$ for all baselines on all datasets.
We used 0.5 of $q$ for LfF.
For JTT, we searched $\lambda_\text{up}$ in $\{20, 50, 100\}$ and updated the error set every epoch for CelebA dataset and every 60 epochs for Waterbirds dataset.
For Group DRO, we tuned $C$ in $\{0, 1, 2, 3, 4\}$, and used 0.1 of $\eta$.

 \iffalse
 \paragraph{CelebA}
 \revision{
%For LfF, we used $1\times 10^{-4}$ of learning rate, $1\times 10^{-4}$ of weight decay, and 0.5 of $q$.
%For JTT, we used $1\times10^{-5}$ of learning rate, 0.1 of weight decay, and 50 of $\lambda_\text{up}$, and updated the error set every epoch.
%For Group DRO, we used $1\times10^{-5}$ of learning rate, 0.1 of weight decay, 3 of $C$, and 0.1 of $\eta$.
 }
 

 \paragraph{Waterbirds}
\revision{
%For LfF, we used $1\times 10^{-4}$ of learning rate, $1\times 10^{-4}$ of weight decay, and 0.5 of $q$.
%For JTT, we used $1\times10^{-5}$ of learning rate, 1.0 of weight decay, and 100 of $\lambda_\text{up}$, and updated the error set every 60 epochs.
%For Group DRO, we used $1\times10^{-5}$ of learning rate, 0.1 of weight decay, 2 of $C$, and 0.1 of $\eta$.
 }
 \fi
 
 
 
\begin{table}[t]
\begin{center}
\caption{Realized robust coverage results on the Waterbirds and CelebA datasets with the average of three runs (standard deviations in parenthesis).
}
\vspace{0.3cm}
\label{tab:modified_rc}
 \scalebox{0.8}{
% \hspace{-0.3cm}
\setlength\tabcolsep{10pt} 
\begin{tabular}{cl|cc|cc}
\toprule
& & \multicolumn{2}{c|}{Robust Coverage} & \multicolumn{2}{c}{Realized Robust Coverage} \\
Dataset & Method & Worst-group & Unbiased & Worst-group & Unbiased \\
\hline
Waterbirds & ERM & 70.3 (1.3) 	& 79.4 (0.7) & 69.0 (1.5) & 78.7 (0.8) \\
Waterbirds & CR & 68.9 (1.1) 		& 78.5 (0.5) & 67.8 (1.2) & 77.9 (0.4) \\
Waterbirds & Group DRO 			& 80.8 (0.6) & 85.2 (0.1) & 78.6 (1.0) & 83.8 (0.4) \\
Waterbirds & GR & 78.8 (5.6) 		& 83.7 (0.7) & 77.9 (1.4) & 82.8 (0.8)\\
 \hline
CelebA & ERM & 78.9 (1.7) 		& 86.0 (0.6) & 75.9 (2.2) & 85.4 (0.7) \\
CelebA &  CR & 77.2 (2.8) 		& 85.6 (0.9) & 71.8 (1.3) & 85.0 (0.6) \\
CelebA &  Group DRO & 84.2 (0.6) 	& 86.7 (0.5) & 81.0 (1.7) & 86.1 (0.2) \\
CelebA &  GR & 84.2 (0.5) 		& 87.5 (0.3) & 81.2 (1.6) & 87.0 (0.5) \\
\bottomrule
\end{tabular}
 }
\end{center}
\end{table}
 
 
% \begin{table}[t]
%\begin{center}
%\caption{Realized robust coverage results on the Waterbirds and CelebA datasets with the average of three runs (standard deviations in parenthesis).
%}
%%\vspace{0.2cm}
%\label{tab:modified_rc}
% \scalebox{0.85}{
%% \hspace{-0.3cm}
%\setlength\tabcolsep{4pt} 
%\begin{tabular}{l|cc|cc}
%\toprule
%& \multicolumn{2}{c|}{Robust Coverage} & \multicolumn{2}{c}{Realized Robust Coverage} \\
%Method & Worst-group & Unbiased  & Worst-group & Unbiased \\
%\hline
%\multicolumn{5}{c}{CelebA} \\
%\hline
%ERM & 70.3(1.3) & 79.4(0.7) & 69.0(1.5) & 78.7(0.8) \\
%CR & 68.9(1.1) & 78.5(0.5) & 67.8(1.2) & 77.9(0.4) \\
%Group DRO & 80.8(0.6) & 85.2(0.1) & 78.6(1.0) & 83.8(0.4) \\
%GR & 78.8(5.6) & 83.7(0.7) & 77.9(1.4) & 82.8(0.8)\\
% \hline
% \multicolumn{5}{c}{Waterbirds} \\
%\hline
%ERM & 78.9(1.7) & 86.0(0.6) & 75.9(2.2) & 85.4(0.7) \\
%CR & 77.2(2.8) & 85.6(0.9) & 71.8(1.3) & 85.0(0.6) \\
%Group DRO & 84.2(0.6) & 86.7(0.5) & 81.0(1.7) & 86.1(0.2) \\
%GR & 84.2(0.5) & 87.5(0.3) & 81.2(1.6) & 87.0(0.5) \\
%\bottomrule
%\end{tabular}
% }
%\end{center}
%\end{table}
  
 
 
 
 
 
 
 \section{Additional Results}

 
%\subsection{Additional Analysis}

\iffalse
  \begin{table}[t]
\begin{center}
\caption{Variations of robust scaling methods tested on the FairFace dataset.
%where $S$ and $C$ denote the number of superclasses/classes.
}
%\vspace{-3mm}
\label{tab:superclass_supple}
 \scalebox{0.8}{
\setlength\tabcolsep{6pt} 
\begin{tabular}{lc|ccc}
\toprule
%& \multicolumn{3}{|c}{Accuracy (\%)} \\
Method & Cost & Worst-group & Unbiased &  Average  \\
\hline
%ERM & &47.0 & 52.3\\
%ERM + RS (superclass) & & 51.8 & 51.5\\
%ERM + RS (full) & & 52.3 & 50.7\\
ERM & -- &15.8 &47.0 & 54.1\\
+ RS (2 super classes) & $\mathcal{O}(n)$ & 18.6& 51.8 & 52.9\\
+ RS (greedy search) & $\mathcal{O}(n)$ & \bf{19.2} & 52.3 & \bf{53.3} \\
+ RS (full grid search) & $\mathcal{O}(n^9)$ & 19.0& \bf{52.8} & 53.1\\
\bottomrule
\end{tabular}
 }
% \vspace{-0.3cm}
\end{center}
\end{table}


 
 \paragraph{Scalability}
As mentioned in Section~\ref{sec:robust_scaling}, we actually search for the scaling factor of each class in a greedy manner.
Hence, the time complexity increases linearly with respect to the number of classes instead of the exponential growth with the full grid search; even with 1000 classes, the whole process takes less than a few minutes in practice, which is negligible compared to the model training time.
Moreover, we can reduce the computational cost even further by introducing the superclass concept and allocating a single scaling factor for each superclass.
We compare three different options---greedy search, superclass-level search, and full grid search---on the FairFace dataset~\cite{FairFace} with 9 classes. 
Table~\ref{tab:superclass} shows that our greedy search is as competitive as the full grid search despite the time complexity gap in several orders of magnitude in computational complexity and the superclass-level search is also effective to reduce cost.
Note that the superclasses are identified by the feature similarity of class signatures.
 
\fi


 \begin{figure}[t]
\centering
    \begin{subfigure}[m]{0.4\linewidth}
    	\includegraphics[width=\linewidth]{figures/worst_scale_curve.png}
	\subcaption{Worst-group accuracy}
	\label{fig:tsne_noise}
    \end{subfigure} 
    	\hspace{0.5cm}
        \begin{subfigure}[m]{0.4\linewidth}
    	\includegraphics[width=\linewidth]{figures/unbias_scale_curve.png}
	\subcaption{Unbiased accuracy}
	\label{fig:tsne_erm}
	\end{subfigure} 
    \caption{Effects of varying the class-specific scaling factors on the robust accuracy using ERM model on the CelebA dataset.
    Since this experiment is based on the binary classifier, a single scaling factor is varied with the other fixed to one. 
%    Note that we can identify the desired optimal points between robust and average accuracies in the test set using the scaling factor learned from the validation set.
These results show that the optimal scaling factor identified in the validation set can be used in the test set to get the final robust prediction.
    }
%    \vspace{-0.3cm}
    \label{fig:observation_scale_curve}
\end{figure}

\paragraph{Feasibility}
\label{sec:analysis}
We visualize the relationship between scaling factors and robust accuracies in Figure~\ref{fig:observation_scale_curve}, where the curves are constructed based on validation and test splits are sufficiently well-aligned to each other.
This implies that the optimal scaling factor identified in the validation set can be used in the test set to get the final robust prediction.

\paragraph{Robust coverage curve}
Figure~\ref{fig:worst_curve_all_celeba} and~\ref{fig:unbias_curve_all_celeba} are robust-average accuracy trade-off curves while Figure~\ref{fig:worst_curve_all_pareto_celeba} and~\ref{fig:unbias_curve_all_pareto_celeba} are their corresponding robust coverage curves, which represent the Pareto frontiers of Figure~\ref{fig:worst_curve_all_celeba} and~\ref{fig:unbias_curve_all_celeba}, respectively.
The area under the curve in Figure~\ref{fig:worst_curve_all_pareto_celeba} and~\ref{fig:unbias_curve_all_pareto_celeba} indicates the robust coverage of each algorithm.

 \begin{figure}[t!]
\centering
 \begin{subfigure}[m]{0.47\linewidth}
    	\includegraphics[width=\linewidth]{figures/worst_curve_all.png}
	\subcaption{Worst-group accuracy}
%\subcaption{Robust-average accuracy trade-off curves}
	\label{fig:worst_curve_all_celeba}
	\vspace{0.2cm}
	\end{subfigure} 
	\hspace{0.5cm}
	    \begin{subfigure}[m]{0.47\linewidth}
    	\includegraphics[width=\linewidth]{figures/unbias_curve_all.png}
	\subcaption{Unbiased accuracy}
	\label{fig:unbias_curve_all_celeba}
%	\vspace{0.2cm}
	\end{subfigure}
	 \begin{subfigure}[m]{0.47\linewidth}
    	\includegraphics[width=\linewidth]{figures/worst_curve_all_pareto}
	\subcaption{Worst-group accuracy}
%\subcaption{Robust-average accuracy Pareto frontiers}
	\label{fig:worst_curve_all_pareto_celeba}
	\end{subfigure} 
	\hspace{0.5cm}
	    \begin{subfigure}[m]{0.47\linewidth}
    	\includegraphics[width=\linewidth]{figures/unbias_curve_all_pareto}
	\subcaption{Unbiased accuracy}
	\label{fig:unbias_curve_all_pareto_celeba}
	\end{subfigure}
%    \caption{
%    The robust-average accuracy trade-off curves on the CelebA dataset.
%    The robust-average accuracy trade-off curves and its corresponding Pareto frontiers on the CelebA dataset.
%    The curves in (c) and (d) represent the Pareto frontiers of the curves in (a) and (b), respectively.
%	In (b), the numbers in the legend denote the robust coverage, which measures the area under the Pareto frontier curve.
%    }
    \caption{The robust-average accuracy trade-off curves ((a), (b)) and their corresponding robust coverage curves ((c), (d)), respectively, on the CelebA dataset.
    The curves in (c) and (d) represent the Pareto frontiers of the curves in (a) and (b), respectively.
	In (c) and (d), the numbers in the legend denote the robust coverage, which measures the area under the curve.
    }
    \vspace{-2mm}
    \label{fig:pareto_celeba}
\end{figure}
 


%\subsection{Motivation of robust scaling}
%Let assume there is a performance discrepancy between minority and majority classes, where majority classes have higher performance. Then, if we upweight the final prediction score of minority classes, the samples will have more chances to be classified into those classes, thus the accuracy of minority classes increases at the expense of those of majority classes, resulting in a desirable trade-off for group robustness. We demonstrate that our class-specific scaling can identify the optimal point that maximizes any target metrics (ex. worst-group acc) on the trade-off on top of existing models in practice with extensive experiments.

\iffalse
 \subsection{Robust coverage in the test split}
 \label{sec:coverage_clarification}
 In Section 4.3 of the main paper, we defined robust coverage as
  \begin{align}
     \text{Coverage} :=  \int_{c=0}^1\max_\mathbf{s}\big\{\text{WA}^\mathbf{s}|\text{AA}^\mathbf{s}\geq c\big\}dc 
     \approx
      \sum_{d=0}^{D-1}\frac{1}{D}\max_\mathbf{s}\big\{\text{WA}^\mathbf{s}|\text{AA}^\mathbf{s}\geq \frac{d}{D}\big\}.
     \label{eq:coverage_main}
 \end{align}
 The robust coverage can be directly calculated in the validation split, but unfortunately, it is basically unavailable in the test split.
This is because we need to know the values of $\text{WA}^{\mathbf{s}}$ in advance to conduct max operation in~(\ref{eq:coverage_main}), but we cannot use the information in the test split.

To bypass this issue, we report the robust coverage of validation split in this paper, which tends to be similar to those of the test split.
% However, because the optimal scaling factors identified in the validation split can be used in the test split as well, we believe the robust coverage can provide meaningful upper bound.
We also validate the reliability of the robust coverage of the test split.
We first obtain a set of optimal scaling factors for each threshold in the validation split $\mathcal{S}_\text{val}$ as
  \begin{equation}
%      \mathcal{S}_\text{val} := \Big\{ \max_\mathbf{s}\big\{\text{UA}^\mathbf{s}|\text{AA}^\mathbf{s}\geq c\big\}~~\forall c \in [0, 1]
%      \Big\},
      \mathcal{S}_\text{val} := \Big\{ \max_\mathbf{s}\big\{\text{WA}_{\text{val}}^\mathbf{s}|\text{AA}_{\text{val}}^\mathbf{s}\geq \frac{d}{D} \big\}~~\text{for}~0 \leq d \leq D-1
      \Big\},
  \end{equation}
then the realized robust coverage of test split is calculated by
\begin{equation}
      \text{Realized Coverage} := \frac{1}{|\mathcal{S}_\text{val}|} \sum_\mathbf{s \in \mathcal{S}_\text{val}} \text{WA}_{\text{test}}^\mathbf{s}.
  \end{equation}
%
Table~\ref{tab:modified_rc} presents the original robust coverage and realized robust coverage results on the test splits of Waterbirds and CelebA datasets.
%Both coverage results  where the original robust coverage can provide tight upper bound
Both coverage results are almost similar, because the optimal scaling factors identified in the validation split are close to optimal in the test split as well.

\fi

%\vspace{-2mm} 
\paragraph{Scalability}


  \begin{table}[t]
\begin{center}
\caption{Variations of robust scaling methods and their performances tested on the FairFace dataset.
%where $S$ and $C$ denote the number of superclasses/classes.
}
\vspace{3mm}
\label{tab:superclass}
 \scalebox{0.8}{
\setlength\tabcolsep{8pt} 
\begin{tabular}{lc|ccc}
\toprule
%& \multicolumn{3}{|c}{Accuracy (\%)} \\
Method & Cost & Worst-group & Unbiased &  Average  \\
\hline
%ERM & &47.0 & 52.3\\
%ERM + RS (superclass) & & 51.8 & 51.5\\
%ERM + RS (full) & & 52.3 & 50.7\\
ERM & -- &15.8 &47.0 & 54.1\\
+ RS (2 super classes) & $\mathcal{O}(n)$ & 18.6& 51.8 & 52.9\\
+ RS (greedy search) & $\mathcal{O}(n)$ & \bf{19.2} & 52.3 & \bf{53.3} \\
+ RS (full grid search) & $\mathcal{O}(n^9)$ & 19.0& \bf{52.8} & 53.1\\
\bottomrule
\end{tabular}
 }
% \vspace{-0.3cm}
\end{center}
\end{table}


 
As mentioned in Section~\ref{sec:robust_scaling}, we search for the scaling factor of each class in a greedy manner.
Hence, the time complexity increases linearly with respect to the number of classes instead of the exponential growth with the full grid search; even with 1,000 classes, the whole process takes less than a few minutes in practice, which is negligible compared to the model training time.
Moreover, we can reduce the computational cost even further by introducing the superclass concept and allocating a single scaling factor for each superclass.
We compare three different options---greedy search, superclass-level search, and full grid search---on the FairFace dataset~\cite{FairFace} with 9 classes. 
Table~\ref{tab:superclass} shows that the greedy search is as competitive as the full grid search despite the time complexity reduction by several orders of magnitude and the superclass-level search is also effective to reduce cost with competitive accuracies.
Note that the superclasses are identified by the feature similarity of class signatures.
 
 


 \paragraph {Additional Results}
 
Table~\ref{tab:all_exp} presents full experimental results on the CelebA and Waterbirds datasets, which supplement Table~\ref{tab:celebA} and~\ref{tab:waterbirds}.
We test our robust scaling strategies (RS, IRS) with two scenarios, each aimed at maximizing worst-group or average accuracies, respectively, where each target metric is marked in blue in the tables.

 
  \begin{table*}[t]
\begin{center}
\caption{Results of our robust scaling methods on top of various baselines on the CelebA dataset, which supplement Table~\ref{tab:celebA}.
%\textcolor{blue}{Blue} color denotes the target metric for the robust scaling and $\ast$ indicates that the method requires spurious-attribute supervision in the training split.
\textcolor{blue}{Blue} color denotes the target metric that the robust scaling aims to maximize.
Compared to RS, IRS improves the overall trade-off.
}
%\vspace{0.2cm}
\label{tab:all_exp}
 \scalebox{0.78}{
 \hspace{-0.4cm}
\setlength\tabcolsep{6pt} 
\begin{tabular}{l|cc|ccc|ccc}
\toprule
 & \multicolumn{2}{c|}{Robust Coverage} & \multicolumn{3}{c|}{Accuracy (\%)} & \multicolumn{3}{c}{Accuracy (\%)} \\
Method & Worst-group & Unbiased & \textcolor{blue}{Worst-group} & {Unbiased} &  Average & Worst-group & Unbiased &  \textcolor{blue}{Average}  \\
\hline
 ERM  & - & - & 34.5 (6.1) &77.7 (1.8) &\textbf{95.5 (0.4)}   & 34.5 (6.1) &77.7 (1.8) &95.5 (0.4) \\
ERM + RS  &83.0 (0.7) &88.1 (0.5)&82.1 (3.7) &91.1 (0.6) &92.2 (1.3)   &\textbf{45.0 (7.4)} &\textbf{81.7 (1.8)} &\textbf{95.8 (0.2)} \\
ERM + IRS     &\textbf{83.4 (0.1)} & \textbf{88.4 (0.4)} & \textbf{87.2 (2.0)} & \textbf{91.7 (0.2)} &91.5 (0.8)  &44.1 (4.2) &81.3( 0.8) &\textbf{95.8 (0.1)} \\
\cdashline{1-9}
%\cdashline{2-10}
CR & - & - &70.6 (6.0) &88.7 (1.2) &\textbf{94.2 (0.7)}   &\textbf{70.6 (6.0)} &\textbf{88.7 (1.2)} &94.2 (0.7) \\
CR + RS  &82.9 (0.5) &88.2 (0.3) &82.7 (5.2) &91.0 (1.0) &91.7 (1.3)   &48.5 (8.9) &82.5 (2.2) & \textbf{95.8 (0.1)}\\
CR + IRS & \textbf{83.6 (1.1)} &\textbf{88.6 (0.5)} &\textbf{84.8 (1.5)} & \textbf{91.3 (0.4)} & 90.7 (1.3)   &48.8 (9.1) &82.7 (2.4) &\textbf{95.8 (0.1)} \\
\cdashline{1-9}
SUBY & - & - &65.7 (3.9) &87.5 (0.9) &\textbf{94.5 (0.7)}  & \textbf{65.7 (3.9)} & \textbf{87.5 (0.9)} &{94.5 (0.7)}  \\
SUBY + RS  & 81.5 (1.0)  & 87.4 (0.1) & 80.8 (2.9) & 90.5 (0.8) & 91.1 (1.7) & 45.4 (6.7) & 81.4 (2.0) & \textbf{95.5 (0.0)} \\
SUBY + IRS & \textbf{82.3 (1.1)} &\textbf{87.8 (0.2)} &\textbf{82.3 (2.0)} & \textbf{90.8 (0.8)} & 90.7 (1.9)  & 46.0 (6.9) & 81.5 (2.1) & \textbf{95.5 (0.1)}  \\
\cdashline{1-9}
SUBG & - & - &87.8 (1.2) &90.4 (1.2) &\textbf{91.9 (0.3)}   & \textbf{87.8 (1.2)} &\textbf{90.4 (1.2)} &{91.9 (0.3)} \\
SUBG + RS  & 83.6 (1.6) & 87.5 (0.7) & 88.3 (0.7) & 90.9 (0.5) & 90.6 (1.0) & 67.8 (6.5) & 85.2 (2.0) & {93.9 (0.2)} \\
SUBG + IRS & \textbf{84.5 (0.8)} &\textbf{87.9 (0.1)} &\textbf{88.7 (0.6)} & \textbf{91.0 (0.3)} & 90.6 (0.8) & 68.5 (6.5) & 85.5 (1.9) & \textbf{94.0 (0.2)} \\
\cdashline{1-9}
GR & - & - & 88.6 (1.9) &92.0 (0.4) &\textbf{92.9 (0.8)}  & \textbf{88.6 (1.9)} & \textbf{92.0 (0.4)} &{92.9 (0.8)}  \\
GR + RS  &{86.9 (0.4)} &{88.4 (0.2)} & \textbf{90.0 (1.6)} & {92.4 (0.5)} & 92.5 (0.5) &  66.5 (0.3) & 85.4 (0.4) & {93.8 (0.4)} \\
GR + IRS & \textbf{87.0 (0.2)} & \textbf{88.6 (0.2)} & \textbf{90.0 (2.3)} & \textbf{92.6 (0.6)} & 92.5 (0.4) & 62.0 (5.3) & 84.5 (0.7) & \textbf{94.2 (0.3)} \\
\cdashline{1-9}
GroupDRO & - & - &  88.4 (2.3) & 92.0 (0.4) & {93.2 (0.8)}  &  \textbf{88.4 (2.3)} & \textbf{92.0 (0.4)} & {93.2 (0.8)}  \\
GroupDRO + RS  &{87.3 (0.2)} & {88.3 (0.2)} & 89.7 (1.2) & 92.3 (0.1) &\textbf{93.7 (0.5)} & 64.9 (3.3) & 85.1 (0.7)& 93.9 (0.3)\\
GroupDRO + IRS & \textbf{87.5 (0.4)} & \textbf{88.4 (0.2)}& \textbf{90.0 (2.3)} & \textbf{92.6 (0.6)} & 93.5 (0.4) & 60.4 (5.4) & 84.4 (0.6) & \textbf{94.7 (0.3)} \\
\bottomrule
\end{tabular}
}
 \vspace{-0.2cm}
\end{center}

\end{table*}



  \begin{table*}[t]
\begin{center}
\caption{Results of our robust scaling methods on top of various baselines on the Waterbirds dataset, which supplement Table~\ref{tab:waterbirds}.
\textcolor{blue}{Blue} color denotes the target metric that the robust scaling aims to maximize.
Compared to RS, IRS improves the overall trade-off.
}
%\vspace{0.2cm}
\label{tab:all_exp}
 \scalebox{0.78}{
 \hspace{-0.4cm}
\setlength\tabcolsep{6pt} 
\begin{tabular}{l|cc|ccc|ccc}
\toprule
  & \multicolumn{2}{c|}{Robust Coverage} & \multicolumn{3}{c|}{Accuracy (\%)} & \multicolumn{3}{c}{Accuracy (\%)} \\
Method & Worst-group & Unbiased & \textcolor{blue}{Worst-group} & {Unbiased} &  Average & Worst-group & Unbiased &  \textcolor{blue}{Average}  \\
\hline
ERM     & - & - &76.3 (0.8) &89.4 (0.6) &\textbf{97.2 (0.2)}   &76.3 (0.8) &89.4 (0.6) &97.2 (0.2) \\
ERM + RS      & 76.1 (0.4) & 82.6 (0.3)  &81.6 (1.9) &89.8 (0.5) &\textbf{97.2 (0.2)}  &\textbf{79.1 (2.7)} & \textbf{89.7 (0.6)}&{97.5 (0.1)} \\
ERM + IRS     &\textbf{83.4 (1.1)} &\textbf{86.9 (0.4)} &\textbf{89.3 (0.5)} &\textbf{92.7 (0.4)} &{94.1 (0.3)}  &{77.6 (7.0)} &{89.6 (1.1)} &\textbf{97.5 (0.3)} \\
\cdashline{1-9}
CR   & - & -  &76.1 (0.7) &89.1 (0.7) &\textbf{97.1 (0.5)}  &76.1 (0.7) &89.1 (0.7) &{97.1 (0.3)} \\
CR + RS   &73.6 (2.3) &82.0 (1.5)  &79.4 (2.4) &89.4 (1.0) &96.8(0.8)  & 76.4(1.5)& \textbf{89.3 (0.8)}&\textbf{97.5 (0.3)} \\
CR + IRS     & \textbf{84.2 (2.5)} & \textbf{88.3 (1.0)} & \textbf{88.2 (2.7)} & \textbf{92. 1(0.7)} & {95.7 (1.1)}  &\textbf{77.3 (4.7)} & 88.6( 1.2)& 97.4 (0.2) \\
\cdashline{1-9}
SUBY   & - & -  & 72.8 (4.1) & 84.9 (0.4) & {93.8 (1.5)} & 72.8 (4.1) & 84.9 (0.4) & {93.8(1.5)} \\
SUBY + RS   &72.5 (1.0) &81.2 (1.4) & 75.9 (4.4) & 86.3 (0.9) & \textbf{95.2 (1.4)} & 70.7 (5.8) & 85.4 (1.6) & {95.5 (0.2)}  \\
SUBY + IRS     & \textbf{78.8 (2.7)} & \textbf{85.9 (1.0)}  & \textbf{82.1 (4.0)} & \textbf{89.1 (0.9)} & 92.6 (2.2) & \textbf{74.1 (4.1)} & \textbf{86.3 (0.9)} & \textbf{96.2 (0.6)} \\
\cdashline{1-9}
SUBG   & - & - & 86.5 (0.9) & 88.2 (1.2) & 87.3 (1.1) & \textbf{86.5 (0.9)} & \textbf{88.2 (1.2)} & 87.3 (1.1) \\
SUBG + RS   & 80.6 (2.0) & 82.3 (2.0) & 87.1 (0.7) & \textbf{88.5 (1.2)} & \textbf{87.9 (1.1)} & 74.0 (5.6) & 85.9 (2.8) & 91.3 (0.4) \\
SUBG + IRS   & \textbf{82.2 (0.8)} & \textbf{84.1 (0.8)} & \textbf{87.3 (1.3)} & 88.2 (1.2) & 87.6 (1.2) & 70.2 (1.6) & 84.5 (1.0) & \textbf{93.5 (0.4)} \\
\cdashline{1-9}
GR   & - & -  &86.1 (1.3) &89.3 (0.9) &\textbf{95.1 (1.3)}  &\textbf{86.1 (1.3)} &89.3 (0.9) &{95.1 (1.3)}  \\
GR + RS  &{83.7 (0.3)} & {86.8 (0.7)}&\textbf{89.3 (1.3)} &{92.0 (0.7)} & 93.1 (3.2) & 82.2 (1.3) & \textbf{90.8 (0.5)} &{95.4 (1.3)} \\
GR + IRS  & \textbf{84.8 (1.7)} &\textbf{87.4 (0.4)} &89.1 (0.8) & \textbf{92.2 (1.0)} & 92.9 (2.1) &  82.1 (1.4) & 90.5 (0.7) & \textbf{95.6 (0.8)} \\
\cdashline{1-9}
GroupDRO   & - & -  &88.0 (1.0) &92.5 (0.9) &{95.8 (1.8)} &\textbf{88.0 (1.0)} &\textbf{92.5 (0.9)} &{95.8 (1.8)} \\
GroupDRO + RS   & {83.4 (1.1)}&{87.4 (1.4)} &{89.1 (1.7)} &{92.7 (0.8)} & \textbf{96.4 (1.5)} & 80.9 (4.4) & 91.3 (1.0) & \textbf{97.1 (0.3)} \\
GroupDRO + IRS     & \textbf{86.3 (2.3)} & \textbf{90.1 (2.6)} & \textbf{90.8 (1.3)} & \textbf{93.9 (0.2)} & {96.0 (0.6)} & 83.2 (1.7) & 91.5 (0.8) & \textbf{97.1 (0.4)}  \\
\bottomrule
\end{tabular}
}
 \vspace{-0.2cm}
\end{center}

\end{table*}










\iffalse
\section{Discussion}

\paragraph{Why ERM achieves competitive trade-off results to existing baselines?}
We believe this is partly because most of recent existing approaches conduct sample reweighting to focus on the minor groups (Refer to Section~\ref{sec:related}), which may give similar effects to our class-specific scaling as a result. Note that some other papers~\cite{idrissi2022simple, kirichenko2022last} also argue that simple group reweighting or subsampling achieve competitive robust accuracy to other state-of-the-art approaches. However, different from these works, our framework is much more efficient in that it does not require any extra training but yet achieves meaningful performance improvement in robust accuracy.
\fi











\section{Discussion}

\paragraph{Limitation}
Although our framework is simple yet effective for improving target metrics with no extra training, it does not learn debiased representations as it is a post-processing method. 
However, this suggests that existing training approaches may also not actually learn debiased representations, but rather focus on prediction adjustment for group robustness in terms of robust accuracy.
From this point of view, our comprehensive measurement enables a more accurate and fairer evaluation of base algorithms, considering the full landscape of trade-off curve.
%To address this concern, we introduce a comprehensive measurement that enables a more accurate and fairer evaluation of base algorithms, which considers the full landscape of trade-off.
% curve.
%In Appendix~\ref{sec:ars}., we noted the limitation of our attribute-specific robust scaling.




 
 
 
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \iffalse
\section {Additional Figures}


\begin{figure*}[t]
\centering
 \begin{subfigure}[m]{0.47\linewidth}
    	\includegraphics[width=\linewidth]{figures/water_worst_curve_all.png}
	\subcaption{Worst-group accuracy}
	\label{fig:worst_curve_all_water}
	\vspace{0.3cm}
	\end{subfigure} 
	\hspace{0.5cm}
	    \begin{subfigure}[m]{0.47\linewidth}
    	\includegraphics[width=\linewidth]{figures/water_unbias_curve_all.png}
	\subcaption{Unbiased accuracy}
	\label{fig:unbias_curve_all_water}
	\vspace{0.3cm}
	\end{subfigure}
	 \begin{subfigure}[m]{0.47\linewidth}
    	\includegraphics[width=\linewidth]{figures/water_worst_curve_all_pareto}
	\subcaption{Worst-group accuracy}
	\label{fig:worst_curve_all_pareto_water}
	\end{subfigure} 
	\hspace{0.5cm}
	    \begin{subfigure}[m]{0.47\linewidth}
    	\includegraphics[width=\linewidth]{figures/water_unbias_curve_all_pareto}
	\subcaption{Unbiased accuracy}
	\label{fig:unbias_curve_all_pareto_water}
	\end{subfigure}
    \caption{The robust-average accuracy trade-off curves ((a), (b)) and the corresponding robust coverage curves ((c), (d)), respectively, on the Waterbirds dataset.
    In (a) and (b), the black markers indicate where the scaling factor $\mathbf{s}=\mathbf{1}$.
    }
    %\vspace{-0.5cm}
    \label{fig:robust_all_water}
\end{figure*}



 \begin{figure*}[t]
\centering
    \begin{subfigure}[m]{0.43\linewidth}
    	\includegraphics[width=\linewidth]{figures/water_worst_scale.png}
	\subcaption{Worst-group accuracy}
	\label{fig:tsne_noise}
    \end{subfigure} 
    	\hspace{0.8cm}
        \begin{subfigure}[m]{0.43\linewidth}
    	\includegraphics[width=\linewidth]{figures/water_unbias_scale.png}
	\subcaption{Unbiased accuracy}
	\label{fig:tsne_erm}
	\end{subfigure} 
%\vspace{-0.1cm}
    \caption{Effects of varying the class-specific scaling factors on the robust accuracy using ERM model on the Waterbirds dataset. Since this experiment is based on the binary classifier, a single scaling factor is varied with the other fixed to one. 
    Like as other datasets, we can identify the desired performance point on the trade-off between the robust and average accuracies in the test set using the scaling factor learned from the validation set.
    }
    %\vspace{-0.5cm}
    \label{fig:observation_scale_curve_water}
\end{figure*}

\paragraph{Visualization of the trade-off on Waterbirds dataset }
We additionally visualize the robust-average accuracy trade-off and its corresponding robust coverage curves with several existing approaches on the Waterbirds dataset in Fig.~\ref{fig:robust_all_water}.



\paragraph{Effects of class-specific scaling on Waterbirds dataset}
Fig.~\ref{fig:observation_scale_curve_water} presents the effect of the class-specific scaling on the Waterbirds dataset, which provides a consistent tendency with CelebA dataset (Fig.~\ref{fig:observation_scale_curve}) that the curves in validation and test splits are aligned sufficiently well.
\fi


%\section{Discussion}

%\vspace{-2mm}
%\paragraph{Comparison to reweighting or resampling techniques}
%As mentioned in Section~\ref{sec:related}, most existing debiasing techniques~\cite{GroupDRO, JTT, LfF, seo2021unsupervised, idrissi2022simple, kirichenko2022last}, in principle, perform reweighting and/or resampling of training data.
%Our approach has a similar idea, but, instead of giving favor to the examples in minority groups during training and boosting their classification scores indirectly via iterative model updates, we directly adjust their classification scores by class-wise scaling after training, thus it gives similar but clearer effects on the results.
%%Please refer to Figure~\ref{fig:robust_all_celeba} that class reweighting (CR) also achieved almost the same results as one of the Pareto optimal points of the trade-off curve of ERM.
%%provided almost the same Pareto curve as ERM, which implies that CR identifies 
%As shown in Figure~\ref{fig:robust_all_celeba}, although class reweighting (CR) improves the robust accuracy, this in fact identifies one of the Pareto optimal points on the trade-off curve of ERM obtained by class-specific scaling.
%However, because class reweighting employs a single fixed reweighting factor during training based on class frequency, it only reflects a single point and has limited flexibility compared to our wide range of scaling search.
%If CR employs a wide range of reweighting factors, then it can identify additional optimal points and achieve additional performance gains, but it requires training separate models for each factor, which is not realistic.
%% If the reweighting factor is too large or small, it makes training unstable.
%Note that our method can be easily applied to CR or other methods, which allows us to identify more desirable optimal points on the trade-off curve with negligible computational overhead.

%To identify the scaling factors, our approach requires a held-out validation set and a greedy search of scaling factors.
%\paragraph{Limitation}
%Although our framework is simple yet effective for improving target metrics with no extra training, it does not learn debiased representations as it is a post-processing method. 
%However, this suggests that existing training approaches may also not actually learn debiased representations, but rather focus on prediction adjustment for group robustness in terms of robust accuracy.
%To address this concern, we introduce a comprehensive measurement that enables a more accurate and fairer evaluation of base algorithms, which considers the full landscape of trade-off curve.
%In Appendix~\ref{sec:ars}., we noted the limitation of our attribute-specific robust scaling.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

