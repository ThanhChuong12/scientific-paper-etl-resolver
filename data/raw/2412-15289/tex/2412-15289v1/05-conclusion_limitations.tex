\section{Conclusion} % 不能到 page 9

We present a LLM jailbreak paradigm via simple assistive task linkage. We employ Mask Language Model and Element Lookup by Position as assistive tasks in the paradigm, and introduce SATA-MLM and SATA-ELP jailbreak attack. 
We show that SATA achieves state-of-the-art performance compared to strong baselines across latest closed-source and open-sourced models (\eg GPT-4o), demonstrating the effectiveness of the paradigm on AdvBench and JBB-Behaviors datasets. % address limitation 2 % , with assistive tasks,
Furthermore, SATA is cost-efficient for its average input token usage when performing jailbreak. % address limitation 1
We hope our study can contribute to building safer LLMs in collaboration with the entire community.




\section{Limitations}

While our jailbreak paradigm is effective, we conjecture that they could be mitigated through the chain-of-thought (CoT)~\cite{weiChainofThoughtPromptingElicits} generation process (\eg GPT-4-o1). The reason is that the CoT may evaluate the intent of a query in a holistic manner after completing the assistive task, potentially exposing the malicious intent of the harmful instruction during the intermediate steps and causing the LLM to refuse to respond to the jailbreak query. However, due to the API access restriction, we cannot evaluate the jailbreak attacks on GPT-4-o1.

Furthermore, this study remains largely empirical and lacks interpretability. It would be interesting to analyze how the internal represenations~\cite{arditi2024refusal} of LLM shift with SATA and we leave it to future work.

% This lack of interpretability can be attributed to the complexity of large language models (LLMs), which contain billions of parameters.
% 极少数情况下，因为安全原因，LLM会拒绝创建关于masked contents的wiki词条。



\section{Ethical Consideration}
This work presents a paradigm for automatically generating jailbreak prompts to elicit harmful content from closed-source and open-source LLMs. The aim of our work is to strengthen LLM safety as well as highlight the importance of continuously improving the safety alignment of LLMs. However, the techniques presented in this work can be exploited by any dedicated team that attempts to utilize LLMs for harmful purposes. 

Despite the risks involved, we believe it is important to fully disclose our study to foster discussions on the vulnerabilities of LLMs revealed through our jailbreaks and to encourage collaboration across the AI community to develop more countermeasures and safety protocols that can prevent the exploitation of LLMs for malicious activities. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5

% We conclude two main limitations of this paper: 

% 资源有限，只选了representative victim modlels，没有跑更多victim models
% (1) Due to budget constraints, we primarily evaluate SATL paradigm on prevalent state-of-the-art LLMs, and we focus on single-turn jailbreak prompt generation in generation.

% 防御方面不能抵抗CoT范式的生成方式
% (2) We conjecture that BAZINGA jailbreak can fail if the generation of LLM response is performed in a chain-of-thought (CoT) manner (\eg gpt4-o1). For example, we adapt AdaShield-S\footnote{AdaShield is a reminder-based multimodal LLM jailbreak defense method, which simplify and simulate the CoT process by firstly evaluating the whole semantic / intention of the adversarial prompt, and then deciding whether to respond the query.} to defend against BAZINGA as well as baselines, and we find all these method become invalid (see the detailed evaluation results in Appendix YYYY). In future work, we will extend the idea to jailbreak such CoT paradigm.