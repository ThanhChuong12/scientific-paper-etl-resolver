\section{Experimental Evaluations}
\label{sec:ExpEval}

\input{tables/tab-main-table}             % 调整主表位置！！！
\input{experiment_setup}
\subsection{Main Results}           % main experimental results
\label{subsec:ExpRes}
\paragraph{Attack Effectiveness.}
% 1.有效
% 2. structure-based attack becomes performance stumb block when the victim model is small size.
% 3. 又在JBB上做了测试：选3个victim models。Experiment Choice: 要不要和baseline模型比——如果比，那就是consistently outperform baselines; 如果不比，那就是consistently achieve surprising jailbreak ASR and HS.
We first evaluate SATA against baselines on AdvBench. As shown in Table~\ref{tab:main-table}, SATA achieves state-of-the-art performance compared baselines across all victim LLMs in HS and ASR, respectively, indicating the effectiveness of SATA.
Specifically, we observe that:~(1)~With \texttt{ensemble} configuration, SATA-MLM attains an overall ASR of \pt{85} and an overall HS of 4.57, significantly outperforming baselines;~(2)~With the \texttt{top-1} configuration, SATA-MLM can outperform the strongest baseline with its  \texttt{ensemble} configuration;~(3)~SATA-MLM is generally more effective  than SATA-ELP across all victim models, except Claude-v2. %, which suggests the two assistive tasks can achieve performance complementarity.
We provide qualitative examples of our jailbreak results in Appendix~\ref{app:jailbreak-result-examples}.

\input{tables/tab-JBB-result-table}
We further evaluate SATA on JBB-Behaviros. % to verify whether SATA can sustain its effectiveness. 
As shown in Table~\ref{tab:JBB-result-table}, SATA  maintains its performance comparing to the ArtPrompt baseline. For instance, SATA-MLM and SATA-ELP achieve an overall ASR of \pt{75} and \pt{72} on GPT-4o model, respectively. The performance drop mainly arise from the Harassment/Discrimination and Sexual/Adult content Category in JBB dataset. 
% SATA can maintain its ASR with performance fluctuation ranging from \pt{-17} to \pt{+15}. \todo{(Re-use wiki entry caused, will re-run)}

\input{tables/tab-main-defense}
\paragraph{Performance Against Defenses.}
% 1. robust to perturbation (paraphrase and retokenization)
% 2. stealthy to perplexiity-detection filter
We evaluate the performance of SATA against windowed PPL-Filter and Paraphrase jailbreak defenses, and compare to state-of-the-art baseline, with the results shown in Table~\ref{tab:main-defense}. Our observations are as follows:~(1)~The perplexity-based detection only minimally reduce the jailbreak performance, demonstrating that SATA is stealthy to bypass windowed PPL-Filter defense.~(2)~Paraphrase is somewhat more effective than PPL-Filter to defense SATA jailbreak attack, causing an average drop of ~\pt{12} ASR for SATA-ELP and ~\pt{19} ASR for SATA-MLM. (3) SATA consistently elicits toxic response and outperforms ArtPrompt under windowed PPL-Filter and paraphrase defense, achieving an average of ASR~\pt{63} and~\pt{58} for SATA-ELP and SATA-MLM, respectively. Finally, we compare the paraphrased adversarial prompt to the original one, and find that the paraphrase defense works by summarizing the wiki entry content and disrupting the wiki entry text-infilling format.
% However, SATA consistently elicit toxic response with an average HS of 4.01 and 3.95 and an average of ASR~\pt{63} and~\pt{58} for SATA-ELP and SATA-MLM, respectively. Additionally, it is particularly effective for claude-v2. We compare the paraphrased adversarial prompt to the original one, and find that the paraphrase defense works by summarizing the wiki entry content and disrupting the wiki entry text-infilling format.

\input{graphs/gh_def-ppl}
To further study the stealthiness of SATA, we visualize the perplexity values computed on GPT-2 in Figure~\ref{fig:def-ppl}. We can observe that, with a small window size (\texttt{max\_length=5}), the perplexities of GPT-2 for the adversarial prompt generated by SATA consistently remain below the threshold, regardless of the chosen assistive task (MLM, ELP) or masking granularity. Furthermore, the adversarial prompt generated by SATA-MLM demonstrate lower perplexity compared to those generated by SATA-ELP, indicating that SATA-MLM is more stealthy. Finally, if we exclude the outliers in harmful instructions and decrease \texttt{T=138.56} (see the dark dashed line), SATA can still bypass the windowed PPL-Filter in most settings. % investigate, more stealthy than those from ELP in terms of perplexity.

We attribute the stealthiness of SATA to two factors. First, the wiki text entry is synthesized by articulated LLMs and there is no opaque sub-string (gibberish) in adversarial prompt. Second, the commendatory words \texttt{List} in the adversarial prompt of SATA-ELP is not long (about ten words). % and meaningless

\input{tables/tab-adashield}
We also investigate whether SATA can defend against prompt-based jailbreak defense. We adopt AdaShield-S~\cite{wangAdaShieldSafeguardingMultimodal2025} to evaluate SATA and ArtPrompt for the Llama3 model, with results shown in Table~\ref{tab:adashield}. Interestingly, we find both attacks experience a large ASR drop, while SATA-MLM gives a relatively decent ASR of 24\%. 

\paragraph{Efficiency Analysis.}
\label{paragraph:cost}

SATA is lightweight in terms of the number of iterations, jailbreak prompt candidates, and jailbreak prompt length. These three factors collectively influence the input token usage, which serves as an indicator of the average inference time cost or economic cost (when invoking API) for a jailbreak. % adversarial prompt candidates
We calculate the average input token usage\footnote{To simplify,  we opt to calculate and report the word count,  as the token count and word count can be approximately linear.} for different jailbreak methods (see Appendix~\ref{app:input_token_usage} for detailed calculation process), and compare SATA to the baselines, with results shown in Figure~\ref{fig:cost-tokens}. We can observe that SATA-MLM consumes comparable or less input tokens compared with state-of-the-art baselines (ArtPrompt) while it attains significant higher jailbreak HS and ASR (see Table~\ref{tab:main-table}). In addition, SATA-ELP achieves a significant reduction in input token usage, reaching about an order of magnitude savings, while maintaining state-of-the-art jailbreak performance. Lastly, we observe from Figure~\ref{fig:appendix-jailbreak-WET-attack} and~\ref{fig:appendix-jailbreak-ELP-mw} in Appendix~\ref{app:jailbreakprompt} that the jailbreak prompt is designed to be simple, requiring minimal human design effort, and the input token usage in SATA-MLM primarily stems from the synthesized wiki entry.
Theses observations showcase SATA is cost-efficient for both jailbreak and human-effort.

The main reasons for its cost-efficiency are: (1) there is no need for multiple iterations or jailbreak prompt candidates; (2) masking harmful keywords by LLMs avoids multiple tries; (3) the synthesized wiki entry is limited to six paragraphs, whereas retrieving a wiki entry from Wikipedia is often unbearably long; and (4) assistive tasks are designed to be friendly for victim LLMs to perform. % understand and perform (4)所以prompt可以写得很简单
\input{graphs/gh_cost-tokens}

\subsection{Ablation Study}     % ablation study results
\label{subsec:ablation}
We conduct ablation studies to analyze the impact of the following factors on jailbreak performance. Due to budget constraints, we primarily select GPT-3.5-turbo and Llama-3-8B as our victim models and Advbench as dataset for the ablation study.

\paragraph{Impact of the Insert Position of Harmful Keywords in the Sequence.}
\label{paragraph:ab-position}
% 此Ablation要说明随机位置对Jailbreak性能影响很大，empirically前一半位置
% 最终证明要选择简单任务(LLM擅长的任务)参与任务连接，只有这样才能准确恢复语义信息,实现harmful instruction的语义重建
Although ELP is relatively simple, LLMs can still fail to identify the correct element in the commendatory words \texttt{List}, though infrequently. Empirically, this issue becomes slightly pronounced when the insert position is closer to the end of the \texttt{List}. 
We tune the ELP task to be a little bit more difficult for victim LLMs to perform by forcibly shifting the insert position to the latter half of the \texttt{List}, and we analyze the impact of the insert position of the masked keywords in \texttt{List} on performance. We consider the single-word and single-phrase masking granularity.

As shown in Table~\ref{tab:ablation-position}, when the insert position is forcibly shifted to the latter half, the ASR occasionally experiences a moderate drop in the ablation experiment settings, demonstrating that assistive tasks should remain simple to ensure that the semantics conveyed by the assistive task do not deviate from the harmful keywords. % from the first half
\input{tables/tab-ablation-position}



\paragraph{Effectiveness of Constructing an Assistive Task.}
\label{paragraph:ab-necessity}
% 此Ablation通过case study ELP来说明任务连接的必要性
We investigate the effectiveness of assistive task. Specifically, we replace the ELP task with directly informing the victim LLMs of the masked keywords and term this approach as \texttt{Inform}. As shown in Table~\ref{tab:ablation-assistive-task}, the jailbreak performance drops drastically, indicating that constructing an addition assistive task is effective for jailbreaking.
% 原因可能是辅助任务让LLM分神，没了辅助任务就没有distraction了。
\input{tables/tab-ablation-assistive-task}





% \paragraph{Synthesize Wiki Entry v.s. Retrieve Wiki Entry.}
% \label{paragraph:ab-wiki}
% % 此Ablation要说明规整的wiki更容易提升performance，但是词条多义对jailbreak性能影响不大。



%%%%%%%%%%%%%%%%%%%%
% WET上mw mp一直好； ELP上各个粒度是性能互补的
% We demonstrate the impact of the four masking granularity on jailbreak performance, and we show the details of WET attack in Table~\ref{tab:ablation-mask-granularity}.
% We can observe that the four mask granularity can provide performance complementarity across victim models in the ELP-attack.