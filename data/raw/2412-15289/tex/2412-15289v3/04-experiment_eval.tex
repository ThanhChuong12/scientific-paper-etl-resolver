\secvspace
\section{Experiments}
\label{sec:ExpEval}
\subsecvspace
\input{experiment_setup}
\subsecvspace
\subsection{Main Results}           % main experimental results
\label{subsec:ExpRes}
\paragraph{Attack Effectiveness.}
% 1.有效
% 2. structure-based attack becomes performance stumb block when the victim model is small size.
% 3. 又在JBB上做了测试：选3个victim models。Experiment Choice: 要不要和baseline模型比——如果比，那就是consistently outperform baselines; 如果不比，那就是consistently achieve surprising jailbreak ASR and HS.
We first evaluate SATA against baselines on AdvBench. As shown in Table~\ref{tab:main-table}, SATA achieves superior performance compared to strong baselines across all victim LLMs in HS and ASR, respectively, indicating the effectiveness of SATA.
Specifically, we observe that:~(1)~With the \texttt{ensemble} configuration, SATA-MLM attains an overall ASR of \pt{85} and an overall HS of 4.57, significantly outperforming baselines;~(2)~With the \texttt{top-1} configuration, SATA-MLM can outperform the strongest baseline with \texttt{ensemble} configuration;~(3)~SATA-MLM is generally more effective  than SATA-ELP across all victim models, except Claude-v2. %, which suggests the two assistive tasks can achieve performance complementarity.
We provide qualitative examples of the jailbreak efficacy in Appendix~\ref{app:jailbreak-result-examples}.

% \input{tables/tab-JBB-result-table}
% We further evaluate SATA on JBB-Behaviors. % to verify whether SATA can sustain its effectiveness. 
% As shown in Table~\ref{tab:JBB-result-table}, SATA maintains its superior performance compared to DrAttack and ArtPrompt. For instance, SATA-MLM and SATA-ELP achieve an overall ASR of \pt{75} and \pt{72} on GPT-4o, respectively. The performance drop primarily stems from the Harassment/Discrimination and Sexual/Adult content categories in the JBB dataset.
% % SATA can maintain its ASR with performance fluctuation ranging from \pt{-17} to \pt{+15}. \todo{(Re-use wiki entry caused, will re-run)}

\input{graphs/gh_JBB-result-radar}
We further evaluate SATA on JBB-Behaviors. % to verify whether SATA can sustain its effectiveness. 
As shown in Figure~\ref{fig:jbb-result}, SATA maintains its superior performance compared to DrAttack and ArtPrompt. For instance, SATA-MLM and SATA-ELP achieve an overall ASR of \pt{75} and \pt{72} on GPT-4o, respectively. The performance drop primarily stems from the Harassment/Discrimination and Sexual/Adult content categories in the JBB dataset.
% SATA can maintain its ASR with performance fluctuation ranging from \pt{-17} to \pt{+15}. \todo{(Re-use wiki entry caused, will re-run)}


\paravspace
\paragraph{Effectiveness of Jailbreaking Reasoning LLMs} 
We use SATA to jailbreak reasoning LLMs on the AdvBench dataset and compare it to ArtPrompt (which has the strongest performance in the previous experiments). Specifically, we select DeepSeek-R1 and OpenAI o3-mini as victims and evaluate with \texttt{ensemble} configuration. 
As shown in Table~\ref{tab:R1o3-mini}, both attacks jailbreak Deepseek-R1 with high HS and ASR, while SATA-MLM consistently outperforms ArtPrompt. In addition, SATA-MLM, with an ASR of \pt{40}, is significantly superior to ArtPrompt when jailbreaking OpenAI o3-mini. Our evaluation may indicate that reasoning LLMs with the chain-of-thought generation process cannot directly mitigate the SATA jailbreak.
\input{tables/tab-R1o3}

\paragraph{Underlying Mechanism of SATA}
We begin by analyzing the role of the \texttt{[MASK]} token in enhancing SATA’s effectiveness. Specifically, we first take the jailbreak prompt generated by SATA-MLM and substitute the \texttt{[MASK]} token with the original harmful keyword, while keeping all other content unchanged (i.e., we restore the masked harmful instruction in the prompt). We then compare the attack success rate (ASR) of these modified prompts against the original SATA-MLM prompts. This analysis is conducted on two victim models, Llama3-8B and OpenAI o3-mini, using the \texttt{single-word} masking granularity. 

As shown in Table~\ref{tab:rebuttal-mask}, replacing the \texttt{[MASK]} token with the original harmful keyword leads to a substantial drop in ASR, highlighting the importance of the \texttt{[MASK]} token in mitigating toxicity and enhancing the stealthiness of SATA-MLM jailbreaks.
\input{tables/tab-rebuttal-Internal-Repre}

We then examine how the internal representations of LLMs shift under the SATA jailbreak attack. Specifically, for each sample in the AdvBench dataset, we first construct a pair of prompts: \texttt{Prompt-1} is the jailbreak prompt generated by SATA-MLM, and \texttt{Prompt-2} is derived by substituting the \texttt{[MASK]} token in \texttt{prompt-1} with the original harmful keyword. We then compute the cosine similarity (ranging from [-1, 1]) between the hidden states of the \texttt{[MASK]} token in \texttt{prompt-1} and the corresponding harmful keyword token in \texttt{prompt-2}, across all layers of the model. 
We use Llama3-8B (with 32 layers) as the victim model since the analysis requires access to intermediate hidden states, and we only choose successful jailbreak examples so that we can explore the reasons for SATA's effectiveness. For this analysis, we randomly sample twenty successful examples from AdvBench dataset, and we average the similarity values over the chosen twenty examples layer-wise, yielding one average similarity value per layer.

As illustrated in Figure~\ref{fig:similarity}, the average similarity between the \texttt{[MASK]} token and the harmful keyword increases progressively across layers\footnote{We manually inspect each case and observe consistent trends.}, suggesting that the victim model increasingly interprets the \texttt{[MASK]} token as the intended harmful keyword. This finding underscores the effectiveness of the MLM assistive task in guiding the model to semantically reconstruct harmful content within the jailbreak prompt.
\input{graphs/gh_rebuttal-similarity}

In summary, masking harmful keywords in harmful instruction with the \texttt{[MASK]} token reduces their surface-level toxicity, while the assistive task enables the LLM to internally infer the semantics of \texttt{[MASK]} as the original harmful keyword. This mechanism helps explain the effectiveness of the SATA jailbreak attack.


\paravspace
\paragraph{Robustness Against Defenses.} % Performance
\input{tables/tab-main-defense}
% 1. robust to perturbation (paraphrase and retokenization)
% 2. stealthy to perplexiity-detection filter
We evaluate the performance of SATA against windowed PPL-filter, paraphrase, self-reminder and RPO defenses, and compare to baseline, with results shown in Table~\ref{tab:main-defense}. Our observations are as follows:~(1)~The perplexity-based detection fails to mitigate the SATA jailbreak, demonstrating that SATA is stealthy to bypass windowed PPL-filter defense.~(2)~RPO is ineffective in defending against SATA jailbreak, resulting in an absolute ASR drop of \pt{10} for SATA-MLM and \pt{9} for SATA-ELP on average. Similarly, paraphrase slightly reduces the jailbreak performance. We compare the paraphrased adversarial prompt to the original one, and find that the paraphrase defense works by summarizing the wiki entry content and disrupting the text-infilling format.~(3)~Interestingly, we also find both attacks experience a large ASR drop under self-reminder defense, while SATA-MLM gives a relatively decent average ASR of 17\%. Overall, SATA consistently elicits toxic response and outperforms ArtPrompt under the \textit{windowed PPL-filter}, \textit{paraphrase}, \textit{self-reminder} and \textit{RPO} defenses, achieving an average ASR of~\pt{77},~\pt{63},~\pt{17} and~\pt{67}, respectively.
% However, SATA consistently elicit toxic response with an average HS of 4.01 and 3.95 and an average of ASR~\pt{63} and~\pt{58} for SATA-ELP and SATA-MLM, respectively. Additionally, it is particularly effective for claude-v2. We compare the paraphrased adversarial prompt to the original one, and find that the paraphrase defense works by summarizing the wiki entry content and disrupting the wiki entry text-infilling format.
\input{graphs/gh_def-ppl}

To further study the stealthiness of SATA, we visualize the perplexity values computed on GPT-2~\cite{radfordLanguageModelsAre} in Figure~\ref{fig:def-ppl}. We can observe that, with a small window size (\texttt{max\_length=5}), the perplexities of GPT-2 for the adversarial prompt generated by SATA consistently remain below the threshold, regardless of the chosen assistive task (MLM, ELP) or masking granularity. Furthermore, the adversarial prompts generated by SATA-MLM exhibit lower perplexity compared to those generated by SATA-ELP, indicating that SATA-MLM is more stealthy. Finally, if we exclude the outliers in harmful instructions and decrease \texttt{T=138.56} (see the dark dashed line), SATA can still bypass the windowed PPL-filter in most settings. % investigate, more stealthy than those from ELP in terms of perplexity.

We attribute the stealthiness of SATA to two factors. First, the wiki entry is synthesized by articulated LLMs, ensuring that no opaque substrings appear in the adversarial prompt. Second, in the case of SATA-ELP, the commendatory words \texttt{List} within the adversarial prompt is relatively short, consisting of approximately ten words. % and meaningless,  (\ie gibberish)
% \input{graphs/gh_def-ppl}

Beyond evaluating SATA against the four recent or widely adopted defense techniques, we also investigate the impact of the Retrieval-Augmented Generation (RAG) scheme—commonly used in real-world LLM deployments—on SATA's robustness. Detailed analysis and results are provided in Appendix~\ref{app:rag-as-defense}.


\paravspace
\paragraph{Efficiency Analysis.}
\label{paragraph:cost}
SATA is lightweight in terms of the number of iterations, jailbreak prompt candidates, and jailbreak prompt length. These three factors collectively impact input token usage, which serves as a more fundamental indicator of the average inference time cost or economic cost (when invoking API) for a jailbreak. % an indicator, adversarial prompt candidates
We calculate the average input token usage\footnote{To simplify, we opt to calculate and report the word count, as the token count and word count can be approximately linear.} for various jailbreak methods (see Appendix~\ref{app:input_token_usage} for detailed calculation process), and compare SATA to the baselines, with results shown in Figure~\ref{fig:cost-tokens}. We observe that SATA-MLM consumes comparable or less input tokens compared to ArtPrompt while it attains significant higher jailbreak HS and ASR (see Table~\ref{tab:main-table}). In addition, SATA-ELP achieves a significant reduction in input token usage, reaching about an order of magnitude savings, while maintaining state-of-the-art jailbreak performance. Lastly, we observe from Figure~\ref{fig:appendix-jailbreak-WET-attack} and~\ref{fig:appendix-jailbreak-ELP-mw} in Appendix~\ref{app:jailbreakprompt} that the jailbreak prompt template is designed to be concise, requiring minimal human design effort, and the input token usage in SATA-MLM primarily originates from the synthesized wiki entry.
Theses observations showcase SATA is cost-efficient. % for jailbreak.% both jailbreak and human-effort.

\input{graphs/gh_cost-tokens}

The cost-efficiency of SATA comes from: (1) the workflow of SATA eliminates the need for multiple iterations and jailbreak prompt candidates; (2) leveraging LLMs to mask all harmful keywords at once avoids the need for multiple trials; (3) the length of synthesized wiki entries are restricted to six paragraphs, whereas those retrieved from Wikipedia are often excessively long.


\subsecvspace
\subsection{Ablation Study}     % ablation study results
\label{subsec:ablation}
We conduct ablation studies to analyze the impact of the following factors on jailbreak performance. Due to budget constraints, we primarily select GPT-3.5-turbo and Llama-3-8B as our victim models and conduct experiments with single-word and single-phrase masking granularity on Advbench.

\paravspace
\paragraph{Impact of the Insert Position of Harmful Keywords in the Sequence.}
\label{paragraph:ab-position}
% 此Ablation要说明随机位置对Jailbreak性能影响很大，empirically前一半位置
% 最终证明要选择简单任务(LLM擅长的任务)参与任务连接，只有这样才能准确恢复语义信息,实现harmful instruction的语义重建
Although ELP is relatively simple, LLMs may still occasionally fail to identify the correct element in the commendatory words \texttt{List}. Empirically, this issue becomes slightly pronounced when the insert position is closer to the end of the \texttt{List}.  
To introduce a controlled increase in task difficulty for the victim LLMs, we deliberately shift the insert position to the latter half of the \texttt{List} and analyze the impact of masked keyword placement on performance. % We consider both single-word and single-phrase masking granularity.  
\input{tables/tab-ablation-position}

As shown in Table~\ref{tab:ablation-position}, forcing the insert position toward the latter half leads to a moderate drop in ASR. This highlights the importance of keeping assistive tasks simple to ensure that the semantics conveyed by assistive task remain aligned with the intended harmful keywords. % drop in ASR under ablation experiment settings.

\paravspace
\paragraph{Effectiveness of Constructing an Assistive Task.}
\label{paragraph:ab-necessity}
% 此Ablation通过case study ELP来说明任务连接的必要性
We evaluate the effectiveness of the assistive task by replacing the MLM and ELP task with directly informing the victim LLMs of masked keywords, respectively. As shown in Table~\ref{tab:ablation-assistive-task}, the jailbreak performance drops drastically in both cases, highlighting the importance of constructing an additional assistive task to effectively encodes and conveys the semantics of harmful keywords.
% 原因可能是辅助任务让LLM分神，没了辅助任务就没有distraction了。
\input{tables/tab-ablation-assistive-task}








% \paragraph{Synthesize Wiki Entry v.s. Retrieve Wiki Entry.}
% \label{paragraph:ab-wiki}
% % 此Ablation要说明规整的wiki更容易提升performance，但是词条多义对jailbreak性能影响不大。



%%%%%%%%%%%%%%%%%%%%
% \paragraph{Performance Against Defenses.}
% We evaluate the performance of SATA against windowed PPL-filter and paraphrase jailbreak defenses, and compare to the baselines, with the results shown in Table~\ref{tab:main-defense}. Our observations are as follows:~(1)~The perplexity-based detection only minimally reduce the jailbreak performance, demonstrating that SATA is stealthy to bypass windowed PPL-filter defense.~(2)~Paraphrase is somewhat more effective than PPL-filter to defend against SATA jailbreak attack, causing an absolute drop of ~\pt{12} ASR for SATA-ELP and ~\pt{19} ASR for SATA-MLM on average. (3) SATA consistently elicits toxic response and outperforms DrAttack and ArtPrompt under the four chosen defenses, achieving an average of ASR~\pt{63} and~\pt{58} for SATA-ELP and SATA-MLM, respectively. Finally, we compare the paraphrased adversarial prompt to the original one, and find that the paraphrase defense works by summarizing the wiki entry content and disrupting the wiki entry text-infilling format.

%%%%%%%%%%%%%%%%%%%%
% \paragraph{Efficiency Analysis.}
% The main reasons for its cost-efficiency are: (1) there is no need for multiple iterations or jailbreak prompt candidates; (2) masking harmful keywords by LLMs avoids multiple tries; (3) the synthesized wiki entry is limited to six paragraphs, whereas retrieving a wiki entry from Wikipedia is often unbearably long; and (4) assistive tasks are designed to be friendly for victim LLMs to perform. % understand and perform (4)所以prompt可以写得很简单

%%%%%%%%%%%%%%%%%%%%
% Ablation
% Although ELP is relatively simple, LLMs can still fail to identify the correct element in the commendatory words \texttt{List}, though infrequently. Empirically, this issue becomes slightly pronounced when the insert position is closer to the end of the \texttt{List}. 
% We tune the ELP task to be a little bit more difficult for victim LLMs to perform by forcibly shifting the insert position to the latter half of the \texttt{List}, and we analyze the impact of the insert position of the masked keywords in \texttt{List} on performance. We consider the single-word and single-phrase masking granularity.

% As shown in Table~\ref{tab:ablation-position}, when the insert position is forcibly shifted to the latter half, the ASR occasionally experiences a moderate drop in the ablation experiment settings, demonstrating that assistive tasks should remain simple to ensure that the semantics conveyed by the assistive task do not deviate from the harmful keywords. % from the first half


%%%%%%%%%%%%%%%%%%%%
% WET上mw mp一直好； ELP上各个粒度是性能互补的
% We demonstrate the impact of the four masking granularity on jailbreak performance, and we show the details of WET attack in Table~\ref{tab:ablation-mask-granularity}.
% We can observe that the four mask granularity can provide performance complementarity across victim models in the ELP-attack.

%%%%%%%%%%%%%%%%%%%%
% AdaShield
% \input{tables/tab-adashield}
% We also investigate whether SATA can defend against prompt-based jailbreak defense. We adopt AdaShield-S~\cite{wangAdaShieldSafeguardingMultimodal2025} to evaluate SATA and ArtPrompt for the Llama3 model, with results shown in Table~\ref{tab:adashield}. Interestingly, we find both attacks experience a large ASR drop, while SATA-MLM gives a relatively decent ASR of 24\%. 