
@misc{openai2024gpt4technicalreport,
      title={{GPT-4 Technical Report}}, 
      author={OpenAI and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman, et al.},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}, 
}


@misc{lu2024emergentabilitieslargelanguage,
      title={{Are Emergent Abilities in Large Language Models just In-Context Learning?}}, 
      author={Sheng Lu and Irina Bigoulaeva and Rachneet Sachdeva and Harish Tayyar Madabushi and Iryna Gurevych},
      year={2024},
      eprint={2309.01809},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.01809}, 
}

@misc{wei2023chainofthoughtpromptingelicitsreasoning,
      title={{Chain-of-Thought Prompting Elicits Reasoning in Large Language Models}}, 
      author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},
      year={2023},
      eprint={2201.11903},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2201.11903}, 
}

@inproceedings{NEURIPS2022_8bb0d291,
 author = {Kojima, Takeshi and Gu, Shixiang (Shane) and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {22199--22213},
 publisher = {Curran Associates, Inc.},
 title = {{Large Language Models are Zero-Shot Reasoners}},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/8bb0d291acd4acf06ef112099c16f326-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

@misc{nezhurina2024alicewonderlandsimpletasks,
      title={{Alice in Wonderland: Simple Tasks Showing Complete Reasoning Breakdown in State-Of-the-Art Large Language Models}}, 
      author={Marianna Nezhurina and Lucia Cipolina-Kun and Mehdi Cherti and Jenia Jitsev},
      year={2024},
      eprint={2406.02061},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.02061}, 
}

@article{gardenfors2023reasoning,
  title={{Reasoning with concepts: A unifying framework}},
  author={G{\"a}rdenfors, Peter and Osta-V{\'e}lez, Mat{\'\i}as},
  journal={Minds and Machines},
  volume={33},
  number={3},
  pages={451--485},
  year={2023},
  publisher={Springer}
}

@inproceedings{Tang_2023,
   title={{Large Language Models Can be Lazy Learners: Analyze Shortcuts in In-Context Learning}},
   url={http://dx.doi.org/10.18653/v1/2023.findings-acl.284},
   DOI={10.18653/v1/2023.findings-acl.284},
   booktitle={Findings of the Association for Computational Linguistics: ACL 2023},
   publisher={Association for Computational Linguistics},
   author={Tang, Ruixiang and Kong, Dehan and Huang, Longtao and Xue, Hui},
   year={2023} }

@article{WANG201081,
title = {{On the cognitive process of human problem solving}},
journal = {Cognitive Systems Research},
volume = {11},
number = {1},
pages = {81-92},
year = {2010},
note = {Brain Informatics},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2008.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S1389041708000417},
author = {Yingxu Wang and Vincent Chiew},
keywords = {Cognitive informatics, Cognitive computing, Brain informatics, Computational intelligence, Reference model of the brain, Cognitive processes, Problem solving, Mathematical model, Concept algebra, RTPA},
abstract = {One of the fundamental human cognitive processes is problem solving. As a higher-layer cognitive process, problem solving interacts with many other cognitive processes such as abstraction, searching, learning, decision making, inference, analysis, and synthesis on the basis of internal knowledge representation by the object–attribute-relation (OAR) model. Problem solving is a cognitive process of the brain that searches a solution for a given problem or finds a path to reach a given goal. When a problem object is identified, problem solving can be perceived as a search process in the memory space for finding a relationship between a set of solution goals and a set of alternative paths. This paper presents both a cognitive model and a mathematical model of the problem solving process. The cognitive structures of the brain and the mechanisms of internal knowledge representation behind the cognitive process of problem solving are explained. The cognitive process is formally described using real-time process algebra (RTPA) and concept algebra. This work is a part of the cognitive computing project that designed to reveal and simulate the fundamental mechanisms and processes of the brain according to Wang’s layered reference model of the brain (LRMB), which is expected to lead to the development of future generation methodologies for cognitive computing and novel cognitive computers that are capable of think, learn, and perceive.}
}

@book{Pizlo2022-PIZPSC,
	author = {Zygmunt Pizlo},
	editor = {},
	publisher = {Cambridge University Press},
	title = {{Problem Solving: Cognitive Mechanisms and Formal Models}},
	year = {2022}
}

@incollection{Laurence1999-LAUCAC-3,
	author = {Stephen Laurence and Eric Margolis},
	booktitle = {{Concepts: Core Readings}},
	editor = {Eric Margolis and Stephen Laurence},
	pages = {3--81},
	publisher = {MIT Press},
	title = {Concepts and Cognitive Science},
	year = {1999}
}

@book{sternberg1980reasoning,
  title={{Reasoning, problem solving, and intelligence}},
  author={Sternberg, Robert J},
  year={1980},
  publisher={Canada Institute for Scientific and Technical Information}
}

@article{DBLP:journals/corr/abs-2106-09685,
  author       = {Edward J. Hu and
                  Yelong Shen and
                  Phillip Wallis and
                  Zeyuan Allen{-}Zhu and
                  Yuanzhi Li and
                  Shean Wang and
                  Weizhu Chen},
  title        = {{LoRA: Low-Rank Adaptation of Large Language Models}},
  journal      = {CoRR},
  volume       = {abs/2106.09685},
  year         = {2021},
  url          = {https://arxiv.org/abs/2106.09685},
  eprinttype    = {arXiv},
  eprint       = {2106.09685},
  timestamp    = {Tue, 29 Jun 2021 16:55:04 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2106-09685.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{lin2023usinglanguagemodelsknowledge,
      title={{Using Language Models For Knowledge Acquisition in Natural Language Reasoning Problems}}, 
      author={Fangzhen Lin and Ziyi Shou and Chengcai Chen},
      year={2023},
      eprint={2304.01771},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2304.01771}, 
}

@misc{hahn2023theoryemergentincontextlearning,
      title={{A Theory of Emergent In-Context Learning as Implicit Structure Induction}}, 
      author={Michael Hahn and Navin Goyal},
      year={2023},
      eprint={2303.07971},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.07971}, 
}

@misc{zhang2024scalingmeetsllmfinetuning,
      title={{When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method}}, 
      author={Biao Zhang and Zhongtao Liu and Colin Cherry and Orhan Firat},
      year={2024},
      eprint={2402.17193},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.17193}, 
}

@misc{shen2024tagllmrepurposinggeneralpurposellms,
      title={{Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains}}, 
      author={Junhong Shen and Neil Tenenholtz and James Brian Hall and David Alvarez-Melis and Nicolo Fusi},
      year={2024},
      eprint={2402.05140},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.05140}, 
}

@article{radford2018improving,
  title={{Improving language understanding by generative pre-training}},
  author={Radford, Alec},
  year={2018}
}

@article{DBLP:journals/corr/abs-2005-14165,
  author       = {Tom B. Brown and
                  Benjamin Mann and
                  Nick Ryder and et. al.},
  title        = {{Language Models are Few-Shot Learners}},
  journal      = {CoRR},
  volume       = {abs/2005.14165},
  year         = {2020},
  url          = {https://arxiv.org/abs/2005.14165},
  eprinttype    = {arXiv},
  eprint       = {2005.14165},
  timestamp    = {Thu, 25 May 2023 10:38:31 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2005-14165.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{antoniou2009web,
  title={Web ontology language: Owl},
  author={Antoniou, Grigoris and Harmelen, Frank van},
  journal={Handbook on ontologies},
  pages={91--110},
  year={2009},
  publisher={Springer}
}

@inproceedings{bader2019semantic,
  title={{The Semantic Asset \\Administration Shell}},
  author={Bader, Sebastian R and Maleshkova, Maria},
  booktitle={Semantic Systems. The Power of AI and Knowledge Graphs: 15th International Conference, SEMANTiCS 2019, Karlsruhe, Germany, September 9--12, 2019, Proceedings 15},
  pages={159--174},
  year={2019},
  organization={Springer}
}

@article{madaan2022text,
  title={{Text and patterns: For effective chain of thought, it takes two to tango}},
  author={Madaan, Aman and Yazdanbakhsh, Amir},
  journal={arXiv preprint arXiv:2209.07686},
  year={2022}
}

@article{li2024chain,
  title={{Chain of thought empowers transformers to solve inherently serial problems}},
  author={Li, Zhiyuan and Liu, Hong and Zhou, Denny and Ma, Tengyu},
  journal={arXiv preprint arXiv:2402.12875},
  year={2024}
}

@article{WUSTENBERG20121,
title = {{Complex problem solving — More than reasoning?}},
journal = {Intelligence},
volume = {40},
number = {1},
pages = {1-14},
year = {2012},
issn = {0160-2896},
doi = {https://doi.org/10.1016/j.intell.2011.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S0160289611001401},
author = {Sascha Wüstenberg and Samuel Greiff and Joachim Funke},
keywords = {Complex problem solving, Intelligence, Dynamic problem solving, MicroDYN, Linear structural equations, Measurement},
abstract = {This study investigates the internal structure and construct validity of Complex Problem Solving (CPS), which is measured by a Multiple-Item-Approach. It is tested, if (a) three facets of CPS – rule identification (adequateness of strategies), rule knowledge (generated knowledge) and rule application (ability to control a system) – can be empirically distinguished, how (b) reasoning is related to these CPS-facets and if (c) CPS shows incremental validity in predicting school grade point average (GPA) beyond reasoning. N=222 university students completed MicroDYN, a computer-based CPS test and Ravens Advanced Progressive Matrices. Analysis including structural equation models showed that a 2-dimensionsal model of CPS including rule knowledge and rule application fitted the data best. Furthermore, reasoning predicted performance in rule application only indirectly through its influence on rule knowledge indicating that learning during system exploration is a prerequisite for controlling a system successfully. Finally, CPS explained variance in GPA even beyond reasoning, showing incremental validity of CPS. Thus, CPS measures important aspects of academic performance not assessed by reasoning and should be considered when predicting real life criteria such as GPA.}
}

@misc{frieder2023mathematicalcapabilitieschatgpt,
      title={{Mathematical Capabilities of ChatGPT}}, 
      author={Simon Frieder and Luca Pinchetti and Alexis Chevalier and Ryan-Rhys Griffiths and Tommaso Salvatori and Thomas Lukasiewicz and Philipp Christian Petersen and Julius Berner},
      year={2023},
      eprint={2301.13867},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2301.13867}, 
}

@misc{kalyanpur2024llmarcenhancingllmsautomated,
      title={{LLM-ARC: Enhancing LLMs with an Automated Reasoning Critic}}, 
      author={Aditya Kalyanpur and Kailash Karthik Saravanakumar and Victor Barres and Jennifer Chu-Carroll and David Melville and David Ferrucci},
      year={2024},
      eprint={2406.17663},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.17663}, 
}

@misc{mahowald2024dissociatinglanguagethoughtlarge,
      title={{Dissociating language and thought in large language models}}, 
      author={Kyle Mahowald and Anna A. Ivanova and Idan A. Blank and Nancy Kanwisher and Joshua B. Tenenbaum and Evelina Fedorenko},
      year={2024},
      eprint={2301.06627},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2301.06627}, 
}

@misc{collins2022structuredflexiblerobustbenchmarking,
      title={{Structured, flexible, and robust: Benchmarking and improving large language models towards more human-like behavior in out-of-distribution reasoning tasks}}, 
      author={Katherine M. Collins and Catherine Wong and Jiahai Feng and Megan Wei and Joshua B. Tenenbaum},
      year={2022},
      eprint={2205.05718},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2205.05718}, 
}

@inproceedings{hardy2023large,
  title={{Large language models meet cognitive science: LLMs as tools, models, and participants}},
  author={Hardy, Mathew and Sucholutsky, Ilia and Thompson, Bill and Griffiths, Tom},
  booktitle={Proceedings of the annual meeting of the cognitive science society},
  volume={45},
  number={45},
  year={2023}
}

@misc{williams2024easyproblemsllmswrong,
      title={{Easy Problems That LLMs Get Wrong}}, 
      author={Sean Williams and James Huckle},
      year={2024},
      eprint={2405.19616},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2405.19616}, 
}

@misc{davoodi2024llmsintelligentthinkersintroducing,
      title={{LLMs Are Not Intelligent Thinkers: Introducing Mathematical Topic Tree Benchmark for Comprehensive Evaluation of LLMs}}, 
      author={Arash Gholami Davoodi and Seyed Pouyan Mousavi Davoudi and Pouya Pezeshkpour},
      year={2024},
      eprint={2406.05194},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.05194}, 
}

@article{chen2023understanding,
  title={{Understanding and Improving In-Context Learning on Vision-language Models}},
  author={Chen, Shuo and Han, Zhen and He, Bailan and Buckley, Mark and Torr, Philip and Tresp, Volker and Gu, Jindong},
  journal={arXiv preprint arXiv:2311.18021},
  volume={1},
  number={2},
  year={2023}
}

@misc{ha2024fusiondomainadaptedvisionlanguage,
      title={{Fusion of Domain-Adapted Vision and Language Models for Medical Visual Question Answering}}, 
      author={Cuong Nhat Ha and Shima Asaadi and Sanjeev Kumar Karn and Oladimeji Farri and Tobias Heimann and Thomas Runkler},
      year={2024},
      eprint={2404.16192},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.16192}, 
}

@inproceedings{NEURIPS2023_271db992,
 author = {Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {11809--11822},
 publisher = {Curran Associates, Inc.},
 title = {{Tree of Thoughts: Deliberate Problem Solving with Large Language Models}},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/271db9922b8d1f4dd7aaef84ed5ac703-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@misc{wang2024retaskrevisitingllmtasks,
      title={{Re-TASK: Revisiting LLM Tasks from Capability, Skill, and Knowledge Perspectives}}, 
      author={Zhihu Wang and Shiwan Zhao and Yu Wang and Heyuan Huang and Jiaxin Shi and Sitao Xie and Zhixing Wang and Yubo Zhang and Hongyan Li and Junchi Yan},
      year={2024},
      eprint={2408.06904},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.06904}, 
}

@misc{jiang2024llmsmathematicalreasoningmistakes,
      title={{LLMs can Find Mathematical Reasoning Mistakes by Pedagogical Chain-of-Thought}}, 
      author={Zhuoxuan Jiang and Haoyuan Peng and Shanshan Feng and Fan Li and Dongsheng Li},
      year={2024},
      eprint={2405.06705},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.06705}, 
}

@article{Besta_2024,
   title={{Graph of Thoughts: Solving Elaborate Problems with Large Language Models}},
   volume={38},
   ISSN={2159-5399},
   url={http://dx.doi.org/10.1609/aaai.v38i16.29720},
   DOI={10.1609/aaai.v38i16.29720},
   number={16},
   journal={Proceedings of the AAAI Conference on Artificial Intelligence},
   publisher={Association for the Advancement of Artificial Intelligence (AAAI)},
   author={Besta, Maciej and Blach, Nils and Kubicek, Ales and Gerstenberger, Robert and Podstawski, Michal and Gianinazzi, Lukas and Gajda, Joanna and Lehmann, Tomasz and Niewiadomski, Hubert and Nyczyk, Piotr and Hoefler, Torsten},
   year={2024},
   month=mar, pages={17682–17690} }

@misc{cheng2024inductivedeductiverethinkingfundamental,
      title={{Inductive or Deductive? Rethinking the Fundamental Reasoning Abilities of LLMs}}, 
      author={Kewei Cheng and Jingfeng Yang and Haoming Jiang and Zhengyang Wang and Binxuan Huang and Ruirui Li and Shiyang Li and Zheng Li and Yifan Gao and Xian Li and Bing Yin and Yizhou Sun},
      year={2024},
      eprint={2408.00114},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2408.00114}, 
}

@misc{bao2024llmschainofthoughtnoncausalreasoners,
      title={{LLMs with Chain-of-Thought Are Non-Causal Reasoners}}, 
      author={Guangsheng Bao and Hongbo Zhang and Linyi Yang and Cunxiang Wang and Yue Zhang},
      year={2024},
      eprint={2402.16048},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.16048}, 
}

@misc{ucedasosa2024reasoningconceptsllmsinconsistencies,
      title={{Reasoning about concepts with LLMs: Inconsistencies abound}}, 
      author={Rosario Uceda-Sosa and Karthikeyan Natesan Ramamurthy and Maria Chang and Moninder Singh},
      year={2024},
      eprint={2405.20163},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.20163}, 
}

@misc{see2019massivelypretrainedlanguagemodels,
      title={{Do Massively Pretrained Language Models Make Better Storytellers?}}, 
      author={Abigail See and Aneesh Pappu and Rohun Saxena and Akhila Yerukola and Christopher D. Manning},
      year={2019},
      eprint={1909.10705},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1909.10705}, 
}

@misc{lewis2021retrievalaugmentedgenerationknowledgeintensivenlp,
      title={{Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks}}, 
      author={Patrick Lewis and Ethan Perez and Aleksandra Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich Küttler and Mike Lewis and Wen-tau Yih and Tim Rocktäschel and Sebastian Riedel and Douwe Kiela},
      year={2021},
      eprint={2005.11401},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.11401}, 
}

@incollection{Floridi2004-FLOLAT,
	author = {Luciano Floridi and J. W. Sanders},
	booktitle = {IEG Research Report},
	editor = {},
	title = {{Levellism and the Method of Abstraction}},
	year = {2004}
}
