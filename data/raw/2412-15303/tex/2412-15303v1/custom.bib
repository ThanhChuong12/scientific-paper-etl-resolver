% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@inproceedings{garcia2023unreasonable,
  title={The unreasonable effectiveness of few-shot learning for machine translation},
  author={Garcia, Xavier and Bansal, Yamini and Cherry, Colin and Foster, George and Krikun, Maxim and Johnson, Melvin and Firat, Orhan},
  booktitle={International Conference on Machine Learning},
  pages={10867--10878},
  year={2023},
  organization={PMLR}
}

@article{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}

@article{wang2023document,
  title={Document-level machine translation with large language models},
  author={Wang, Longyue and Lyu, Chenyang and Ji, Tianbo and Zhang, Zhirui and Yu, Dian and Shi, Shuming and Tu, Zhaopeng},
  journal={arXiv preprint arXiv:2304.02210},
  year={2023}
}

@article{zhu2023multilingual,
  title={Multilingual machine translation with large language models: Empirical results and analysis},
  author={Zhu, Wenhao and Liu, Hongyi and Dong, Qingxiu and Xu, Jingjing and Kong, Lingpeng and Chen, Jiajun and Li, Lei and Huang, Shujian},
  journal={arXiv preprint arXiv:2304.04675},
  year={2023}
}

@inproceedings{jiao2023parrot,
  title={ParroT: Translating during chat using large language models tuned with human translation and feedback},
  author={Jiao, Wenxiang and Huang, Jen-tse and Wang, Wenxuan and He, Zhiwei and Liang, Tian and Wang, Xing and Shi, Shuming and Tu, Zhaopeng},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  year={2023}
}

@article{zeng2023tim,
  title={Tim: Teaching large language models to translate with comparison},
  author={Zeng, Jiali and Meng, Fandong and Yin, Yongjing and Zhou, Jie},
  journal={arXiv preprint arXiv:2307.04408},
  year={2023}
}

@article{gou2021knowledge,
  title={Knowledge distillation: A survey},
  author={Gou, Jianping and Yu, Baosheng and Maybank, Stephen J and Tao, Dacheng},
  journal={International Journal of Computer Vision},
  year={2021}
}

@article{wu2023lamini,
  title={Lamini-lm: A diverse herd of distilled models from large-scale instructions},
  author={Wu, Minghao and Waheed, Abdul and Zhang, Chiyu and Abdul-Mageed, Muhammad and Aji, Alham Fikri},
  journal={arXiv preprint arXiv:2304.14402},
  year={2023}
}

@article{chiang2023vicuna,
  title={Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality},
  author={Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E and others},
  journal={See https://vicuna. lmsys. org (accessed 14 April 2023)},
  year={2023}
}

@article{zhong2023self,
  title={Self-Evolution Learning for Discriminative Language Model Pretraining},
  author={Zhong, Qihuang and Ding, Liang and Liu, Juhua and Du, Bo and Tao, Dacheng},
  journal={arXiv preprint arXiv:2305.15275},
  year={2023}
}

@article{zheng2023self,
  title={Self-Evolution Learning for Mixup: Enhance Data Augmentation on Few-Shot Text Classification Tasks},
  author={Zheng, Haoqi and Zhong, Qihuang and Ding, Liang and Tian, Zhiliang and Niu, Xin and Li, Dongsheng and Tao, Dacheng},
  journal={arXiv preprint arXiv:2305.13547},
  year={2023}
}

@inproceedings{mirzadeh2020improved,
  title={Improved knowledge distillation via teacher assistant},
  author={Mirzadeh, Seyed Iman and Farajtabar, Mehrdad and Li, Ang and Levine, Nir and Matsukawa, Akihiro and Ghasemzadeh, Hassan},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  year={2020}
}

@inproceedings{cho2019efficacy,
  title={On the efficacy of knowledge distillation},
  author={Cho, Jang Hyun and Hariharan, Bharath},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  year={2019}
}

@article{zhang2023lifting,
  title={Lifting the Curse of Capacity Gap in Distilling Language Models},
  author={Zhang, Chen and Yang, Yang and Liu, Jiahao and Wang, Jingang and Xian, Yunsen and Wang, Benyou and Song, Dawei},
  journal={arXiv preprint arXiv:2305.12129},
  year={2023}
}

@article{zhang2023towards,
  title={Towards Understanding and Improving Knowledge Distillation for Neural Machine Translation},
  author={Zhang, Songming and Liang, Yunlong and Wang, Shuaibo and Han, Wenjuan and Liu, Jian and Xu, Jinan and Chen, Yufeng},
  journal={arXiv preprint arXiv:2305.08096},
  year={2023}
}

@article{rao2023parameter,
  title={Parameter-efficient and student-friendly knowledge distillation},
  author={Rao, Jun and Meng, Xv and Ding, Liang and Qi, Shuhan and Liu, Xuebo and Zhang, Min and Tao, Dacheng},
  journal={IEEE Transactions on Multimedia},
  year={2023},
  publisher={IEEE}
}

@article{gu2020token,
  title={Token-level adaptive training for neural machine translation},
  author={Gu, Shuhao and Zhang, Jinchao and Meng, Fandong and Feng, Yang and Xie, Wanying and Zhou, Jie and Yu, Dong},
  journal={arXiv preprint arXiv:2010.04380},
  year={2020}
}

@article{zhang2022conditional,
  title={Conditional bilingual mutual information based adaptive training for neural machine translation},
  author={Zhang, Songming and Liu, Yijin and Meng, Fandong and Chen, Yufeng and Xu, Jinan and Liu, Jian and Zhou, Jie},
  journal={arXiv preprint arXiv:2203.02951},
  year={2022}
}

@article{wang2021selective,
  title={Selective knowledge distillation for neural machine translation},
  author={Wang, Fusheng and Yan, Jianhao and Meng, Fandong and Zhou, Jie},
  journal={arXiv preprint arXiv:2105.12967},
  year={2021}
}

@misc{taori2023stanford,
  title={Stanford alpaca: An instruction-following llama model},
  author={Taori, Rohan and Gulrajani, Ishaan and Zhang, Tianyi and Dubois, Yann and Li, Xuechen and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B},
  year={2023}
}

@article{zhou2023lima,
  title={Lima: Less is more for alignment},
  author={Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srini and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and others},
  journal={arXiv preprint arXiv:2305.11206},
  year={2023}
}

@article{xu2024contrastive,
  title={Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation},
  author={Xu, Haoran and Sharaf, Amr and Chen, Yunmo and Tan, Weiting and Shen, Lingfeng and Van Durme, Benjamin and Murray, Kenton and Kim, Young Jin},
  journal={arXiv preprint arXiv:2401.08417},
  year={2024}
}

@article{qian2020glancing,
  title={Glancing transformer for non-autoregressive neural machine translation},
  author={Qian, Lihua and Zhou, Hao and Bao, Yu and Wang, Mingxuan and Qiu, Lin and Zhang, Weinan and Yu, Yong and Li, Lei},
  journal={arXiv preprint arXiv:2008.07905},
  year={2020}
}

@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

@inproceedings{Peng2023ChatGPT4MT,
  title={Towards Making the Most of ChatGPT for Machine Translation},
  author={Peng, Keqin and Ding, Liang and Zhong, Qihuang and Shen, Li and Liu, Xuebo and Zhang, Min and Ouyang, Yuanxin and Tao, Dacheng},
  booktitle={Findings of EMNLP},
  year={2023}
}

@article{wen2023f,
  title={f-Divergence Minimization for Sequence-Level Knowledge Distillation},
  author={Wen, Yuqiao and Li, Zichao and Du, Wenyu and Mou, Lili},
  journal={arXiv preprint arXiv:2307.15190},
  year={2023}
}

@article{post2018call,
  title={A Call for Clarity in Reporting BLEU Scores},
  author={Post, Matt},
  journal={WMT 2018},
  year={2018}
}

@inproceedings{rei2020comet,
  title={COMET: A Neural Framework for MT Evaluation},
  author={Rei, Ricardo and Stewart, Craig and Farinha, Ana C and Lavie, Alon},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2020}
}


@inproceedings{kocmi2022findings,
  title={Findings of the 2022 conference on machine translation (WMT22)},
  author={Kocmi, Tom and Bawden, Rachel and Bojar, Ond{\v{r}}ej and Dvorkovich, Anton and Federmann, Christian and Fishel, Mark and others},
  booktitle={Proceedings of the Seventh Conference on Machine Translation (WMT)},
  year={2022}
}

@article{hochreiter1997flat,
  title={Flat minima},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  year={1997},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{li2018visualizing,
  title={Visualizing the loss landscape of neural nets},
  author={Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{he2021effectiveness,
  title={On the Effectiveness of Adapter-based Tuning for Pretrained Language Model Adaptation},
  author={He, Ruidan and Liu, Linlin and Ye, Hai and Tan, Qingyu and Ding, Bosheng and Cheng, Liying and Low, Jiawei and Bing, Lidong and Si, Luo},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  year={2021}
}

@article{piantadosi2014zipf,
  title={Zipf’s word frequency law in natural language: A critical review and future directions},
  author={Piantadosi, Steven T},
  journal={Psychonomic bulletin \& review},
  year={2014}
}

@inproceedings{chen2020content,
  title={Content word aware neural machine translation},
  author={Chen, Kehai and Wang, Rui and Utiyama, Masao and Sumita, Eiichiro},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  year={2020}
}

@inproceedings{peng2023token,
  title={Token-level self-evolution training for sequence-to-sequence learning},
  author={Peng, Keqin and Ding, Liang and Zhong, Qihuang and Ouyang, Yuanxin and Rong, Wenge and Xiong, Zhang and Tao, Dacheng},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  year={2023}
}

@article{ko2024distillm,
  title={Distillm: Towards streamlined distillation for large language models},
  author={Ko, Jongwoo and Kim, Sungnyun and Chen, Tianyi and Yun, Se-Young},
  journal={arXiv preprint arXiv:2402.03898},
  year={2024}
}

@article{kim2016sequence,
  title={Sequence-level knowledge distillation},
  author={Kim, Yoon and Rush, Alexander M},
  journal={arXiv preprint arXiv:1606.07947},
  year={2016}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint},
  year={2023}
}
@article{hendy2023good,
  title={How good are gpt models at machine translation? a comprehensive evaluation},
  author={Hendy, Amr and Abdelrehim, Mohamed and Sharaf, Amr and Raunak, Vikas and Gabr, Mohamed and Matsushita, Hitokazu and Kim, Young Jin and Afify, Mohamed and Awadalla, Hany Hassan},
  journal={arXiv preprint arXiv:2302.09210},
  year={2023}
}

@inproceedings{gu2024minillm,
  title={MiniLLM: Knowledge distillation of large language models},
  author={Gu, Yuxian and Dong, Li and Wei, Furu and Huang, Minlie},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@inproceedings{agarwal2024policy,
  title={On-policy distillation of language models: Learning from self-generated mistakes},
  author={Agarwal, Rishabh and Vieillard, Nino and Zhou, Yongchao and Stanczyk, Piotr and Garea, Sabela Ramos and Geist, Matthieu and Bachem, Olivier},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@article{qiu2022better,
  title={Better teacher better student: Dynamic prior knowledge for knowledge distillation},
  author={Qiu, Zengyu and Ma, Xinzhu and Yang, Kunlin and Liu, Chunya and Hou, Jun and Yi, Shuai and Ouyang, Wanli},
  journal={arXiv preprint arXiv:2206.06067},
  year={2022}
}

@article{zhong2023can,
  title={Can chatgpt understand too? a comparative study on chatgpt and fine-tuned bert},
  author={Zhong, Qihuang and Ding, Liang and Liu, Juhua and Du, Bo and Tao, Dacheng},
  journal={arXiv preprint},
  year={2023}
}

@article{kocon2023chatgpt,
  title={ChatGPT: Jack of all trades, master of none},
  author={Koco{\'n}, Jan and Cichecki, Igor and Kaszyca, Oliwier and Kochanek, Mateusz and Szyd{\l}o, Dominika and Baran, Joanna and Bielaniewicz, Julita and Gruza, Marcin and Janz, Arkadiusz and Kanclerz, Kamil and others},
  journal={Information Fusion},
  year={2023}
}

@inproceedings{lu2024error,
    title = "Error Analysis Prompting Enables Human-Like Translation Evaluation in Large Language Models",
    author = "Lu, Qingyu  and
      Qiu, Baopu  and
      Ding, Liang  and
      Zhang, Kanjian  and
      Kocmi, Tom  and
      Tao, Dacheng",
    booktitle = "Findings of the Association for Computational Linguistics ACL 2024",
    year = "2024"
}

@inproceedings{zhong2024revisiting,
    title = "Revisiting Knowledge Distillation for Autoregressive Language Models",
    author = "Zhong, Qihuang  and
      Ding, Liang  and
      Shen, Li  and
      Liu, Juhua  and
      Du, Bo  and
      Tao, Dacheng",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    year = "2024"
}

@article{xu2023paradigm,
  title={A paradigm shift in machine translation: Boosting translation performance of large language models},
  author={Xu, Haoran and Kim, Young Jin and Sharaf, Amr and Awadalla, Hany Hassan},
  journal={arXiv preprint arXiv:2309.11674},
  year={2023}
}

@article{karpinska2023large,
  title={Large language models effectively leverage document-level context for literary translation, but critical errors persist},
  author={Karpinska, Marzena and Iyyer, Mohit},
  journal={arXiv preprint arXiv:2304.03245},
  year={2023}
}

@article{jiao2023chatgpt,
  title={Is ChatGPT a good translator? A preliminary study},
  author={Jiao, Wenxiang and Wang, Wenxuan and Huang, Jen-tse and Wang, Xing and Tu, Zhaopeng},
  journal={arXiv preprint arXiv:2301.08745},
  volume={1},
  number={10},
  year={2023}
}

@article{bawden2023investigating,
  title={Investigating the translation performance of a large multilingual language model: the case of bloom},
  author={Bawden, Rachel and Yvon, Fran{\c{c}}ois},
  journal={arXiv preprint arXiv:2303.01911},
  year={2023}
}

@article{guerreiro2023hallucinations,
  title={Hallucinations in large multilingual translation models},
  author={Guerreiro, Nuno M and Alves, Duarte M and Waldendorf, Jonas and Haddow, Barry and Birch, Alexandra and Colombo, Pierre and Martins, Andr{\'e} FT},
  journal={Transactions of the Association for Computational Linguistics},
  year={2023},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@article{zhu2023extrapolating,
  title={Extrapolating large language models to non-english by aligning languages},
  author={Zhu, Wenhao and Lv, Yunzhe and Dong, Qingxiu and Yuan, Fei and Xu, Jingjing and Huang, Shujian and Kong, Lingpeng and Chen, Jiajun and Li, Lei},
  journal={arXiv preprint arXiv:2308.04948},
  year={2023}
}

@article{wu2024rethinking,
  title={Rethinking Kullback-Leibler Divergence in Knowledge Distillation for Large Language Models},
  author={Wu, Taiqiang and Tao, Chaofan and Wang, Jiahao and Zhao, Zhe and Wong, Ngai},
  journal={arXiv preprint arXiv:2404.02657},
  year={2024}
}

@article{zan2024building,
  title={Building Accurate Translation-Tailored LLMs with Language Aware Instruction Tuning},
  author={Zan, Changtong and Ding, Liang and Shen, Li and Zhen, Yibing and Liu, Weifeng and Tao, Dacheng},
  journal={arXiv preprint arXiv:2403.14399},
  year={2024}
}

@inproceedings{akhbardeh2021findings,
  title={Findings of the 2021 conference on machine translation (WMT21)},
  author={Akhbardeh, Farhad and Arkhangorodsky, Arkady and Biesialska, Magdalena and Bojar, Ond{\v{r}}ej and Chatterjee, Rajen and others},
  booktitle={Proceedings of the sixth conference on machine translation},
  year={2021}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

@article{freitag2017beam,
  title={Beam search strategies for neural machine translation},
  author={Freitag, Markus and Al-Onaizan, Yaser},
  journal={arXiv preprint arXiv:1702.01806},
  year={2017}
}

@article{kim2024promptkd,
  title={PromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt Tuning},
  author={Kim, Gyeongman and Jang, Doohyuk and Yang, Eunho},
  journal={arXiv preprint arXiv:2402.12842},
  year={2024}
}

@article{chen2024knowledge,
  title={Knowledge Distillation for Closed-Source Language Models},
  author={Chen, Hongzhan and Quan, Xiaojun and Chen, Hehong and Yan, Ming and Zhang, Ji},
  journal={arXiv preprint arXiv:2401.07013},
  year={2024}
}

@article{jiang2023lion,
  title={Lion: Adversarial distillation of proprietary large language models},
  author={Jiang, Yuxin and Chan, Chunkit and Chen, Mingyang and Wang, Wei},
  journal={arXiv preprint arXiv:2305.12870},
  year={2023}
}

@article{hsieh2023distilling,
  title={Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes},
  author={Hsieh, Cheng-Yu and Li, Chun-Liang and Yeh, Chih-Kuan and Nakhost, Hootan and Fujii, Yasuhisa and Ratner, Alexander and Krishna, Ranjay and Lee, Chen-Yu and Pfister, Tomas},
  journal={arXiv preprint arXiv:2305.02301},
  year={2023}
}

@article{peng2023instruction,
  title={Instruction tuning with gpt-4},
  author={Peng, Baolin and Li, Chunyuan and He, Pengcheng and Galley, Michel and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2304.03277},
  year={2023}
}

@article{zheng2024llamafactory,
  title={Llamafactory: Unified efficient fine-tuning of 100+ language models},
  author={Zheng, Yaowei and Zhang, Richong and Zhang, Junhao and Ye, Yanhan and Luo, Zheyan and Feng, Zhangchi and Ma, Yongqiang},
  journal={arXiv preprint arXiv:2403.13372},
  year={2024}
}

@article{rao2024exploring,
  title={Exploring and Enhancing the Transfer of Distribution in Knowledge Distillation for Autoregressive Language Models},
  author={Rao, Jun and Liu, Xuebo and Lin, Zepeng and Ding, Liang and Li, Jing and Tao, Dacheng and Zhang, Min},
  journal={arXiv preprint arXiv:2409.12512},
  year={2024}
}

@article{zhong2022toward,
  title={Toward efficient language model pretraining and downstream adaptation via self-evolution: A case study on superglue},
  author={Zhong, Qihuang and Ding, Liang and Zhan, Yibing and Qiao, Yu and Wen, Yonggang and Shen, Li and Liu, Juhua and Yu, Baosheng and Du, Bo and Chen, Yixin and others},
  journal={arXiv preprint arXiv:2212.01853},
  year={2022}
}

@inproceedings{kenton2019bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Kenton, Jacob Devlin Ming-Wei Chang and Toutanova, Lee Kristina},
  booktitle={Proceedings of naacL-HLT},
  year={2019},
  organization={Minneapolis, Minnesota}
}

@inproceedings{zhuincorporating,
  title={Incorporating BERT into Neural Machine Translation},
  author={Zhu, Jinhua and Xia, Yingce and Wu, Lijun and He, Di and Qin, Tao and Zhou, Wengang and Li, Houqiang and Liu, Tieyan},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{guo2021adaptive,
  title={Adaptive adapters: An efficient way to incorporate BERT into neural machine translation},
  author={Guo, Junliang and Zhang, Zhirui and Xu, Linli and Chen, Boxing and Chen, Enhong},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  year={2021}
}

@inproceedings{lewis-etal-2020-bart,
    title = "{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
    author = "Lewis, Mike  and
      Liu, Yinhan  and
      Goyal, Naman  and
      others",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    year = "2020"
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  year={2020}
}

@article{liu2020multilingual,
  title={Multilingual Denoising Pre-training for Neural Machine Translation},
  author={Liu, Yinhan and Gu, Jiatao and Goyal, Naman and Li, Xian and Edunov, Sergey and Ghazvininejad, Marjan and Lewis, Mike and Zettlemoyer, Luke},
  journal={Transactions of the Association for Computational Linguistics},
  year={2020}
}

@inproceedings{liu2021complementarity,
  title={On the Complementarity between Pre-Training and Back-Translation for Neural Machine Translation},
  author={Liu, Xuebo and Wang, Longyue and Wong, Derek F and Ding, Liang and Chao, Lidia S and Shi, Shuming and Tu, Zhaopeng},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2021},
  year={2021}
}

@inproceedings{zan2022complementarity,
  title={On the Complementarity between Pre-Training and Random-Initialization for Resource-Rich Machine Translation},
  author={Zan, Changtong and Ding, Liang and Shen, Li and Cao, Yu and Liu, Weifeng and Tao, Dacheng},
  booktitle={Proceedings of the 29th International Conference on Computational Linguistics},
  year={2022}
}

@article{zan2022bridging,
  title={Bridging cross-lingual gaps during leveraging the multilingual sequence-to-sequence pretraining for text generation and understanding},
  author={Zan, Changtong and Ding, Liang and Shen, Li and Cao, Yu and Liu, Weifeng and Tao, Dacheng},
  journal={arXiv preprint arXiv:2204.07834},
  year={2022}
}

@inproceedings{pan-etal-2024-pomp,
    title = "{POMP}: Probability-driven Meta-graph Prompter for {LLM}s in Low-resource Unsupervised Neural Machine Translation",
    author = "Pan, Shilong  and
      Tian, Zhiliang  and
      Ding, Liang  and
      Zheng, Haoqi  and
      Huang, Zhen  and
      Wen, Zhihua  and
      Li, Dongsheng",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    year = "2024"
}
