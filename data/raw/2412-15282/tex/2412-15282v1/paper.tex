\documentclass[]{fairmeta}
% Option "twocolumn" available, but please prioritize single-column
\usepackage{graphicx}
\usepackage{tabularx}

\usepackage{colortbl}
\usepackage{tcolorbox}
\usepackage{inconsolata}
\usepackage{amsmath}
\usepackage{arydshln}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{fancyvrb}
\usepackage{framed}
\usepackage{float}

\definecolor{verylightgray}{rgb}{0.9,0.9,0.9}

\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\DeclareMathOperator*{\argmax}{\arg\!\max}

\title{A Systematic Examination of Preference Learning through the Lens of Instruction-Following}

\author[1,2]{Joongwon Kim}
\author[1]{Anirudh Goyal}
\author[1]{Aston Zhang}
\author[1]{Bo Xiong}
\author[1]{Rui Hou}
\author[1]{Melanie Kambadur}
\author[1]{Dhruv Mahajan}
\author[2]{Hannaneh Hajishirzi}
\author[1]{Liang Tan}

\affiliation[1]{Llama Team, AI @ Meta}
\affiliation[2]{University of Washington}

% \contribution[*]{Work done during internship at Meta}
% \contribution[\dagger]{Joint last author}

\abstract{
Preference learning is a widely adopted post-training technique that aligns large language models (LLMs) to human preferences and improves specific downstream task capabilities.
In this work we systematically investigate how specific attributes of preference datasets affect the alignment and downstream performance of LLMs in instruction-following tasks.
We use a novel synthetic data generation pipeline to generate 48,000 unique instruction-following prompts with combinations of 23 verifiable constraints that enable fine-grained and automated quality assessments of model responses.
With our synthetic prompts, we use two preference dataset curation methods -- rejection sampling (RS) and Monte Carlo Tree Search (MCTS) -- to obtain pairs of (chosen, rejected) responses.
Then, we perform experiments investigating the effects of (1) the presence of shared prefixes between the chosen and rejected responses, (2) the contrast and quality of the chosen, rejected responses and (3) the complexity of the training prompts.
Our experiments reveal that shared prefixes in preference pairs, as generated by MCTS, provide marginal but consistent improvements and greater stability across challenging training configurations.
High-contrast preference pairs generally outperform low-contrast pairs; however, combining both often yields the best performance by balancing diversity and learning efficiency.
Additionally, training on prompts of moderate difficulty leads to better generalization across tasks, even for more complex evaluation scenarios, compared to overly challenging prompts.
Our findings provide actionable insights into optimizing preference data curation for instruction-following tasks, offering a scalable and effective framework for enhancing LLM training and alignment.
}

\date{\today}
\correspondence{Joongwon Kim at \email{jwonkim@meta.com}}

% You can add additional metadata fields as follows 
% \metadata[Code]{TBD}
% \metadata[Blogpost]{\url{https://ai.meta.com/blog/?page=1}}

\begin{document}

\maketitle

\section{Introduction}
\label{section:intro}
\input{sections/intro}
\begin{figure}
    \centering
    \includegraphics[scale=0.71, clip, trim=0.6cm 6.2cm 1.5cm 1.0cm]{img/rs_mcts.pdf}
    \caption{
    Automatically curating preference pairs via rejection sampling (RS, left) and Monte Carlo Tree Search (MCTS, right).
    \textbf{RS}: We independently sample $N$ different outputs from the policy, score each output with a verifier and take (high, low) scoring responses as the (chosen, rejected) pairs.
    \textbf{MCTS}: We perform tree search with the policy while generating multiple actions per each search iteration.
    Then, we use the rollouts from sibling nodes with (high, low) reward scores as the (chosen, rejected) pairs to obtain preference pairs with common prefixes up to the parent nodes.
    }
    \label{fig:rs-mcts}
\end{figure}

\section{Background}
\label{section:background}
\input{sections/background}

\section{Prompt Synthesis}
\label{section:prompt_synthesis}
\input{sections/prompt_synthesis}

\section{Preference Data Curation}
\label{section:preference_curation}
\input{sections/preference_curation}

\section{Experiments and Results}
\label{section:experiments}
\input{sections/experiments}

\section{Conclusion}
\label{section:conclusion}
\input{sections/conclusion}

% \section{References and citations}

% Take a look at~\cref{table:demo}, appearing on~\cref{section:intro}.
% %
% Some citation of previous work~\citep{goodman}.

\clearpage
\newpage
\bibliographystyle{assets/plainnat}
\bibliography{paper}

\clearpage
\newpage
\beginappendix
\label{section:appendix}
\input{sections/appendix}

\end{document}