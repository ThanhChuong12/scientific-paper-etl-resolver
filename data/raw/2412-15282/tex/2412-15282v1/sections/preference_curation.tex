Using the prompts obtained in Section~\ref{section:prompt_synthesis}, we generate responses and extract preference data using the aggregated correctness scores of the responses.
Here, we assign high-scoring responses as the chosen responses and low-scoring responses as the rejected responses.
We employ two methods for preference data curation: rejection sampling (RS) and Monte Carlo Tree Search (MCTS).
Rejection sampling presents a straightforward and efficient way to obtain preference data but the (chosen, rejected) responses do not share any relationship.
On the other hand, MCTS is more expensive and slower to run, but returns (chosen, rejected) responses that share a prefix with more nuanced contrast.
We use two contrasting approaches to generate diverse types of preference data and additionally investigate the effect of having common prefixes in preference pairs.

\textbf{Rejection Sampling.}
Refer to Figure~\ref{fig:rs-mcts} (left) for a visual overview. 
We first set a filtering criteria, where the chosen response must achieve a score of $\mathbf{c}$ according to our verifier and the rejected response must achieve a score of $\mathbf{r}$.
Then, we generate $N$ different responses independently with the policy model and score each response with our verifier.
Given a prompt $x$, its associated set of verifiable constraints $\mathcal{C}$, the response $r$ and our verifier $\mathcal{V}$ which verifies whether the response satisfies any given constraint $c$, we compute the score as
$$R(r|x, \mathcal{C}) = \frac{1}{\|\mathcal{C}\|}\sum_{c\in\mathcal{C}}\mathcal{V}(r|x, c)$$
We extract all preference pairs such that 1) the chosen score equals $\mathbf{c}$, 2) the rejected score equals $\mathbf{r}$, and 3) there are no overlapping responses between the extracted preference pairs.
Using rejection sampling offers a simple method for automatically curating preference pairs, but it returns pairs that are independently sampled with no common structures between the chosen and rejected responses.

\textbf{Monte Carlo Tree Search.}
Refer to Figure~\ref{fig:rs-mcts} (right) for a visual overview.
We conduct Monte Carlo Tree Search (MCTS) with a granularity level of token sequences -- for a given prompt $x$ and the MCTS tree $T$, each node $s_i$ in $T$ represents a partial response generated for $x$, and each edge $(s_i, s_j)$ in $T$, also known as an \textit{action}, represents a sequence of tokens generated from $s_i \rightarrow s_j$.
In this setup, we use an LLM $\Pi$ as the policy and our verifier as the outcome reward model via three major steps: selection, expansion and backpropagation.
% Figure~\ref{} provides a visualization of each of the four steps.

\paragraph{Selection.}
Given the current node $s_t$ and $K$ different actions $(a_1, ..., a_K)$ generated by the policy from $s_t$, we balance exploitation and exploration to select the next node for tree search.
Each action is a sequence of tokens under a pre-specified maximum length.

Our selection depends on $Q(s_t, a)$ and $N(s_t, a)$, the Q-value and visit count of each subsequent node reached by taking action $a$ from $s_t$, respectively.
We use the Predictor+Upper Confidence bounds applied to Trees (PUCT) and select the next node $s_{t+1}$ according to the following formula:
$$s_{t+1}^* = \argmax_{(s_{t+1} = s_t\rightarrow a_i)}\left[Q(s_t,a_i)+c_{\text{puct}}\cdot \Pi(a_i|s_t)\frac{\sqrt{N(s_t)}}{1 + N(s_t, a_i)}\right]$$
Refer to Appendix~\ref{sec:mcts_details} for more information on how to compute the policy score $\Pi(a_i|s_t)$.
Using PUCT, we prioritize exploration during the initial stages of tree building when the visit counts have low values, and prioritize exploitation during the later stages of tree building as the visit counts increase and effectively weigh the Q-values more for scoring.
This results in a balanced trade-off during our tree search.

\paragraph{Expansion.}
We perform expansion from the current node $s_t$ and generate $K$ new actions with the policy $\Pi$.
For each new action, we perform $M$ separate rollouts and score each rollout using a linear combination of the score $\mathcal{V}$ assigns and the self-evaluation score assigned by $\Pi$.
We use self-evaluation, denoted as $\Pi_{\text{self-eval}}(s_t)$, in addition to the verifier scores, to provide step-level feedback during the tree search. Again, refer to Appendix~\ref{sec:mcts_details} for more information on how to compute $\Pi_{\text{self-eval}}(s_t)$.
$$R(s_t) = (1 - \lambda)\cdot \left[\frac{1}{M\|\mathcal{C}\|}\sum_{i\in [1...M]}\sum_{c\in\mathcal{C}}\mathcal{V}(\Pi_{\text{rollout}}(s_t)|x, c)\right] + \lambda \cdot \Pi_{\text{self-eval}}(s_t)$$
After assigning the reward scores to each rollout, we average the scores across the rollouts for each new action $a_i$ for $i \in [1,\ldots,K]$ and add the node $s_{t+1} = (s_t\rightarrow a_i)$, with the averaged reward score, to the tree.

\paragraph{Backpropagation.}
After computing the reward scores for the rollouts and adding new nodes, we backpropagate the reward scores through the parent nodes.
We increment the visit counts and directly update the Q-values of a given (state, action) pair based on the Q-values and the visit counts of the children nodes of $s_{t+1} = (s_t, a)$, as the following:
$$N(s_t) = N(s_t) + 1$$
$$Q(s_{t}, a) = \frac{\sum_{i=1}^KQ(s_{t+1}, a_i)\cdot N(s_{t+1}, a_i) + R(s_{t+1})}{\sum_{i=1}^KN(s_{t+1}, a_i) + 1}$$

We repeat the threefold process over multiple iterations for each root node, and we traverse down the tree while switching the root node until we reach a terminal node.

Once the trees are constructed, we curate preference data from pairs of sibling nodes, i.e. nodes that share the same parent node.
For pairs of non-leaf sibling nodes that satisfy the correctness criteria in the tree, we sample their rollouts to obtain complete responses.
We use the same criteria for our data curation as that of rejection sampling -- we set a filtering criteria in which the chosen response must score $\mathbf{c}$ and the rejected response must score $\mathbf{r}$, and sample all pairs with no overlapping responses.
Note that we use the scores assigned by the verifier $\mathcal{V}$ only, and not the policy's self-evaluation, in order to ensure that the correctness of the (chosen, rejected) responses stay consistent across the RS- and MCTS-based curation methods.
% As the pairs share the same parent node, the (chosen, rejected) responses share a common prefix leading up to the parent node and the difference occurs in the suffixes that follow.

\begin{figure}
    \centering
    \includegraphics[scale=0.72, clip, trim=0.1cm 8.2cm 4.5cm 0.1cm]{img/preference_pair_stats.pdf}
    \caption{
    Number of preference pairs for different correctness filtering criteria at $k=5$.
    The light blue color indicates preference pairs obtained via rejection sampling (RS), and the dark blue color indicates preference pairs obtained via Monte Carlo Tree Search (MCTS).
    The left subfigure shows the number of unique prompts with (chosen, rejected) responses associated with each filtering criteria, and the right subfigure shows the total number of preference pairs with (chosen, rejected) responses associated with each filtering criteria.
    }
    \label{fig:preference-pair-stats}
\end{figure}

\textbf{Preference Pair Statistics.}
We curate preference pairs using both RS- and MCTS-based methods for our synthetic prompts with $k \in \{4,5,6\}$.
Figure~\ref{fig:preference-pair-stats} shows the number of preference pairs obtained via both methods for $k=5$ when the correctness criteria for (chosen, rejected) pairs is set as (5 correct, 0 correct), (5 correct, 1 correct), (5 correct, 2 correct), (5 correct, 3 correct) and (5 correct, 4 correct) pairs.
The left subfigure depicts the number of unique prompts corresponding to the preference pairs extracted for each filtering criteria.
Using rejection sampling (RS) returns a higher yield of preference pairs with high contrast between the (chosen, rejected) responses, while using Monte Carlo Tree Search (MCTS) returns a higher yield of preference pairs with low contrast between the responses.
The right subfigure depicts the total number of preference pairs extracted for each criteria -- the same observation can be made about the relative yield, with MCTS yielding a large number of preference pairs due to its tree structure.
We use preference pairs collected for $k \in \{4,5,6\}$ using both methods over different filtering criteria to perform our experiments.