\section{Conclusion}
\system\space enables the pre-training and fine-tuning small language models on custom user data. The framework supports deploying models at the edge, ranging between 30M and 124M parameters. Our results show that these smaller models can match or even surpass the performance of much larger models across various applications. Incorporating domain-specific pre-training data further enhances their effectiveness. This framework takes a step towards deploying smaller, domain-adapted language models optimized for edge computing to support embedded sensing applications.
