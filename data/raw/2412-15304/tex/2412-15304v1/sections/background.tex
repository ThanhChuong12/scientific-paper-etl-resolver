\section{Background}
We provide the necessary background relevant to the design of \system. We also discuss and place related systems and developments related to the proposed system, \system.

\fakepar{Conditional probability view of models} Language modeling is framed as an unsupervised distribution estimation problem. Given a sequence of tokens $\mathbf{x} = [x_1, x_2, \ldots, x_n]$, the language model places a probability distribution $p(\mathbf{x})$ over the output token sequence. This probability can be decomposed into a product of conditional probabilities where each token depends on all the previous tokens:

\begin{equation}
    p(\mathbf{x}) = \prod_{i=1}^{n} p(x_i \mid x_1, x_2, \ldots, x_{i-1})
\end{equation}

This formulation allows the model to generate text by sampling from the distribution $p(\mathbf{x})$ and provides a basis for tractable estimation and sampling. The approach has been significantly improved by introducing models capable of computing these conditional probabilities effectively, such as the Transformer architecture \cite{vaswani2023attention}.

Causal language modeling proves effective for analyzing user prompts and generating text. Auto-regressive decoder models, such as GPT-2, are well-suited. These models, which \system\space trains for deployment on edge devices, excel in handling sequential data by predicting the next token based on previous ones, making them ideal for generating coherent text in response to user prompts.

\fakepar{Characterization of models} LLMs can be characterized along two primary axes: computational and memory requirements and performance. The parameter count—the number of weights a model contains—directly influences its computational and memory demands. Models can vary widely in size, ranging from hundreds of millions to billions of parameters. As parameter counts increase, so do the memory and processing resources required. Commercial vendors now offer models with parameter sizes reaching hundreds of billions, hosted on GPU-based clusters and accessible via web chat interfaces or API calls. Examples include ChatGPT~\cite{llm_ChatGPT}, Claude~\cite{llm_claude}, Gemini~\cite{llm_gemini}, and LLama~\cite{llm_llama2,llm_llama3}.

Performance-wise, two critical factors define LLMs: accuracy and response time. LLMs sometimes produce irrelevant or factually incorrect responses, referred to as hallucinations~\cite{hallucination}. Minimizing hallucination rates is a key goal, as higher rates negatively affect usability. Larger, cloud-based models tend to exhibit lower hallucination rates, with ongoing improvements targeting further reductions~\cite{hallucination_less}. Response time, measured by the token generation rate, refers to how many words or tokens the model generates per unit of time. A higher token rate translates to faster responses to user prompts.

\fakepar{Larger models are unsuitable for \system\space} Highly capable LLMs with tens to hundreds of billions of parameters present challenges, making them unsuitable for \system\space. \emph{First}, these models require expensive and complex infrastructure for inference, typically involving powerful computers with GPUs or custom ASICs. \emph{Second}, due to their high resource demands, they are hosted by third-party providers and accessed via API calls, leading to increased operational costs for end-users. \emph{Third}, sensor data, often containing private information, must be shared with these providers, posing significant privacy risks. \emph{Fourth}, fine-tuning these models is an expensive process requiring highly capable GPU-based machines. \emph{Lastly}, many of these models function as “black boxes,” raising concerns about using private data for training and fine-tuning without transparency.


\fakepar{Smaller models can run locally on edge computers} Smaller language models~\cite{llm_phi2,llm_phi3,llm_llama2,llm_llama3,llm_mistral,llm_gemma} trade parameter size for reduced computational and memory requirements, typically ranging from hundreds of millions to a few billion parameters—much smaller than their larger counterparts. This reduction leads to smaller model weights; for example, Microsoft Phi 2, with 2.7 billion parameters, has weights around 5.5 GB. A system running an LLM requires RAM at least as large as the model weight since the entire model must load into memory for inference. As a result, smaller models in half-precision (FP16) can run on edge-class commodity computers with 8–32 GB of RAM, such as Raspberry Pi~\cite{raspberrypi}, Lattepanda Sigma~\cite{lattepanda}, and Intel N100~\cite{intel_n100}.

There has been recent interest in smaller models, which inspire the design of \system.  Ma et al. \cite{1bitllm} introduces variants of LLM with ternary weights {-1, 0, 1}, effectively using just 1.58 bits per parameter instead of the usual 16-bit (FP16) or 32-bit floating-point (FP32), reducing the computational and memory bandwidths significantly while performing comparably with full precision transformer. Ruoss et al. \cite{chess_llm} shows a development of a 270M parameter LLM based on the transformer that achieves grandmaster-level chess rating, reaching the performance of best chess engines, showing that smaller and specialized models with careful training can perform at par or better than rule-based ML systems.  Srinivas et al. \cite{knowledge-boosting} introduces knowledge boosting, where a smaller model, usually deployed for real-time inferences on wearables, obtains delayed hints from a relatively larger model, often deployed on smartphones. This is shown in tasks involving speech separation and enhancement. There have also been works that have explored applying smaller models to mobile devices. Yuan et al. \cite{mobile-models} introduced the concept of a "mobile foundation model," which functions like firmware and can serve a wide range of  tasks on smartphones. This  model would be managed by the mobile OS and hardware and exposed as a system service to applications. 

\system\space allows for training at least an order of magnitude smaller language models consisting of only tens of millions of parameters. This enables faster inference even on constrained embedded platforms. From a training perspective, the end-user may easily train their custom models even with the modest computational resources. It also facilitates the deployment of models on a wide range of simpler embedded platforms with limited memory and RAM, including  SBCs, opening up the possibility of various embedded applications. 




\fakepar{Challenges with remotely accessing larger models} To bring the capabilities of language models to edge devices, larger models can be accessed remotely. However, this approach introduces several challenges, particularly for embedded sensing applications, which are the focus of this work. \emph{First}, maintaining persistent network connectivity is often unfeasible, as many embedded sensing applications involve mobile devices, resulting in intermittent and unpredictable connections~\cite{persistent_network}. \emph{Second}, variable network latency and server constraints can lead to unpredictable response times, affecting the quality of service for time-sensitive tasks. \emph{Third}, remote model access incurs costs, with providers charging based on usage~\cite{openai_pricing}. \emph{Fourth}, embedded sensing applications frequently collect sensitive user data, raising privacy and security concerns when sharing information with third parties~\cite{inversion_attack}. \emph{Lastly}, while SoTA models offer powerful general-purpose capabilities, they can be excessive for specific embedded sensing tasks, which often do not require the full range of these models’ capabilities.

\fakepar{Our choice}  \system\space focuses on utilizing smaller models running locally on edge devices, driven by several key considerations. \emph{Firstly}, it enables executing the model closer to end devices, minimizing network dependencies and latency. Many end devices are situated in remote or hard-to-reach locations, where maintaining persistent network connectivity for the gateway device is challenging. \emph{Secondly}, many sensor applications involve collecting sensitive environmental data, and transmitting this data to third-party providers raises privacy concerns. Additionally, concerns persist regarding the use of such data to train LLMs. \emph{Thirdly}, local processing lowers operational costs, as third-party providers typically charge per-token usage fees. \emph{Finally}, as demonstrated in this work, smaller, task-specific models running locally can outperform generalized cloud-based models for specific applications, making them preferable over invoking more powerful but less specialized remote models.


 