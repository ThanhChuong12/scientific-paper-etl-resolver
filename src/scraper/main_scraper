import os
import re
import time
import json
import shutil
import gzip
import tarfile
import csv
import threading
import psutil
import requests
import tempfile
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from bs4 import BeautifulSoup

# ==============================================================================
# CONFIGURATION
# ==============================================================================
STUDENT_ID = "23456789" 
START_MONTH = "2310"
START_ID = 12345
END_ID = 12347

BASE_DIR = Path(STUDENT_ID)
MAX_WORKERS = 2     
BATCH_SIZE = 5      
S2_DELAY = 3.0      

ARXIV_EPRINT_HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
}
SEMANTIC_SCHOLAR_HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36"
}
IMAGE_EXTS = {'.png', '.jpg', '.jpeg', '.pdf', '.eps', '.tif', '.tiff', '.gif', '.bmp'}

# ==============================================================================
# UTILS & LOGGING
# ==============================================================================
def log(msg: str):
    print(f"[{datetime.now().strftime('%H:%M:%S')}] {msg}")

def arxiv_id_to_folder(id_str: str) -> str:
    return id_str.replace('.', '-')

def get_total_size(path_root: Path) -> int:
    total = 0
    try:
        if not path_root.exists(): return 0
        for p in path_root.rglob('*'):
            if p.is_file(): total += p.stat().st_size
    except Exception: pass
    return total

def track_memory_usage(stop_event, mem_stats):
    process = psutil.Process(os.getpid())
    samples = []
    while not stop_event.is_set():
        try:
            rss = process.memory_info().rss / (1024 * 1024)  # MB
            samples.append(rss)
        except: pass
        time.sleep(0.1)
    if samples:
        mem_stats["max"] = max(samples)
        mem_stats["avg"] = sum(samples) / len(samples)

# ==============================================================================
# HTTP CLIENT & RATE LIMITING
# ==============================================================================
last_request_time = 0
request_lock = Lock()

session = requests.Session()
retry_strategy = Retry(
    total=3, backoff_factor=0.5, status_forcelist=[429, 500, 502, 503, 504],
)
adapter = HTTPAdapter(max_retries=retry_strategy, pool_connections=10, pool_maxsize=10)
session.mount("http://", adapter)
session.mount("https://", adapter)

def enforce_rate_limit():
    global last_request_time
    with request_lock:
        current_time = time.time()
        elapsed = current_time - last_request_time
        wait_time = S2_DELAY - elapsed
        if wait_time > 0:
            time.sleep(wait_time)
        last_request_time = time.time()

# ==============================================================================
# CORE LOGIC: VERSION DISCOVERY & METADATA
# ==============================================================================
def discover_versions_via_abs(arxiv_base_id: str, timeout: int = 20) -> List[str]:
    url = f"https://arxiv.org/abs/{arxiv_base_id}"
    try:
        enforce_rate_limit()
        r = session.get(url, headers=ARXIV_EPRINT_HEADERS, timeout=timeout)
        if r.status_code != 200: return ["v1"]
        
        match = re.search(r"Submission history(.*?)</div>", r.text, re.S | re.I)
        history_block = match.group(1) if match else r.text
        versions = re.findall(r"\[v(\d+)\]", history_block)
        
        if not versions: return ["v1"]
        return [f"v{v}" for v in sorted(map(int, versions))]
    except Exception as e:
        log(f"Error discovering versions: {e}. Defaulting to v1.")
        return ["v1"]

def get_metadata(arxiv_base_id: str):
    url = f"https://arxiv.org/abs/{arxiv_base_id}"
    try:
        enforce_rate_limit()
        r = session.get(url, headers=ARXIV_EPRINT_HEADERS, timeout=20)
        if r.status_code != 200: return {"paper_title": "", "authors": [], "submission_date": "", "revised_dates": []}

        soup = BeautifulSoup(r.text, 'html.parser')
        
        title_tag = soup.find('h1', class_='title')
        title = title_tag.get_text(strip=True).replace('Title:', '').strip() if title_tag else ""
        
        authors_tag = soup.find('div', class_='authors')
        authors = [a.text.strip() for a in authors_tag.find_all('a')] if authors_tag else []

        revised_dates = []
        hist_div = soup.find('div', class_='submission-history')
        submission_block = hist_div.get_text("\n", strip=True) if hist_div else ""
        
        if submission_block:
            pattern = re.compile(r'\[v(\d+)\]\s+([A-Za-z]{3},\s+\d{1,2}\s+[A-Za-z]{3}\s+\d{4})')
            for vnum, date_str in pattern.findall(submission_block):
                try:
                    dt = datetime.strptime(date_str.strip(), "%a, %d %b %Y")
                    revised_dates.append(dt.strftime("%Y-%m-%d"))
                except: pass
        
        return {
            "paper_title": title, "authors": authors,
            "submission_date": revised_dates[0] if revised_dates else "",
            "revised_dates": sorted(list(set(revised_dates))),
            "publication_venue": soup.find('span', class_='primary-subject').get_text(strip=True) if soup.find('span', class_='primary-subject') else ""
        }
    except Exception as e:
        log(f"Error parsing metadata: {e}")
        return {}

