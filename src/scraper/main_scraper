import os
import re
import time
import json
import shutil
import gzip
import tarfile
import csv
import threading
import psutil
import requests
import tempfile
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from bs4 import BeautifulSoup
from utils import log, arxiv_id_to_folder, format_yymm_id, get_total_size, track_memory_usage
from http_client import session, enforce_rate_limit

# ==============================================================================
# CONFIGURATION
# ==============================================================================
STUDENT_ID = "23456789" 
START_MONTH = "2310"
START_ID = 12345
END_ID = 12347

BASE_DIR = Path(STUDENT_ID)
MAX_WORKERS = 2     
BATCH_SIZE = 5      
S2_DELAY = 3.0      

ARXIV_EPRINT_HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
}
SEMANTIC_SCHOLAR_HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36"
}
IMAGE_EXTS = {'.png', '.jpg', '.jpeg', '.pdf', '.eps', '.tif', '.tiff', '.gif', '.bmp'}

# ==============================================================================
# CORE LOGIC: VERSION DISCOVERY & METADATA
# ==============================================================================
def get_metadata(arxiv_base_id: str):
    url = f"https://arxiv.org/abs/{arxiv_base_id}"
    try:
        enforce_rate_limit()
        r = session.get(url, headers=ARXIV_EPRINT_HEADERS, timeout=20)
        if r.status_code != 200: return {"paper_title": "", "authors": [], "submission_date": "", "revised_dates": []}

        soup = BeautifulSoup(r.text, 'html.parser')
        
        title_tag = soup.find('h1', class_='title')
        title = title_tag.get_text(strip=True).replace('Title:', '').strip() if title_tag else ""
        
        authors_tag = soup.find('div', class_='authors')
        authors = [a.text.strip() for a in authors_tag.find_all('a')] if authors_tag else []

        revised_dates = []
        hist_div = soup.find('div', class_='submission-history')
        submission_block = hist_div.get_text("\n", strip=True) if hist_div else ""
        
        if submission_block:
            pattern = re.compile(r'\[v(\d+)\]\s+([A-Za-z]{3},\s+\d{1,2}\s+[A-Za-z]{3}\s+\d{4})')
            for vnum, date_str in pattern.findall(submission_block):
                try:
                    dt = datetime.strptime(date_str.strip(), "%a, %d %b %Y")
                    revised_dates.append(dt.strftime("%Y-%m-%d"))
                except: pass
        
        return {
            "paper_title": title, "authors": authors,
            "submission_date": revised_dates[0] if revised_dates else "",
            "revised_dates": sorted(list(set(revised_dates))),
            "publication_venue": soup.find('span', class_='primary-subject').get_text(strip=True) if soup.find('span', class_='primary-subject') else ""
        }
    except Exception as e:
        log(f"Error parsing metadata: {e}")
        return {}

# ==============================================================================
# FILE DETECTORS
# ==============================================================================
def is_binary_file(content: bytes) -> bool:
    if len(content) < 4: return False
    # Check magic numbers for PDF, JPG, GZIP, ZIP
    if any(content.startswith(sig) for sig in [b"\x25\x50\x44\x46", b"\xFF\xD8\xFF", b"\x1F\x8B\x08", b"\x50\x4B\x03\x04"]):
        return True
    # Heuristic: check ratio of printable characters
    return (sum(1 for b in content if 32 <= b <= 126 or b in [9, 10, 13]) / len(content)) < 0.7

def is_latex_file(file_path: Path) -> bool:
    try:
        if not file_path.exists() or file_path.stat().st_size == 0: return False
        with open(file_path, 'rb') as f: start = f.read(4096)
        if is_binary_file(start): return False
        text = start.decode('utf-8', errors='ignore')
        return "\\documentclass" in text or "\\begin{document}" in text or "\\section{" in text
    except: return False


# ==============================================================================
# FILE PROCESSORS
# ==============================================================================
def extract_recursive(archive_path: Path, extract_dir: Path, depth: int = 0) -> bool:
    if depth > 3: return False
    try:
        extract_dir.mkdir(parents=True, exist_ok=True)
        # Try TAR extraction
        try:
            with tarfile.open(archive_path, "r:*") as tar:
                tar.extractall(path=extract_dir)
            for item in extract_dir.iterdir():
                if item.is_file() and item.suffix in [".tar", ".gz", ".tar.gz"]:
                    nested_dir = extract_dir / f"nested_{depth}"
                    if extract_recursive(item, nested_dir, depth + 1):
                        for inner in nested_dir.iterdir():
                            shutil.move(str(inner), str(extract_dir / f"{nested_dir.name}_{inner.name}"))
                        shutil.rmtree(nested_dir)
                    item.unlink(missing_ok=True)
            return True
        except: pass
        
        # Try GZIP extraction (single file)
        try:
            with gzip.open(archive_path, "rb") as f_in:
                decompressed_data = f_in.read()
            out_name = archive_path.stem if archive_path.suffix == ".gz" else archive_path.name
            out_path = extract_dir / out_name
            with open(out_path, "wb") as f_out: f_out.write(decompressed_data)
            if out_path.suffix == ".tar":
                extract_recursive(out_path, extract_dir, depth + 1)
                out_path.unlink()
            elif is_latex_file(out_path):
                out_path.rename(out_path.with_suffix(".tex"))
            return True
        except: return False
    except: return False

def extract_archive(archive_path: Path, extract_dir: Path) -> bool:
    if not archive_path.exists() or archive_path.stat().st_size == 0: return False
    ok = extract_recursive(archive_path, extract_dir)
    tex_files = list(extract_dir.rglob("*.tex"))
    return ok and len(tex_files) > 0

def remove_figure_files(path_root: Path) -> int:
    removed = 0
    for root, _, files in os.walk(path_root):
        for fname in files:
            if Path(fname).suffix.lower() in IMAGE_EXTS:
                try: os.remove(Path(root)/fname); removed += 1
                except: pass
    return removed

def copy_tex_and_bib_keep_structure(extracted_root: Path, target_version_dir: Path) -> Tuple[int, int]:
    tex_c, bib_c = 0, 0
    for root, _, files in os.walk(extracted_root):
        for fname in files:
            if Path(fname).suffix.lower() in ('.tex', '.bib'):
                src = Path(root) / fname
                dst = target_version_dir / src.relative_to(extracted_root)
                dst.parent.mkdir(parents=True, exist_ok=True)
                shutil.copy2(src, dst)
                if dst.suffix == '.tex': tex_c += 1
                else: bib_c += 1
    return tex_c, bib_c

# ==============================================================================
# MAIN EXECUTION
# ==============================================================================
def process_paper(arxiv_base_id: str):
    start_time = time.time()
    # Start RAM tracking
    stop_mem, mem_stats = threading.Event(), {"max": 0, "avg": 0}
    threading.Thread(target=track_memory_usage, args=(stop_mem, mem_stats)).start()

    log(f"Processing {arxiv_base_id}")
    folder_name = arxiv_id_to_folder(arxiv_base_id)
    paper_dir = BASE_DIR / folder_name
    paper_dir.mkdir(parents=True, exist_ok=True)
    
    # Metadata & References
    meta = get_metadata(arxiv_base_id)
    versions = discover_versions_via_abs(arxiv_base_id)
    venue, refs = get_semantic_data(arxiv_base_id)
    if venue: meta['publication_venue'] = venue

    # Prepare for download
    total_tex, total_bib, size_before, size_after = 0, 0, 0, 0
    got_tex = False
    tex_root = paper_dir / "tex"
    tex_root.mkdir(exist_ok=True)

    for ver in versions:
        arxiv_ver = f"{arxiv_base_id}{ver}"
        tmp_dir = Path(tempfile.mkdtemp())
        dl_path = tmp_dir / f"{arxiv_ver}.tar.gz"
        
        # Download -> Extract -> Clean -> Copy
        if download_eprint(arxiv_ver, dl_path) or download_eprint(arxiv_base_id, dl_path):
            ver_folder = tex_root / f"{format_yymm_id(arxiv_ver)}{ver}"
            ver_folder.mkdir(exist_ok=True)
            
            extract_dir = tmp_dir / "extracted"
            if extract_archive(dl_path, extract_dir):
                size_before += get_total_size(extract_dir)
                t, b = copy_tex_and_bib_keep_structure(extract_dir, ver_folder)
                total_tex += t; total_bib += b
                remove_figure_files(ver_folder)
                size_after += get_total_size(ver_folder)
                if t > 0: got_tex = True
            shutil.rmtree(tmp_dir, ignore_errors=True)

    # Save JSONs
    json.dump(meta, open(paper_dir/"metadata.json", "w"), indent=4, default=str)
    json.dump(refs, open(paper_dir/"references.json", "w"), indent=4, default=str)

    # Stop tracking
    stop_mem.set()
    duration = time.time() - start_time
    status = 'success' if got_tex else 'no_tex'

    # CSV Report
    with open(BASE_DIR / "performance.csv", "a", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow([
            arxiv_base_id, datetime.fromtimestamp(start_time).isoformat(), round(duration, 3),
            round(size_before/1048576, 3), round(size_after/1048576, 3),
            round(mem_stats["max"], 3), status
        ])

    return {"arxiv_id": arxiv_base_id, "status": status, "tex_files": total_tex}

def run_scraper():
    BASE_DIR.mkdir(parents=True, exist_ok=True)
    ids = [f"{START_MONTH}.{i:05d}" for i in range(START_ID, END_ID + 1)]
    log(f"Starting crawl for {len(ids)} papers with {MAX_WORKERS} workers.")
    
    # Init CSV Header
    csv_file = BASE_DIR / "performance.csv"
    if not csv_file.exists():
        with open(csv_file, "w", newline="", encoding="utf-8") as f:
            csv.writer(f).writerow(["arxiv_id", "start_time", "duration_sec", "size_before_mb", "size_after_mb", "max_ram_mb", "status"])

    # Multi-threading Execution
    results = []
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as exc:
        futures = {exc.submit(process_paper, pid): pid for pid in ids}
        for fut in as_completed(futures):
            try: results.append(fut.result())
            except Exception as e: log(f"Error: {e}")

    log("Scraping Completed.")

if __name__ == "__main__":
    run_scraper()